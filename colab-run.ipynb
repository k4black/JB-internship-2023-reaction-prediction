{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1BjAedCaULD"
   },
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCbvz3Y811Zp",
    "outputId": "ba074583-ebfa-43dc-b264-f291b0f8c2b8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m68.8/68.8 KB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/6.6 MB\u001B[0m \u001B[31m80.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.0/83.0 KB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m56.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m100.3/100.3 KB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.7/12.7 MB\u001B[0m \u001B[31m76.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.5/85.5 MB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.0/4.0 MB\u001B[0m \u001B[31m95.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m200.5/200.5 KB\u001B[0m \u001B[31m15.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for fasttext (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for hpsklearn (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -q fasttext spacy scikit-learn-intelex hyperopt scikit-optimize git+https://github.com/hyperopt/hyperopt-sklearn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IP9Z-mzdGsRx",
    "outputId": "37456614-de1e-4592-b519-e10f5758c630"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m469.0/469.0 KB\u001B[0m \u001B[31m20.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.7/6.7 MB\u001B[0m \u001B[31m98.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.4/81.4 KB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m212.8/212.8 KB\u001B[0m \u001B[31m20.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m448.8/448.8 KB\u001B[0m \u001B[31m36.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.5/410.5 KB\u001B[0m \u001B[31m41.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.7/134.7 KB\u001B[0m \u001B[31m15.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m184.3/184.3 KB\u001B[0m \u001B[31m21.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m840.9/840.9 KB\u001B[0m \u001B[31m64.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.9/55.9 KB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m15.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.9/132.9 KB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m110.5/110.5 KB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m199.2/199.2 KB\u001B[0m \u001B[31m20.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m212.2/212.2 KB\u001B[0m \u001B[31m18.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.6/7.6 MB\u001B[0m \u001B[31m47.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.5/10.5 MB\u001B[0m \u001B[31m97.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m79.6/79.6 KB\u001B[0m \u001B[31m10.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.7/67.7 KB\u001B[0m \u001B[31m8.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m136.8/136.8 KB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m199.2/199.2 KB\u001B[0m \u001B[31m20.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m114.2/114.2 KB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m264.6/264.6 KB\u001B[0m \u001B[31m26.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m158.8/158.8 KB\u001B[0m \u001B[31m17.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.7/62.7 KB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.4/66.4 KB\u001B[0m \u001B[31m8.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for future (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q datasets transformers evaluate torchinfo accelerate neptune-client==0.16.18 nlpaug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0rRtGtl2v0l"
   },
   "source": [
    "## Setup actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKEVBZFzaR8v",
    "outputId": "2071493f-aefa-47cc-b017-b3939b64a487"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Thu Mar 16 15:11:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   42C    P0    25W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grd03Wf912XV",
    "outputId": "3b081c26-bf8e-43e2-c02c-66551d3c9b38"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2023-03-16 15:11:17.564120: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 15:11:18.434092: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 15:11:18.434210: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 15:11:18.434229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m59.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.25.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (63.4.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.4.1\n",
      "    Uninstalling en-core-web-sm-3.4.1:\n",
      "      Successfully uninstalled en-core-web-sm-3.4.1\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3esG3YuCOPVT",
    "outputId": "91fa7451-f757-4769-fa1e-9e61a911b1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-15 12:40:16--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4503593528 (4.2G) [application/octet-stream]\n",
      "Saving to: ‘cc.en.300.bin.gz’\n",
      "\n",
      "cc.en.300.bin.gz      4%[                    ] 194.56M  17.9MB/s    eta 4m 27s ^C\n",
      "CPU times: user 120 ms, sys: 24.6 ms, total: 145 ms\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "!gzip -dk cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExIgw0PCJRu8"
   },
   "outputs": [],
   "source": [
    "# import fasttext.util\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nziaYuEjO4F",
    "outputId": "209b482c-351a-4618-9efa-acf3df5bf986"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=10CvDP3AFOTYmoXhWXLRDm6n_XSZV6Yev\n",
      "To: /content/fb_sentiment.csv\n",
      "\r  0% 0.00/123k [00:00<?, ?B/s]\r100% 123k/123k [00:00<00:00, 66.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 10CvDP3AFOTYmoXhWXLRDm6n_XSZV6Yev\n",
    "import pandas as pd\n",
    "df = pd.read_csv('fb_sentiment.csv')\n",
    "df.columns = ['id', 'text', 'label']\n",
    "df['label'] = df['label'].apply(lambda x: {'N': 'negative', 'P': 'positive', 'O': 'neutral'}[x])\n",
    "df.to_csv('dataset.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJar75xFRiph",
    "outputId": "4652dcc8-a29b-48e4-cbe6-aac7a3a6667a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "len 79\n",
      "text len 190.44303797468353\n",
      "neutral\n",
      "len 280\n",
      "text len 93.76428571428572\n",
      "positive\n",
      "len 641\n",
      "text len 115.49921996879876\n",
      "total\n",
      "len 1000\n",
      "text len 115.334\n"
     ]
    }
   ],
   "source": [
    "!python eda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3XGNJ9I2Jim",
    "outputId": "1b0e84cc-a4a1-4ad3-8ae9-9daa90b27291",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%env NEPTUNE_PROJECT=k4black/jb-reaction-prediction\n",
    "%env NEPTUNE_API_TOKEN=TBA_TBA_TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhHsoc-aHMC5"
   },
   "source": [
    "## ML Baseline searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "v1FkXQD2HLaN",
    "outputId": "27273df0-1366-4e44-b8e8-947dc4b3b4f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-03-15 00:09:18.568200: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 00:09:21.255205: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-15 00:09:21.255378: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-15 00:09:21.255402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-15 00:09:24.069464: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "2882\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "vectorizer:   0% 0/4 [00:00<?, ?it/s]\n",
      "models:   0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "models:  14% 1/7 [00:09<00:54,  9.16s/it]\u001B[A\n",
      "models:  29% 2/7 [00:10<00:21,  4.33s/it]\u001B[A\n",
      "models:  43% 3/7 [00:10<00:10,  2.58s/it]\u001B[A\n",
      "models:  57% 4/7 [00:13<00:07,  2.55s/it]\u001B[A\n",
      "models:  71% 5/7 [00:19<00:08,  4.04s/it]\u001B[A\n",
      "models:  86% 6/7 [00:20<00:02,  2.85s/it]\u001B[A\n",
      "models: 100% 7/7 [00:20<00:00,  2.96s/it]\n",
      "vectorizer:  25% 1/4 [00:20<01:02, 20.72s/it]\n",
      "models:   0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "models:  14% 1/7 [00:00<00:04,  1.38it/s]\u001B[A\n",
      "models:  29% 2/7 [00:02<00:06,  1.21s/it]\u001B[A\n",
      "models:  43% 3/7 [00:02<00:03,  1.25it/s]\u001B[A\n",
      "models:  57% 4/7 [00:05<00:05,  1.82s/it]\u001B[A\n",
      "models:  71% 5/7 [00:16<00:09,  4.81s/it]\u001B[A\n",
      "models:  86% 6/7 [00:16<00:03,  3.27s/it]\u001B[A\n",
      "models: 100% 7/7 [00:18<00:00,  2.57s/it]\n",
      "vectorizer:  50% 2/4 [00:38<00:38, 19.12s/it]\n",
      "models:   0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "models:  14% 1/7 [00:08<00:50,  8.34s/it]\u001B[A\n",
      "models:  29% 2/7 [00:09<00:19,  3.99s/it]\u001B[A\n",
      "models:  43% 3/7 [00:09<00:08,  2.23s/it]\u001B[A\n",
      "models:  57% 4/7 [00:28<00:26,  8.83s/it]\u001B[A\n",
      "models:  71% 5/7 [01:06<00:38, 19.43s/it]\u001B[AERROR: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "models: 100% 7/7 [01:07<00:00,  9.69s/it]\n",
      "vectorizer:  75% 3/4 [01:46<00:41, 41.36s/it]\n",
      "models:   0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "models:  14% 1/7 [00:03<00:18,  3.01s/it]\u001B[A\n",
      "models:  29% 2/7 [00:03<00:08,  1.68s/it]\u001B[A\n",
      "models:  43% 3/7 [00:04<00:04,  1.09s/it]\u001B[A\n",
      "models:  57% 4/7 [00:08<00:06,  2.20s/it]\u001B[A\n",
      "models:  71% 5/7 [02:17<01:36, 48.17s/it]\u001B[AERROR: Negative values in data passed to MultinomialNB (input X)\n",
      "models: 100% 7/7 [02:17<00:00, 19.69s/it]\n",
      "vectorizer: 100% 4/4 [04:04<00:00, 61.09s/it]\n",
      "\n",
      "Scores:\n",
      "                     Count  ...  FastText\n",
      "LogReg            0.586648  ...  0.375206\n",
      "SVM               0.505477  ...  0.473674\n",
      "LinearSVM         0.598193  ...  0.459878\n",
      "RandomForest      0.476926  ...  0.467725\n",
      "GradientBoosting  0.588904  ...  0.495743\n",
      "NaiveBayes        0.416246  ...      None\n",
      "KNeighbors        0.444902  ...  0.440651\n",
      "\n",
      "[7 rows x 4 columns]\n",
      "\\text{LogReg} & 0.587 & 0.446 & 0.472 & 0.375 \\\\\n",
      "\\text{SVM} & 0.505 & 0.440 & 0.469 & 0.474 \\\\\n",
      "\\text{LinearSVM} & 0.598 & 0.521 & 0.532 & 0.460 \\\\\n",
      "\\text{RandomForest} & 0.477 & 0.452 & 0.421 & 0.468 \\\\\n",
      "\\text{GradientBoosting} & 0.589 & 0.558 & 0.536 & 0.496 \\\\\n",
      "\\text{NaiveBayes} & 0.416 & 0.273 & - & - \\\\\n",
      "\\text{KNeighbors} & 0.445 & 0.436 & 0.433 & 0.441 \\\\\n",
      "\n",
      "Mean time:\n",
      "                     Count  ...   FastText\n",
      "LogReg            1.814363  ...   0.594711\n",
      "SVM               0.158714  ...   0.104658\n",
      "LinearSVM         0.090341  ...   0.073915\n",
      "RandomForest       0.46575  ...   0.756074\n",
      "GradientBoosting  1.328383  ...  25.926022\n",
      "NaiveBayes        0.094342  ...       None\n",
      "KNeighbors        0.033839  ...   0.000895\n",
      "\n",
      "[7 rows x 4 columns]\n",
      "\\text{LogReg} & 1.814 & 0.131 & 1.658 & 0.595 \\\\\n",
      "\\text{SVM} & 0.159 & 0.262 & 0.157 & 0.105 \\\\\n",
      "\\text{LinearSVM} & 0.090 & 0.047 & 0.022 & 0.074 \\\\\n",
      "\\text{RandomForest} & 0.466 & 0.635 & 3.742 & 0.756 \\\\\n",
      "\\text{GradientBoosting} & 1.328 & 2.011 & 7.629 & 25.926 \\\\\n",
      "\\text{NaiveBayes} & 0.094 & 0.040 & - & - \\\\\n",
      "\\text{KNeighbors} & 0.034 & 0.037 & 0.024 & 0.001 \\\\\n"
     ]
    }
   ],
   "source": [
    "!python classical_ml.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPP_dzkD7Miu"
   },
   "source": [
    "## Baseline searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9362oWro7MLq",
    "outputId": "2df1d512-1302-4695-cce1-303c61a500cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 18:10:17.923316: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 18:10:19.501708: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 18:10:19.501881: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 18:10:19.501908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 506.31it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f1b8cf5410ca5d25.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dff3e1e2e4558f52.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/500 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.9524, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<01:58,  3.79it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.63it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7533538937568665, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.4958, 'eval_samples_per_second': 403.401, 'eval_steps_per_second': 14.119, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<01:58,  3.79it/s]\n",
      "100% 7/7 [00:00<00:00, 13.32it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "{'loss': 0.5901, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:30<01:16,  5.25it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.32it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6346195340156555, 'eval_f1': 0.4762924305276774, 'eval_runtime': 0.4978, 'eval_samples_per_second': 401.801, 'eval_steps_per_second': 14.063, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:16,  5.25it/s]\n",
      "100% 7/7 [00:00<00:00, 13.18it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3725, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:49<01:36,  3.63it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.33it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5725430846214294, 'eval_f1': 0.5251119251119251, 'eval_runtime': 0.5158, 'eval_samples_per_second': 387.717, 'eval_steps_per_second': 13.57, 'epoch': 6.0}\n",
      " 30% 150/500 [00:49<01:36,  3.63it/s]\n",
      "100% 7/7 [00:00<00:00, 12.91it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2411, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:07<01:12,  4.14it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.77it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6284597516059875, 'eval_f1': 0.6569975827290155, 'eval_runtime': 0.4981, 'eval_samples_per_second': 401.549, 'eval_steps_per_second': 14.054, 'epoch': 8.0}\n",
      " 40% 200/500 [01:08<01:12,  4.14it/s]\n",
      "100% 7/7 [00:00<00:00, 13.01it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1458, 'learning_rate': 5.305263157894738e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:26<00:55,  4.50it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.58it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7702865600585938, 'eval_f1': 0.649415825143849, 'eval_runtime': 0.5026, 'eval_samples_per_second': 397.945, 'eval_steps_per_second': 13.928, 'epoch': 10.0}\n",
      " 50% 250/500 [01:26<00:55,  4.50it/s]\n",
      "100% 7/7 [00:00<00:00, 12.87it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.0803, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:44<00:56,  3.52it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.90it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7084994316101074, 'eval_f1': 0.6746666666666666, 'eval_runtime': 0.5033, 'eval_samples_per_second': 397.363, 'eval_steps_per_second': 13.908, 'epoch': 12.0}\n",
      " 60% 300/500 [01:45<00:56,  3.52it/s]\n",
      "100% 7/7 [00:00<00:00, 12.83it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0383, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:04<00:50,  2.95it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.34it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8130157589912415, 'eval_f1': 0.7296399535423926, 'eval_runtime': 0.5306, 'eval_samples_per_second': 376.925, 'eval_steps_per_second': 13.192, 'epoch': 14.0}\n",
      " 70% 350/500 [02:04<00:50,  2.95it/s]\n",
      "100% 7/7 [00:00<00:00, 12.53it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0223, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:23<00:25,  3.89it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.73it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9088502526283264, 'eval_f1': 0.7227303304095495, 'eval_runtime': 0.5105, 'eval_samples_per_second': 391.795, 'eval_steps_per_second': 13.713, 'epoch': 16.0}\n",
      " 80% 400/500 [02:23<00:25,  3.89it/s]\n",
      "100% 7/7 [00:00<00:00, 12.64it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0197, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:41<00:11,  4.43it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.21it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 1.0381795167922974, 'eval_f1': 0.6747835497835498, 'eval_runtime': 0.5046, 'eval_samples_per_second': 396.366, 'eval_steps_per_second': 13.873, 'epoch': 18.0}\n",
      " 90% 450/500 [02:42<00:11,  4.43it/s]\n",
      "100% 7/7 [00:00<00:00, 12.84it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.014, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [03:00<00:00,  4.29it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.74it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 1.008382797241211, 'eval_f1': 0.6813870842356599, 'eval_runtime': 0.5336, 'eval_samples_per_second': 374.805, 'eval_steps_per_second': 13.118, 'epoch': 20.0}\n",
      "100% 500/500 [03:01<00:00,  4.29it/s]\n",
      "100% 7/7 [00:00<00:00, 12.37it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-roberta-base-config-default/checkpoint-350 (score: 0.7296399535423926).\n",
      "{'train_runtime': 190.4926, 'train_samples_per_second': 83.993, 'train_steps_per_second': 2.625, 'train_loss': 0.24766792571544646, 'epoch': 20.0}\n",
      "100% 500/500 [03:10<00:00,  4.29it/s]Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "100% 500/500 [03:10<00:00,  2.62it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 13.67it/s]\n",
      "f1 0.7296399535423926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5385    0.5833    0.5600        12\n",
      "           1     0.7667    0.7302    0.7480        63\n",
      "           2     0.8740    0.8880    0.8810       125\n",
      "\n",
      "    accuracy                         0.8200       200\n",
      "   macro avg     0.7264    0.7338    0.7296       200\n",
      "weighted avg     0.8201    0.8200    0.8198       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-77f1bcfd71e59ba6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-561afed198fb9a9b.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "{'loss': 0.9781, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<01:42,  4.39it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.35it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7629150152206421, 'eval_f1': 0.26506024096385544, 'eval_runtime': 0.4097, 'eval_samples_per_second': 488.159, 'eval_steps_per_second': 17.086, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<01:42,  4.39it/s]\n",
      "100% 7/7 [00:00<00:00, 18.02it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.651, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<02:05,  3.18it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.60it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5661313533782959, 'eval_f1': 0.5172568172568172, 'eval_runtime': 0.4054, 'eval_samples_per_second': 493.355, 'eval_steps_per_second': 17.267, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:05,  3.18it/s]\n",
      "100% 7/7 [00:00<00:00, 18.11it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4077, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:44,  3.36it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.19it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5590932369232178, 'eval_f1': 0.5371008782730035, 'eval_runtime': 0.4179, 'eval_samples_per_second': 478.588, 'eval_steps_per_second': 16.751, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:44,  3.36it/s]\n",
      "100% 7/7 [00:00<00:00, 17.96it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2785, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:17,  3.86it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.56it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6075260639190674, 'eval_f1': 0.6264714290518952, 'eval_runtime': 0.4097, 'eval_samples_per_second': 488.212, 'eval_steps_per_second': 17.087, 'epoch': 8.0}\n",
      " 40% 200/500 [01:11<01:17,  3.86it/s]\n",
      "100% 7/7 [00:00<00:00, 17.91it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1991, 'learning_rate': 5.305263157894738e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:30<00:56,  4.44it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.32it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7247644066810608, 'eval_f1': 0.6407668251724287, 'eval_runtime': 0.4104, 'eval_samples_per_second': 487.283, 'eval_steps_per_second': 17.055, 'epoch': 10.0}\n",
      " 50% 250/500 [01:30<00:56,  4.44it/s]\n",
      "100% 7/7 [00:00<00:00, 18.03it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1364, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:49<00:47,  4.22it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.15it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.701741635799408, 'eval_f1': 0.6446266620179664, 'eval_runtime': 0.4204, 'eval_samples_per_second': 475.758, 'eval_steps_per_second': 16.652, 'epoch': 12.0}\n",
      " 60% 300/500 [01:50<00:47,  4.22it/s]\n",
      "100% 7/7 [00:00<00:00, 17.93it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0866, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:08<00:46,  3.24it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.23it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7957136631011963, 'eval_f1': 0.6286335596680425, 'eval_runtime': 0.4054, 'eval_samples_per_second': 493.333, 'eval_steps_per_second': 17.267, 'epoch': 14.0}\n",
      " 70% 350/500 [02:09<00:46,  3.24it/s]\n",
      "100% 7/7 [00:00<00:00, 18.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0651, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:27<00:21,  4.60it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.20it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8546047806739807, 'eval_f1': 0.6312331327388001, 'eval_runtime': 0.4073, 'eval_samples_per_second': 491.008, 'eval_steps_per_second': 17.185, 'epoch': 16.0}\n",
      " 80% 400/500 [02:28<00:21,  4.60it/s]\n",
      "100% 7/7 [00:00<00:00, 18.15it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0436, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:47<00:10,  4.86it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.37it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8301452398300171, 'eval_f1': 0.6383692643234627, 'eval_runtime': 0.4063, 'eval_samples_per_second': 492.248, 'eval_steps_per_second': 17.229, 'epoch': 18.0}\n",
      " 90% 450/500 [02:47<00:10,  4.86it/s]\n",
      "100% 7/7 [00:00<00:00, 18.12it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0368, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [03:05<00:00,  4.61it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.38it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8580496907234192, 'eval_f1': 0.6283229041849732, 'eval_runtime': 0.4306, 'eval_samples_per_second': 464.506, 'eval_steps_per_second': 16.258, 'epoch': 20.0}\n",
      "100% 500/500 [03:06<00:00,  4.61it/s]\n",
      "100% 7/7 [00:00<00:00, 17.61it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-roberta-base-config-default/checkpoint-300 (score: 0.6446266620179664).\n",
      "{'train_runtime': 195.0656, 'train_samples_per_second': 82.024, 'train_steps_per_second': 2.563, 'train_loss': 0.28828612446784974, 'epoch': 20.0}\n",
      "100% 500/500 [03:14<00:00,  4.61it/s]Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "100% 500/500 [03:15<00:00,  2.56it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 17.42it/s]\n",
      "f1 0.6446266620179664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3333    0.2857    0.3077        14\n",
      "           1     0.6885    0.7778    0.7304        54\n",
      "           2     0.9134    0.8788    0.8958       132\n",
      "\n",
      "    accuracy                         0.8100       200\n",
      "   macro avg     0.6451    0.6474    0.6446       200\n",
      "weighted avg     0.8121    0.8100    0.8100       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-44a8567a3a3fb495.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "{'loss': 0.9735, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<01:44,  4.32it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.39it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7873480319976807, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.3973, 'eval_samples_per_second': 503.46, 'eval_steps_per_second': 17.621, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<01:44,  4.32it/s]\n",
      "100% 7/7 [00:00<00:00, 19.19it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.6511, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:03,  3.23it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.52it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5333119034767151, 'eval_f1': 0.5214416291348796, 'eval_runtime': 0.3987, 'eval_samples_per_second': 501.643, 'eval_steps_per_second': 17.557, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:03,  3.23it/s]\n",
      "100% 7/7 [00:00<00:00, 19.12it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3903, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:51,  3.15it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.16it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5196203589439392, 'eval_f1': 0.5290103442951984, 'eval_runtime': 0.4176, 'eval_samples_per_second': 478.953, 'eval_steps_per_second': 16.763, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:51,  3.15it/s]\n",
      "100% 7/7 [00:00<00:00, 18.25it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2632, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:25,  3.51it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.87it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5803332328796387, 'eval_f1': 0.6791585028194018, 'eval_runtime': 0.3979, 'eval_samples_per_second': 502.603, 'eval_steps_per_second': 17.591, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:25,  3.51it/s]\n",
      "100% 7/7 [00:00<00:00, 19.15it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.172, 'learning_rate': 5.305263157894738e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:29<00:57,  4.36it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.62it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6232116222381592, 'eval_f1': 0.6543058187918, 'eval_runtime': 0.3992, 'eval_samples_per_second': 500.943, 'eval_steps_per_second': 17.533, 'epoch': 10.0}\n",
      " 50% 250/500 [01:30<00:57,  4.36it/s]\n",
      "100% 7/7 [00:00<00:00, 19.06it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.1184, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:48<00:55,  3.62it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.70it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7294715046882629, 'eval_f1': 0.6520602443876101, 'eval_runtime': 0.4084, 'eval_samples_per_second': 489.691, 'eval_steps_per_second': 17.139, 'epoch': 12.0}\n",
      " 60% 300/500 [01:49<00:55,  3.62it/s]\n",
      "100% 7/7 [00:00<00:00, 18.76it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0855, 'learning_rate': 3.2421052631578945e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:07<00:46,  3.26it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.77it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7867041230201721, 'eval_f1': 0.6610578278904972, 'eval_runtime': 0.3965, 'eval_samples_per_second': 504.409, 'eval_steps_per_second': 17.654, 'epoch': 14.0}\n",
      " 70% 350/500 [02:08<00:46,  3.26it/s]\n",
      "100% 7/7 [00:00<00:00, 19.20it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0435, 'learning_rate': 2.1894736842105264e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:26<00:24,  4.10it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.86it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8879941701889038, 'eval_f1': 0.6335624786699819, 'eval_runtime': 0.3945, 'eval_samples_per_second': 506.931, 'eval_steps_per_second': 17.743, 'epoch': 16.0}\n",
      " 80% 400/500 [02:27<00:24,  4.10it/s]\n",
      "100% 7/7 [00:00<00:00, 19.39it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0261, 'learning_rate': 1.136842105263158e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:45<00:13,  3.70it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 26.04it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9252164363861084, 'eval_f1': 0.6217246474500984, 'eval_runtime': 0.3942, 'eval_samples_per_second': 507.311, 'eval_steps_per_second': 17.756, 'epoch': 18.0}\n",
      " 90% 450/500 [02:45<00:13,  3.70it/s]\n",
      "100% 7/7 [00:00<00:00, 19.34it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-roberta-base-config-default/checkpoint-200 (score: 0.6791585028194018).\n",
      "{'train_runtime': 174.6238, 'train_samples_per_second': 91.626, 'train_steps_per_second': 2.863, 'train_loss': 0.3026268747117784, 'epoch': 18.0}\n",
      " 90% 450/500 [02:54<00:13,  3.70it/s]Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      " 90% 450/500 [02:55<00:19,  2.57it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 16.69it/s]\n",
      "f1 0.6791585028194018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3548    0.6875    0.4681        16\n",
      "           1     0.7805    0.5818    0.6667        55\n",
      "           2     0.9062    0.8992    0.9027       129\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.6805    0.7228    0.6792       200\n",
      "weighted avg     0.8276    0.7950    0.8030       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1bf83d760e7b15cb.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4bbbdfab4d4c8833.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "{'loss': 0.9681, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<02:12,  3.40it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.19it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7358551025390625, 'eval_f1': 0.26921422663358147, 'eval_runtime': 0.4839, 'eval_samples_per_second': 413.312, 'eval_steps_per_second': 14.466, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:12,  3.40it/s]\n",
      "100% 7/7 [00:00<00:00, 14.13it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.5745, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:43,  3.87it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.25it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.49648329615592957, 'eval_f1': 0.5502645502645502, 'eval_runtime': 0.4724, 'eval_samples_per_second': 423.392, 'eval_steps_per_second': 14.819, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<01:43,  3.87it/s]\n",
      "100% 7/7 [00:00<00:00, 13.99it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3436, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:50<01:20,  4.36it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.54it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.523468554019928, 'eval_f1': 0.6381596316687594, 'eval_runtime': 0.464, 'eval_samples_per_second': 431.04, 'eval_steps_per_second': 15.086, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:20,  4.36it/s]\n",
      "100% 7/7 [00:00<00:00, 14.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2279, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:09<01:14,  4.01it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.03it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5300778150558472, 'eval_f1': 0.6309770362268167, 'eval_runtime': 0.4739, 'eval_samples_per_second': 422.027, 'eval_steps_per_second': 14.771, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:14,  4.01it/s]\n",
      "100% 7/7 [00:00<00:00, 14.23it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.1385, 'learning_rate': 5.305263157894738e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:28<01:00,  4.10it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.54it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6856815814971924, 'eval_f1': 0.6060190157934519, 'eval_runtime': 0.4622, 'eval_samples_per_second': 432.692, 'eval_steps_per_second': 15.144, 'epoch': 10.0}\n",
      " 50% 250/500 [01:29<01:00,  4.10it/s]\n",
      "100% 7/7 [00:00<00:00, 14.29it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0972, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:48<00:53,  3.71it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.98it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6792186498641968, 'eval_f1': 0.6686748324679358, 'eval_runtime': 0.4644, 'eval_samples_per_second': 430.658, 'eval_steps_per_second': 15.073, 'epoch': 12.0}\n",
      " 60% 300/500 [01:48<00:53,  3.71it/s]\n",
      "100% 7/7 [00:00<00:00, 14.18it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0573, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:07<00:34,  4.37it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.23it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7515406608581543, 'eval_f1': 0.6815073686839851, 'eval_runtime': 0.4794, 'eval_samples_per_second': 417.168, 'eval_steps_per_second': 14.601, 'epoch': 14.0}\n",
      " 70% 350/500 [02:08<00:34,  4.37it/s]\n",
      "100% 7/7 [00:00<00:00, 14.07it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0421, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:26<00:37,  2.65it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.16it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7561274170875549, 'eval_f1': 0.6957239145658263, 'eval_runtime': 0.4668, 'eval_samples_per_second': 428.404, 'eval_steps_per_second': 14.994, 'epoch': 16.0}\n",
      " 80% 400/500 [02:27<00:37,  2.65it/s]\n",
      "100% 7/7 [00:00<00:00, 14.26it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0275, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:45<00:12,  4.06it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.70it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.825640857219696, 'eval_f1': 0.6718249268917514, 'eval_runtime': 0.4674, 'eval_samples_per_second': 427.937, 'eval_steps_per_second': 14.978, 'epoch': 18.0}\n",
      " 90% 450/500 [02:46<00:12,  4.06it/s]\n",
      "100% 7/7 [00:00<00:00, 14.12it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "{'loss': 0.0231, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [03:04<00:00,  4.12it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.60it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.868746817111969, 'eval_f1': 0.6600682908822444, 'eval_runtime': 0.4633, 'eval_samples_per_second': 431.668, 'eval_steps_per_second': 15.108, 'epoch': 20.0}\n",
      "100% 500/500 [03:04<00:00,  4.12it/s]\n",
      "100% 7/7 [00:00<00:00, 14.26it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-roberta-base-config-default/checkpoint-400 (score: 0.6957239145658263).\n",
      "{'train_runtime': 193.642, 'train_samples_per_second': 82.627, 'train_steps_per_second': 2.582, 'train_loss': 0.24999642395973207, 'epoch': 20.0}\n",
      "100% 500/500 [03:13<00:00,  4.12it/s]Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "100% 500/500 [03:14<00:00,  2.58it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 15.90it/s]\n",
      "f1 0.6957239145658263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4167    0.3846    0.4000        13\n",
      "           1     0.7931    0.7541    0.7731        61\n",
      "           2     0.9000    0.9286    0.9141       126\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7033    0.6891    0.6957       200\n",
      "weighted avg     0.8360    0.8400    0.8377       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9919c8c94277c7c9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-32f42fe096624aa1.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "{'loss': 0.966, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<02:05,  3.58it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.93it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.8470709323883057, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.4228, 'eval_samples_per_second': 473.016, 'eval_steps_per_second': 16.556, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:05,  3.58it/s]\n",
      "100% 7/7 [00:00<00:00, 17.79it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.6321, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<01:30,  4.40it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.87it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5564267039299011, 'eval_f1': 0.504932815499308, 'eval_runtime': 0.4172, 'eval_samples_per_second': 479.363, 'eval_steps_per_second': 16.778, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<01:30,  4.40it/s]\n",
      "100% 7/7 [00:00<00:00, 17.86it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4011, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:13,  4.74it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.28it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.48463523387908936, 'eval_f1': 0.5113651294179477, 'eval_runtime': 0.411, 'eval_samples_per_second': 486.568, 'eval_steps_per_second': 17.03, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:13,  4.74it/s]\n",
      "100% 7/7 [00:00<00:00, 18.15it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2829, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:57,  2.55it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.35it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5759761929512024, 'eval_f1': 0.6327853363567649, 'eval_runtime': 0.432, 'eval_samples_per_second': 462.939, 'eval_steps_per_second': 16.203, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:57,  2.55it/s]\n",
      "100% 7/7 [00:00<00:00, 17.10it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1901, 'learning_rate': 5.305263157894738e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:29<00:57,  4.31it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.32it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6161927580833435, 'eval_f1': 0.6183671170394485, 'eval_runtime': 0.4084, 'eval_samples_per_second': 489.769, 'eval_steps_per_second': 17.142, 'epoch': 10.0}\n",
      " 50% 250/500 [01:29<00:57,  4.31it/s]\n",
      "100% 7/7 [00:00<00:00, 18.28it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.1223, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:48<00:57,  3.50it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.05it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7544253468513489, 'eval_f1': 0.6663384953789935, 'eval_runtime': 0.4097, 'eval_samples_per_second': 488.147, 'eval_steps_per_second': 17.085, 'epoch': 12.0}\n",
      " 60% 300/500 [01:48<00:57,  3.50it/s]\n",
      "100% 7/7 [00:00<00:00, 18.29it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0766, 'learning_rate': 3.2421052631578945e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:07<00:47,  3.13it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.91it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7407439351081848, 'eval_f1': 0.6659367396593674, 'eval_runtime': 0.4214, 'eval_samples_per_second': 474.568, 'eval_steps_per_second': 16.61, 'epoch': 14.0}\n",
      " 70% 350/500 [02:07<00:47,  3.13it/s]\n",
      "100% 7/7 [00:00<00:00, 17.73it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0453, 'learning_rate': 2.1894736842105264e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:26<00:31,  3.16it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.41it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8113462924957275, 'eval_f1': 0.66306868472296, 'eval_runtime': 0.4205, 'eval_samples_per_second': 475.583, 'eval_steps_per_second': 16.645, 'epoch': 16.0}\n",
      " 80% 400/500 [02:26<00:31,  3.16it/s]\n",
      "100% 7/7 [00:00<00:00, 17.89it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0323, 'learning_rate': 1.136842105263158e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:45<00:10,  4.56it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.22it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8540376424789429, 'eval_f1': 0.6989376473247443, 'eval_runtime': 0.4037, 'eval_samples_per_second': 495.418, 'eval_steps_per_second': 17.34, 'epoch': 18.0}\n",
      " 90% 450/500 [02:45<00:10,  4.56it/s]\n",
      "100% 7/7 [00:00<00:00, 18.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.031, 'learning_rate': 8.421052631578947e-08, 'epoch': 20.0}\n",
      "100% 500/500 [03:04<00:00,  4.59it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.61it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9321638345718384, 'eval_f1': 0.6556458034718905, 'eval_runtime': 0.4081, 'eval_samples_per_second': 490.044, 'eval_steps_per_second': 17.152, 'epoch': 20.0}\n",
      "100% 500/500 [03:04<00:00,  4.59it/s]\n",
      "100% 7/7 [00:00<00:00, 18.32it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-roberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-roberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-roberta-base-config-default/checkpoint-450 (score: 0.6989376473247443).\n",
      "{'train_runtime': 191.1674, 'train_samples_per_second': 83.696, 'train_steps_per_second': 2.616, 'train_loss': 0.27799074292182924, 'epoch': 20.0}\n",
      "100% 500/500 [03:11<00:00,  4.59it/s]Deleting older checkpoint [/content/results/finetuning-roberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "100% 500/500 [03:11<00:00,  2.61it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 18.47it/s]\n",
      "f1 0.6989376473247443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7692    0.4167    0.5405        24\n",
      "           1     0.6522    0.6383    0.6452        47\n",
      "           2     0.8723    0.9535    0.9111       129\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.7646    0.6695    0.6989       200\n",
      "weighted avg     0.8082    0.8150    0.8041       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.6938435448288617\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-20\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=roberta-base --cross-validation=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DHM78UaUMcl",
    "outputId": "83b96c13-ff61-4a4f-e9a9-28ff23b9986a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 18:31:43.795534: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 18:31:45.508413: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 18:31:45.508610: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 18:31:45.508641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 3.54kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 84.8kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 1.24MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 1.87MB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 40 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 40 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 149.33it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "Downloading pytorch_model.bin: 100% 440M/440M [00:02<00:00, 218MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 109484547\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/500 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.9317, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:10,  3.45it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.54it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.794771134853363, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.4348, 'eval_samples_per_second': 460.004, 'eval_steps_per_second': 16.1, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:10,  3.45it/s]\n",
      "100% 7/7 [00:00<00:00, 15.25it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "{'loss': 0.6348, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:30<01:13,  5.47it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.49it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5683239698410034, 'eval_f1': 0.5161064425770309, 'eval_runtime': 0.4367, 'eval_samples_per_second': 457.935, 'eval_steps_per_second': 16.028, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:13,  5.47it/s]\n",
      "100% 7/7 [00:00<00:00, 15.11it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3902, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:48<01:38,  3.56it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.39it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.4975830912590027, 'eval_f1': 0.5446737910506027, 'eval_runtime': 0.4405, 'eval_samples_per_second': 454.077, 'eval_steps_per_second': 15.893, 'epoch': 6.0}\n",
      " 30% 150/500 [00:48<01:38,  3.56it/s]\n",
      "100% 7/7 [00:00<00:00, 14.96it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2357, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:06<01:21,  3.69it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.25it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5304628610610962, 'eval_f1': 0.7530554374797909, 'eval_runtime': 0.4579, 'eval_samples_per_second': 436.777, 'eval_steps_per_second': 15.287, 'epoch': 8.0}\n",
      " 40% 200/500 [01:06<01:21,  3.69it/s]\n",
      "100% 7/7 [00:00<00:00, 14.62it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1386, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:24<00:59,  4.22it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.17it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5478189587593079, 'eval_f1': 0.7502304050277857, 'eval_runtime': 0.4673, 'eval_samples_per_second': 427.982, 'eval_steps_per_second': 14.979, 'epoch': 10.0}\n",
      " 50% 250/500 [01:24<00:59,  4.22it/s]\n",
      "100% 7/7 [00:00<00:00, 14.42it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.0743, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:42<00:57,  3.46it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.59it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5724093914031982, 'eval_f1': 0.7640219825894198, 'eval_runtime': 0.466, 'eval_samples_per_second': 429.219, 'eval_steps_per_second': 15.023, 'epoch': 12.0}\n",
      " 60% 300/500 [01:43<00:57,  3.46it/s]\n",
      "100% 7/7 [00:00<00:00, 14.48it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0414, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:00<00:55,  2.70it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.28it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.12it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.616443395614624, 'eval_f1': 0.7598963591089575, 'eval_runtime': 0.7871, 'eval_samples_per_second': 254.096, 'eval_steps_per_second': 8.893, 'epoch': 14.0}\n",
      " 70% 350/500 [02:01<00:55,  2.70it/s]\n",
      "100% 7/7 [00:00<00:00, 10.19it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0275, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:20<00:28,  3.49it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.58it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6517879962921143, 'eval_f1': 0.7613725490196078, 'eval_runtime': 0.4555, 'eval_samples_per_second': 439.105, 'eval_steps_per_second': 15.369, 'epoch': 16.0}\n",
      " 80% 400/500 [02:20<00:28,  3.49it/s]\n",
      "100% 7/7 [00:00<00:00, 14.55it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0195, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:38<00:11,  4.23it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.40it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6702379584312439, 'eval_f1': 0.765989457929329, 'eval_runtime': 0.4567, 'eval_samples_per_second': 437.91, 'eval_steps_per_second': 15.327, 'epoch': 18.0}\n",
      " 90% 450/500 [02:38<00:11,  4.23it/s]\n",
      "100% 7/7 [00:00<00:00, 14.40it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0169, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:56<00:00,  4.33it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.23it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6798973083496094, 'eval_f1': 0.7613725490196078, 'eval_runtime': 0.4616, 'eval_samples_per_second': 433.244, 'eval_steps_per_second': 15.164, 'epoch': 20.0}\n",
      "100% 500/500 [02:57<00:00,  4.33it/s]\n",
      "100% 7/7 [00:00<00:00, 14.29it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450 (score: 0.765989457929329).\n",
      "{'train_runtime': 182.895, 'train_samples_per_second': 87.482, 'train_steps_per_second': 2.734, 'train_loss': 0.2510667461156845, 'epoch': 20.0}\n",
      "100% 500/500 [03:02<00:00,  4.33it/s]Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "100% 500/500 [03:03<00:00,  2.73it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 15.92it/s]\n",
      "f1 0.765989457929329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6154    0.6667    0.6400        12\n",
      "           1     0.7931    0.7302    0.7603        63\n",
      "           2     0.8837    0.9120    0.8976       125\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7641    0.7696    0.7660       200\n",
      "weighted avg     0.8391    0.8400    0.8389       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 109484547\n",
      "{'loss': 0.867, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<01:43,  4.37it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 17.06it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 17.89it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7292224168777466, 'eval_f1': 0.2904761904761905, 'eval_runtime': 0.4363, 'eval_samples_per_second': 458.402, 'eval_steps_per_second': 16.044, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<01:43,  4.37it/s]\n",
      "100% 7/7 [00:00<00:00, 16.91it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "{'loss': 0.6036, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:30<01:55,  3.47it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 17.15it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 17.69it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5749679803848267, 'eval_f1': 0.5132211973232049, 'eval_runtime': 0.4464, 'eval_samples_per_second': 448.046, 'eval_steps_per_second': 15.682, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:55,  3.47it/s]\n",
      "100% 7/7 [00:00<00:00, 16.82it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3829, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:49<01:34,  3.72it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 16.62it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 17.53it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5311529636383057, 'eval_f1': 0.6155179243414537, 'eval_runtime': 0.4537, 'eval_samples_per_second': 440.828, 'eval_steps_per_second': 15.429, 'epoch': 6.0}\n",
      " 30% 150/500 [00:50<01:34,  3.72it/s]\n",
      "100% 7/7 [00:00<00:00, 16.87it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2221, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:08<01:07,  4.43it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 17.17it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 18.03it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5603579878807068, 'eval_f1': 0.656844993945076, 'eval_runtime': 0.433, 'eval_samples_per_second': 461.876, 'eval_steps_per_second': 16.166, 'epoch': 8.0}\n",
      " 40% 200/500 [01:08<01:07,  4.43it/s]\n",
      "100% 7/7 [00:00<00:00, 17.14it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1248, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:26<00:59,  4.23it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 17.22it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 17.69it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5963720679283142, 'eval_f1': 0.6332865332865333, 'eval_runtime': 0.4356, 'eval_samples_per_second': 459.148, 'eval_steps_per_second': 16.07, 'epoch': 10.0}\n",
      " 50% 250/500 [01:26<00:59,  4.23it/s]\n",
      "100% 7/7 [00:00<00:00, 17.04it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.074, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:44<00:46,  4.26it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 17.15it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 17.84it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6725091338157654, 'eval_f1': 0.6285668472996676, 'eval_runtime': 0.4396, 'eval_samples_per_second': 454.953, 'eval_steps_per_second': 15.923, 'epoch': 12.0}\n",
      " 60% 300/500 [01:45<00:46,  4.26it/s]\n",
      "100% 7/7 [00:00<00:00, 16.85it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0419, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:03<00:46,  3.25it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 17.16it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 17.77it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.729006290435791, 'eval_f1': 0.6226333428515608, 'eval_runtime': 0.4606, 'eval_samples_per_second': 434.219, 'eval_steps_per_second': 15.198, 'epoch': 14.0}\n",
      " 70% 350/500 [02:03<00:46,  3.25it/s]\n",
      "100% 7/7 [00:00<00:00, 16.78it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0222, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:21<00:22,  4.49it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 17.37it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 18.05it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7838456630706787, 'eval_f1': 0.6436440157629749, 'eval_runtime': 0.4315, 'eval_samples_per_second': 463.464, 'eval_steps_per_second': 16.221, 'epoch': 16.0}\n",
      " 80% 400/500 [02:22<00:22,  4.49it/s]\n",
      "100% 7/7 [00:00<00:00, 17.19it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0154, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:39<00:09,  5.10it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 17.37it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 18.12it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.792086660861969, 'eval_f1': 0.6355054302422724, 'eval_runtime': 0.4346, 'eval_samples_per_second': 460.161, 'eval_steps_per_second': 16.106, 'epoch': 18.0}\n",
      " 90% 450/500 [02:40<00:09,  5.10it/s]\n",
      "100% 7/7 [00:00<00:00, 16.96it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200 (score: 0.656844993945076).\n",
      "{'train_runtime': 167.6427, 'train_samples_per_second': 95.441, 'train_steps_per_second': 2.983, 'train_loss': 0.2615548446443346, 'epoch': 18.0}\n",
      " 90% 450/500 [02:47<00:09,  5.10it/s]Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      " 90% 450/500 [02:48<00:18,  2.68it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 17.69it/s]\n",
      "f1 0.656844993945076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4444    0.2857    0.3478        14\n",
      "           1     0.7069    0.7593    0.7321        54\n",
      "           2     0.8872    0.8939    0.8906       132\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.6795    0.6463    0.6568       200\n",
      "weighted avg     0.8075    0.8150    0.8098       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 109484547\n",
      "{'loss': 1.0839, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<01:49,  4.11it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 23.85it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7743933200836182, 'eval_f1': 0.3099697169635758, 'eval_runtime': 0.5118, 'eval_samples_per_second': 390.756, 'eval_steps_per_second': 13.676, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<01:49,  4.11it/s]\n",
      "100% 7/7 [00:00<00:00, 14.67it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.6243, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:30<01:44,  3.81it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 23.45it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5926589965820312, 'eval_f1': 0.49356725146198827, 'eval_runtime': 0.5205, 'eval_samples_per_second': 384.226, 'eval_steps_per_second': 13.448, 'epoch': 4.0}\n",
      " 20% 100/500 [00:30<01:44,  3.81it/s]\n",
      "100% 7/7 [00:00<00:00, 14.36it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3966, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:47<01:44,  3.35it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 23.45it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5786083936691284, 'eval_f1': 0.5018843404808316, 'eval_runtime': 0.5304, 'eval_samples_per_second': 377.091, 'eval_steps_per_second': 13.198, 'epoch': 6.0}\n",
      " 30% 150/500 [00:48<01:44,  3.35it/s]\n",
      "100% 7/7 [00:00<00:00, 14.03it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2547, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:05<01:14,  4.02it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 24.19it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5849318504333496, 'eval_f1': 0.5185185185185185, 'eval_runtime': 0.5035, 'eval_samples_per_second': 397.229, 'eval_steps_per_second': 13.903, 'epoch': 8.0}\n",
      " 40% 200/500 [01:06<01:14,  4.02it/s]\n",
      "100% 7/7 [00:00<00:00, 14.82it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1865, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:23<00:59,  4.20it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 23.97it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5752718448638916, 'eval_f1': 0.5650692677410234, 'eval_runtime': 0.5009, 'eval_samples_per_second': 399.266, 'eval_steps_per_second': 13.974, 'epoch': 10.0}\n",
      " 50% 250/500 [01:24<00:59,  4.20it/s]\n",
      "100% 7/7 [00:00<00:00, 14.88it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1371, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:42<00:54,  3.67it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 24.27it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5766518115997314, 'eval_f1': 0.6997428933550607, 'eval_runtime': 0.5035, 'eval_samples_per_second': 397.211, 'eval_steps_per_second': 13.902, 'epoch': 12.0}\n",
      " 60% 300/500 [01:42<00:54,  3.67it/s]\n",
      "100% 7/7 [00:00<00:00, 14.76it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0953, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:59<00:42,  3.49it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 23.60it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6095969080924988, 'eval_f1': 0.6923666845015161, 'eval_runtime': 0.5177, 'eval_samples_per_second': 386.361, 'eval_steps_per_second': 13.523, 'epoch': 14.0}\n",
      " 70% 350/500 [02:00<00:42,  3.49it/s]\n",
      "100% 7/7 [00:00<00:00, 14.49it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0715, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:17<00:25,  3.91it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 23.57it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6483296155929565, 'eval_f1': 0.6995979895387308, 'eval_runtime': 0.5189, 'eval_samples_per_second': 385.394, 'eval_steps_per_second': 13.489, 'epoch': 16.0}\n",
      " 80% 400/500 [02:17<00:25,  3.91it/s]\n",
      "100% 7/7 [00:00<00:00, 14.41it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0603, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:35<00:14,  3.40it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 24.16it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6389342546463013, 'eval_f1': 0.6978761005076795, 'eval_runtime': 0.5003, 'eval_samples_per_second': 399.776, 'eval_steps_per_second': 13.992, 'epoch': 18.0}\n",
      " 90% 450/500 [02:35<00:14,  3.40it/s]\n",
      "100% 7/7 [00:00<00:00, 14.90it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.053, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:52<00:00,  3.48it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 24.15it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6446253061294556, 'eval_f1': 0.7014435838696774, 'eval_runtime': 0.4996, 'eval_samples_per_second': 400.322, 'eval_steps_per_second': 14.011, 'epoch': 20.0}\n",
      "100% 500/500 [02:53<00:00,  3.48it/s]\n",
      "100% 7/7 [00:00<00:00, 14.94it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500 (score: 0.7014435838696774).\n",
      "{'train_runtime': 178.8484, 'train_samples_per_second': 89.461, 'train_steps_per_second': 2.796, 'train_loss': 0.29631452178955076, 'epoch': 20.0}\n",
      "100% 500/500 [02:58<00:00,  3.48it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "100% 500/500 [02:59<00:00,  2.79it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 13.59it/s]\n",
      "f1 0.7014435838696774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.5000    0.5333        16\n",
      "           1     0.7292    0.6364    0.6796        55\n",
      "           2     0.8623    0.9225    0.8914       129\n",
      "\n",
      "    accuracy                         0.8100       200\n",
      "   macro avg     0.7210    0.6863    0.7014       200\n",
      "weighted avg     0.8024    0.8100    0.8045       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 109484547\n",
      "{'loss': 0.8704, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<02:07,  3.53it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.69it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7228033542633057, 'eval_f1': 0.26921422663358147, 'eval_runtime': 0.4998, 'eval_samples_per_second': 400.147, 'eval_steps_per_second': 14.005, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<02:07,  3.53it/s]\n",
      "100% 7/7 [00:00<00:00, 13.02it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.6279, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:30<01:54,  3.51it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.08it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5539278984069824, 'eval_f1': 0.5222207121500407, 'eval_runtime': 0.5075, 'eval_samples_per_second': 394.093, 'eval_steps_per_second': 13.793, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:54,  3.51it/s]\n",
      "100% 7/7 [00:00<00:00, 12.94it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4173, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:49<01:22,  4.23it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.00it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5045307278633118, 'eval_f1': 0.6613959140274931, 'eval_runtime': 0.5177, 'eval_samples_per_second': 386.321, 'eval_steps_per_second': 13.521, 'epoch': 6.0}\n",
      " 30% 150/500 [00:49<01:22,  4.23it/s]\n",
      "100% 7/7 [00:00<00:00, 12.94it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.253, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:07<01:11,  4.22it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.98it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5297051072120667, 'eval_f1': 0.6662247152813191, 'eval_runtime': 0.5013, 'eval_samples_per_second': 398.998, 'eval_steps_per_second': 13.965, 'epoch': 8.0}\n",
      " 40% 200/500 [01:07<01:11,  4.22it/s]\n",
      "100% 7/7 [00:00<00:00, 13.04it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1482, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:25<01:07,  3.69it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.60it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5906112790107727, 'eval_f1': 0.7123091378905334, 'eval_runtime': 0.4997, 'eval_samples_per_second': 400.227, 'eval_steps_per_second': 14.008, 'epoch': 10.0}\n",
      " 50% 250/500 [01:25<01:07,  3.69it/s]\n",
      "100% 7/7 [00:00<00:00, 12.99it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0912, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:43<00:52,  3.79it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.69it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.67038893699646, 'eval_f1': 0.7180354267310789, 'eval_runtime': 0.5016, 'eval_samples_per_second': 398.749, 'eval_steps_per_second': 13.956, 'epoch': 12.0}\n",
      " 60% 300/500 [01:44<00:52,  3.79it/s]\n",
      "100% 7/7 [00:00<00:00, 12.96it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0541, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:02<00:36,  4.11it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.53it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6679588556289673, 'eval_f1': 0.699626287861582, 'eval_runtime': 0.5252, 'eval_samples_per_second': 380.805, 'eval_steps_per_second': 13.328, 'epoch': 14.0}\n",
      " 70% 350/500 [02:02<00:36,  4.11it/s]\n",
      "100% 7/7 [00:00<00:00, 12.69it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0355, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:20<00:31,  3.16it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.35it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7344738841056824, 'eval_f1': 0.6956036965578951, 'eval_runtime': 0.5196, 'eval_samples_per_second': 384.918, 'eval_steps_per_second': 13.472, 'epoch': 16.0}\n",
      " 80% 400/500 [02:20<00:31,  3.16it/s]\n",
      "100% 7/7 [00:00<00:00, 12.85it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0247, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:38<00:13,  3.65it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 19.87it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7420921921730042, 'eval_f1': 0.7264733002001003, 'eval_runtime': 0.5051, 'eval_samples_per_second': 395.999, 'eval_steps_per_second': 13.86, 'epoch': 18.0}\n",
      " 90% 450/500 [02:38<00:13,  3.65it/s]\n",
      "100% 7/7 [00:00<00:00, 11.60it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0205, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:55<00:00,  3.97it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.47it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7539556622505188, 'eval_f1': 0.7092753623188406, 'eval_runtime': 0.5028, 'eval_samples_per_second': 397.805, 'eval_steps_per_second': 13.923, 'epoch': 20.0}\n",
      "100% 500/500 [02:56<00:00,  3.97it/s]\n",
      "100% 7/7 [00:00<00:00, 12.95it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450 (score: 0.7264733002001003).\n",
      "{'train_runtime': 182.2157, 'train_samples_per_second': 87.808, 'train_steps_per_second': 2.744, 'train_loss': 0.2542755420207977, 'epoch': 20.0}\n",
      "100% 500/500 [03:02<00:00,  3.97it/s]Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "100% 500/500 [03:02<00:00,  2.74it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 13.22it/s]\n",
      "f1 0.7264733002001003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4706    0.6154    0.5333        13\n",
      "           1     0.8077    0.6885    0.7434        61\n",
      "           2     0.8855    0.9206    0.9027       126\n",
      "\n",
      "    accuracy                         0.8300       200\n",
      "   macro avg     0.7213    0.7415    0.7265       200\n",
      "weighted avg     0.8348    0.8300    0.8301       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 109484547\n",
      "{'loss': 0.8537, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:02,  3.68it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.24it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7806048393249512, 'eval_f1': 0.2760840108401084, 'eval_runtime': 0.4209, 'eval_samples_per_second': 475.117, 'eval_steps_per_second': 16.629, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:02,  3.68it/s]\n",
      "100% 7/7 [00:00<00:00, 19.92it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "{'loss': 0.601, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:33,  4.26it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.65it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6132926940917969, 'eval_f1': 0.4605021432945498, 'eval_runtime': 0.4401, 'eval_samples_per_second': 454.454, 'eval_steps_per_second': 15.906, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<01:33,  4.26it/s]\n",
      "100% 7/7 [00:00<00:00, 19.16it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3807, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:50<01:11,  4.89it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.98it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5131005048751831, 'eval_f1': 0.5651372765800626, 'eval_runtime': 0.4296, 'eval_samples_per_second': 465.565, 'eval_steps_per_second': 16.295, 'epoch': 6.0}\n",
      " 30% 150/500 [00:50<01:11,  4.89it/s]\n",
      "100% 7/7 [00:00<00:00, 19.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2221, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:08<01:37,  3.07it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.85it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.608706533908844, 'eval_f1': 0.6777933308448425, 'eval_runtime': 0.4197, 'eval_samples_per_second': 476.549, 'eval_steps_per_second': 16.679, 'epoch': 8.0}\n",
      " 40% 200/500 [01:09<01:37,  3.07it/s]\n",
      "100% 7/7 [00:00<00:00, 19.70it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1279, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:26<00:53,  4.70it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.40it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5665707588195801, 'eval_f1': 0.7341490027795898, 'eval_runtime': 0.4138, 'eval_samples_per_second': 483.332, 'eval_steps_per_second': 16.917, 'epoch': 10.0}\n",
      " 50% 250/500 [01:27<00:53,  4.70it/s]\n",
      "100% 7/7 [00:00<00:00, 20.04it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0697, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:45<01:06,  3.02it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.89it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5843321084976196, 'eval_f1': 0.7328055147503036, 'eval_runtime': 0.4322, 'eval_samples_per_second': 462.766, 'eval_steps_per_second': 16.197, 'epoch': 12.0}\n",
      " 60% 300/500 [01:45<01:06,  3.02it/s]\n",
      "100% 7/7 [00:00<00:00, 19.61it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0416, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:03<00:43,  3.46it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.85it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5936710834503174, 'eval_f1': 0.7474188756108271, 'eval_runtime': 0.429, 'eval_samples_per_second': 466.247, 'eval_steps_per_second': 16.319, 'epoch': 14.0}\n",
      " 70% 350/500 [02:03<00:43,  3.46it/s]\n",
      "100% 7/7 [00:00<00:00, 19.61it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0238, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:21<00:31,  3.14it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.70it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6420307755470276, 'eval_f1': 0.7536108966967481, 'eval_runtime': 0.4206, 'eval_samples_per_second': 475.47, 'eval_steps_per_second': 16.641, 'epoch': 16.0}\n",
      " 80% 400/500 [02:22<00:31,  3.14it/s]\n",
      "100% 7/7 [00:00<00:00, 19.71it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0176, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:39<00:11,  4.53it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.41it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6545612812042236, 'eval_f1': 0.7631471446641171, 'eval_runtime': 0.4186, 'eval_samples_per_second': 477.834, 'eval_steps_per_second': 16.724, 'epoch': 18.0}\n",
      " 90% 450/500 [02:40<00:11,  4.53it/s]\n",
      "100% 7/7 [00:00<00:00, 20.08it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.015, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:57<00:00,  4.43it/s]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.93it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6718531847000122, 'eval_f1': 0.7818247859284676, 'eval_runtime': 0.4191, 'eval_samples_per_second': 477.174, 'eval_steps_per_second': 16.701, 'epoch': 20.0}\n",
      "100% 500/500 [02:58<00:00,  4.43it/s]\n",
      "100% 7/7 [00:00<00:00, 19.92it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-bert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-bert-base-uncased-config-default/checkpoint-500 (score: 0.7818247859284676).\n",
      "{'train_runtime': 183.7912, 'train_samples_per_second': 87.055, 'train_steps_per_second': 2.72, 'train_loss': 0.23530224454402923, 'epoch': 20.0}\n",
      "100% 500/500 [03:03<00:00,  4.43it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "100% 500/500 [03:04<00:00,  2.72it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 20.31it/s]\n",
      "f1 0.7818247859284676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8235    0.5833    0.6829        24\n",
      "           1     0.7347    0.7660    0.7500        47\n",
      "           2     0.8955    0.9302    0.9125       129\n",
      "\n",
      "    accuracy                         0.8500       200\n",
      "   macro avg     0.8179    0.7598    0.7818       200\n",
      "weighted avg     0.8491    0.8500    0.8468       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7322104559641609\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-21\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=bert-base-uncased --cross-validation=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NT8kShICYj_7",
    "outputId": "12ff4048-64e4-4d7b-f995-41cd8035b553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 18:48:04.783787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 18:48:06.687935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 18:48:06.688085: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 18:48:06.688107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 2.93kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 58.5kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 1.23MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 1.85MB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 40 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 40 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 198.94it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "Downloading pytorch_model.bin: 100% 268M/268M [00:01<00:00, 237MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/500 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.9455, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:05,  6.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7854638695716858, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.2334, 'eval_samples_per_second': 856.978, 'eval_steps_per_second': 29.994, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:05,  6.87it/s]\n",
      "100% 7/7 [00:00<00:00, 47.89it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "{'loss': 0.6806, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:43,  9.17it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5856997966766357, 'eval_f1': 0.521105196451204, 'eval_runtime': 0.228, 'eval_samples_per_second': 877.297, 'eval_steps_per_second': 30.705, 'epoch': 4.0}\n",
      " 20% 100/500 [00:17<00:43,  9.17it/s]\n",
      "100% 7/7 [00:00<00:00, 48.42it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4498, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:52,  6.72it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.513274073600769, 'eval_f1': 0.5412344822936831, 'eval_runtime': 0.2508, 'eval_samples_per_second': 797.601, 'eval_steps_per_second': 27.916, 'epoch': 6.0}\n",
      " 30% 150/500 [00:27<00:52,  6.72it/s]\n",
      "100% 7/7 [00:00<00:00, 44.66it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.3141, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:48,  6.15it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.502398669719696, 'eval_f1': 0.7007863076880332, 'eval_runtime': 0.2326, 'eval_samples_per_second': 859.873, 'eval_steps_per_second': 30.096, 'epoch': 8.0}\n",
      " 40% 200/500 [00:37<00:48,  6.15it/s]\n",
      "100% 7/7 [00:00<00:00, 46.27it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.2141, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:31,  7.90it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5041090846061707, 'eval_f1': 0.7287737474497721, 'eval_runtime': 0.2313, 'eval_samples_per_second': 864.539, 'eval_steps_per_second': 30.259, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:31,  7.90it/s]\n",
      "100% 7/7 [00:00<00:00, 46.47it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.149, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:35,  5.61it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5438546538352966, 'eval_f1': 0.7601601697416386, 'eval_runtime': 0.2448, 'eval_samples_per_second': 816.929, 'eval_steps_per_second': 28.593, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:35,  5.61it/s]\n",
      "100% 7/7 [00:00<00:00, 45.38it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.1095, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5282845497131348, 'eval_f1': 0.7371862171074769, 'eval_runtime': 0.2297, 'eval_samples_per_second': 870.773, 'eval_steps_per_second': 30.477, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.69it/s]\n",
      "100% 7/7 [00:00<00:00, 46.70it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0831, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:14,  6.81it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.544437050819397, 'eval_f1': 0.7703899459713414, 'eval_runtime': 0.2269, 'eval_samples_per_second': 881.361, 'eval_steps_per_second': 30.848, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:14,  6.81it/s]\n",
      "100% 7/7 [00:00<00:00, 47.48it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0694, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:26<00:06,  7.81it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5489913821220398, 'eval_f1': 0.7735136554621849, 'eval_runtime': 0.2449, 'eval_samples_per_second': 816.687, 'eval_steps_per_second': 28.584, 'epoch': 18.0}\n",
      " 90% 450/500 [01:26<00:06,  7.81it/s]\n",
      "100% 7/7 [00:00<00:00, 45.13it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.068, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  7.66it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5539283752441406, 'eval_f1': 0.7618902107779225, 'eval_runtime': 0.2285, 'eval_samples_per_second': 875.242, 'eval_steps_per_second': 30.633, 'epoch': 20.0}\n",
      "100% 500/500 [01:36<00:00,  7.66it/s]\n",
      "100% 7/7 [00:00<00:00, 47.20it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450 (score: 0.7735136554621849).\n",
      "{'train_runtime': 100.0875, 'train_samples_per_second': 159.86, 'train_steps_per_second': 4.996, 'train_loss': 0.30830076026916503, 'epoch': 20.0}\n",
      "100% 500/500 [01:39<00:00,  7.66it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "100% 500/500 [01:40<00:00,  4.97it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 24.51it/s]\n",
      "f1 0.7735136554621849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6154    0.6667    0.6400        12\n",
      "           1     0.8393    0.7460    0.7899        63\n",
      "           2     0.8702    0.9120    0.8906       125\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.7750    0.7749    0.7735       200\n",
      "weighted avg     0.8452    0.8450    0.8439       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9357, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:57,  7.80it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7672827243804932, 'eval_f1': 0.26506024096385544, 'eval_runtime': 0.2374, 'eval_samples_per_second': 842.637, 'eval_steps_per_second': 29.492, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:57,  7.80it/s]\n",
      "100% 7/7 [00:00<00:00, 34.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "{'loss': 0.6632, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:59,  6.72it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5878503322601318, 'eval_f1': 0.5350074483932752, 'eval_runtime': 0.2191, 'eval_samples_per_second': 912.652, 'eval_steps_per_second': 31.943, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:59,  6.72it/s]\n",
      "100% 7/7 [00:00<00:00, 36.69it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4467, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:48,  7.29it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.505496084690094, 'eval_f1': 0.5342481417953117, 'eval_runtime': 0.2243, 'eval_samples_per_second': 891.769, 'eval_steps_per_second': 31.212, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:48,  7.29it/s]\n",
      "100% 7/7 [00:00<00:00, 35.82it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "{'loss': 0.3086, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:40,  7.38it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.49563834071159363, 'eval_f1': 0.626396429589361, 'eval_runtime': 0.2401, 'eval_samples_per_second': 832.987, 'eval_steps_per_second': 29.155, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:40,  7.38it/s]\n",
      "100% 7/7 [00:00<00:00, 33.50it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1936, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:31,  8.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.4942978024482727, 'eval_f1': 0.6579782790309106, 'eval_runtime': 0.2188, 'eval_samples_per_second': 914.082, 'eval_steps_per_second': 31.993, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:31,  8.00it/s]\n",
      "100% 7/7 [00:00<00:00, 36.80it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1292, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:27,  7.35it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5189465880393982, 'eval_f1': 0.6688942927458105, 'eval_runtime': 0.2176, 'eval_samples_per_second': 918.95, 'eval_steps_per_second': 32.163, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:27,  7.35it/s]\n",
      "100% 7/7 [00:00<00:00, 36.57it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0965, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:23,  6.40it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5749669075012207, 'eval_f1': 0.6794795468211395, 'eval_runtime': 0.2183, 'eval_samples_per_second': 916.247, 'eval_steps_per_second': 32.069, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:23,  6.40it/s]\n",
      "100% 7/7 [00:00<00:00, 36.58it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0777, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:15<00:12,  8.04it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5993019342422485, 'eval_f1': 0.6723948531728853, 'eval_runtime': 0.2232, 'eval_samples_per_second': 896.151, 'eval_steps_per_second': 31.365, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:12,  8.04it/s]\n",
      "100% 7/7 [00:00<00:00, 36.29it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0672, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:05,  9.03it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.619157612323761, 'eval_f1': 0.6828434723171565, 'eval_runtime': 0.2272, 'eval_samples_per_second': 880.197, 'eval_steps_per_second': 30.807, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:05,  9.03it/s]\n",
      "100% 7/7 [00:00<00:00, 36.55it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0615, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6292106509208679, 'eval_f1': 0.6890365049068968, 'eval_runtime': 0.2377, 'eval_samples_per_second': 841.491, 'eval_steps_per_second': 29.452, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.69it/s]\n",
      "100% 7/7 [00:00<00:00, 35.26it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500 (score: 0.6890365049068968).\n",
      "{'train_runtime': 98.7268, 'train_samples_per_second': 162.063, 'train_steps_per_second': 5.064, 'train_loss': 0.297979519367218, 'epoch': 20.0}\n",
      "100% 500/500 [01:38<00:00,  8.69it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 67 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 67 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "100% 500/500 [01:39<00:00,  5.05it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 27.46it/s]\n",
      "f1 0.6890365049068968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.3571    0.4545        14\n",
      "           1     0.6885    0.7778    0.7304        54\n",
      "           2     0.8855    0.8788    0.8821       132\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.7330    0.6712    0.6890       200\n",
      "weighted avg     0.8141    0.8150    0.8112       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9291, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:56,  7.99it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7831921577453613, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2505, 'eval_samples_per_second': 798.289, 'eval_steps_per_second': 27.94, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:56,  7.99it/s]\n",
      "100% 7/7 [00:00<00:00, 30.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.6629, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:55,  7.20it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.595737636089325, 'eval_f1': 0.5027940220922678, 'eval_runtime': 0.2664, 'eval_samples_per_second': 750.816, 'eval_steps_per_second': 26.279, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:55,  7.20it/s]\n",
      "100% 7/7 [00:00<00:00, 29.51it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4478, 'learning_rate': 7.368421052631579e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:25<00:46,  7.46it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5186398029327393, 'eval_f1': 0.5926548397136632, 'eval_runtime': 0.2598, 'eval_samples_per_second': 769.792, 'eval_steps_per_second': 26.943, 'epoch': 6.0}\n",
      " 30% 150/500 [00:25<00:46,  7.46it/s]\n",
      "100% 7/7 [00:00<00:00, 28.69it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2994, 'learning_rate': 6.31578947368421e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:39,  7.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.4977152943611145, 'eval_f1': 0.6699626709168695, 'eval_runtime': 0.2621, 'eval_samples_per_second': 763.071, 'eval_steps_per_second': 26.707, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:39,  7.69it/s]\n",
      "100% 7/7 [00:00<00:00, 29.88it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1999, 'learning_rate': 5.263157894736842e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:32,  7.79it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.48632049560546875, 'eval_f1': 0.72032377204791, 'eval_runtime': 0.2693, 'eval_samples_per_second': 742.758, 'eval_steps_per_second': 25.997, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:32,  7.79it/s]\n",
      "100% 7/7 [00:00<00:00, 29.41it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1428, 'learning_rate': 4.210526315789474e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:54<00:27,  7.30it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5008935332298279, 'eval_f1': 0.7169324372594521, 'eval_runtime': 0.2504, 'eval_samples_per_second': 798.637, 'eval_steps_per_second': 27.952, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:27,  7.30it/s]\n",
      "100% 7/7 [00:00<00:00, 30.48it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.1066, 'learning_rate': 3.157894736842105e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:04<00:21,  6.86it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5194266438484192, 'eval_f1': 0.7160135155519353, 'eval_runtime': 0.257, 'eval_samples_per_second': 778.116, 'eval_steps_per_second': 27.234, 'epoch': 14.0}\n",
      " 70% 350/500 [01:04<00:21,  6.86it/s]\n",
      "100% 7/7 [00:00<00:00, 29.71it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0849, 'learning_rate': 2.105263157894737e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:13<00:13,  7.51it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5323021411895752, 'eval_f1': 0.7054975342835913, 'eval_runtime': 0.2657, 'eval_samples_per_second': 752.868, 'eval_steps_per_second': 26.35, 'epoch': 16.0}\n",
      " 80% 400/500 [01:14<00:13,  7.51it/s]\n",
      "100% 7/7 [00:00<00:00, 29.30it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0729, 'learning_rate': 1.0526315789473685e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:23<00:07,  6.64it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5388621687889099, 'eval_f1': 0.7103125630167373, 'eval_runtime': 0.2541, 'eval_samples_per_second': 786.985, 'eval_steps_per_second': 27.544, 'epoch': 18.0}\n",
      " 90% 450/500 [01:23<00:07,  6.64it/s]\n",
      "100% 7/7 [00:00<00:00, 30.04it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0668, 'learning_rate': 0.0, 'epoch': 20.0}\n",
      "100% 500/500 [01:33<00:00,  6.92it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5346307158470154, 'eval_f1': 0.6959357587676172, 'eval_runtime': 0.2547, 'eval_samples_per_second': 785.293, 'eval_steps_per_second': 27.485, 'epoch': 20.0}\n",
      "100% 500/500 [01:33<00:00,  6.92it/s]\n",
      "100% 7/7 [00:00<00:00, 29.42it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250 (score: 0.72032377204791).\n",
      "{'train_runtime': 96.7302, 'train_samples_per_second': 165.409, 'train_steps_per_second': 5.169, 'train_loss': 0.3013133473396301, 'epoch': 20.0}\n",
      "100% 500/500 [01:36<00:00,  6.92it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "100% 500/500 [01:37<00:00,  5.15it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 25.69it/s]\n",
      "f1 0.72032377204791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5833    0.4375    0.5000        16\n",
      "           1     0.7500    0.7636    0.7568        55\n",
      "           2     0.8939    0.9147    0.9042       129\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.7424    0.7053    0.7203       200\n",
      "weighted avg     0.8295    0.8350    0.8313       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9343, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:03,  7.08it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7929284572601318, 'eval_f1': 0.25766871165644173, 'eval_runtime': 0.2495, 'eval_samples_per_second': 801.509, 'eval_steps_per_second': 28.053, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:03,  7.08it/s]\n",
      "100% 7/7 [00:00<00:00, 25.72it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.6681, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<01:04,  6.23it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5881887674331665, 'eval_f1': 0.5195439045183291, 'eval_runtime': 0.261, 'eval_samples_per_second': 766.331, 'eval_steps_per_second': 26.822, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<01:04,  6.23it/s]\n",
      "100% 7/7 [00:00<00:00, 25.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4598, 'learning_rate': 7.368421052631579e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:43,  7.98it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5163566470146179, 'eval_f1': 0.5684412755000992, 'eval_runtime': 0.2667, 'eval_samples_per_second': 749.814, 'eval_steps_per_second': 26.244, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:43,  7.98it/s]\n",
      "100% 7/7 [00:00<00:00, 25.48it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.3095, 'learning_rate': 6.31578947368421e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:37,  8.04it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5145564079284668, 'eval_f1': 0.6373912454720877, 'eval_runtime': 0.2524, 'eval_samples_per_second': 792.437, 'eval_steps_per_second': 27.735, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:37,  8.04it/s]\n",
      "100% 7/7 [00:00<00:00, 25.24it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.203, 'learning_rate': 5.263157894736842e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:33,  7.41it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.48493674397468567, 'eval_f1': 0.6814225614977331, 'eval_runtime': 0.2568, 'eval_samples_per_second': 778.821, 'eval_steps_per_second': 27.259, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:33,  7.41it/s]\n",
      "100% 7/7 [00:00<00:00, 25.40it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1348, 'learning_rate': 4.210526315789474e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:27,  7.32it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.4963008463382721, 'eval_f1': 0.7181834542225144, 'eval_runtime': 0.2741, 'eval_samples_per_second': 729.696, 'eval_steps_per_second': 25.539, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:27,  7.32it/s]\n",
      "100% 7/7 [00:00<00:00, 25.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0919, 'learning_rate': 3.157894736842105e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:19,  7.61it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5390101075172424, 'eval_f1': 0.6901996156190875, 'eval_runtime': 0.2596, 'eval_samples_per_second': 770.324, 'eval_steps_per_second': 26.961, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:19,  7.61it/s]\n",
      "100% 7/7 [00:00<00:00, 25.53it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0705, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:15,  6.51it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5448956489562988, 'eval_f1': 0.7063714063714065, 'eval_runtime': 0.2535, 'eval_samples_per_second': 788.841, 'eval_steps_per_second': 27.609, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:15,  6.51it/s]\n",
      "100% 7/7 [00:00<00:00, 25.14it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0616, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:06,  7.15it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.569527804851532, 'eval_f1': 0.6992715494616636, 'eval_runtime': 0.2724, 'eval_samples_per_second': 734.316, 'eval_steps_per_second': 25.701, 'epoch': 18.0}\n",
      " 90% 450/500 [01:26<00:06,  7.15it/s]\n",
      "100% 7/7 [00:00<00:00, 24.88it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0553, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.18it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5649787187576294, 'eval_f1': 0.7160053686771244, 'eval_runtime': 0.2584, 'eval_samples_per_second': 773.919, 'eval_steps_per_second': 27.087, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.18it/s]\n",
      "100% 7/7 [00:00<00:00, 25.45it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300 (score: 0.7181834542225144).\n",
      "{'train_runtime': 99.1611, 'train_samples_per_second': 161.354, 'train_steps_per_second': 5.042, 'train_loss': 0.2988921298980713, 'epoch': 20.0}\n",
      "100% 500/500 [01:38<00:00,  8.18it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 62 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 62 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "100% 500/500 [01:39<00:00,  5.02it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 23.48it/s]\n",
      "f1 0.7181834542225144\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4615    0.4615    0.4615        13\n",
      "           1     0.8654    0.7377    0.7965        61\n",
      "           2     0.8667    0.9286    0.8966       126\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7312    0.7093    0.7182       200\n",
      "weighted avg     0.8399    0.8400    0.8377       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9208, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:06,  6.75it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.8251171708106995, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2208, 'eval_samples_per_second': 905.766, 'eval_steps_per_second': 31.702, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:06,  6.75it/s]\n",
      "100% 7/7 [00:00<00:00, 37.95it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.6513, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:52,  7.66it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6087048053741455, 'eval_f1': 0.49951838631083917, 'eval_runtime': 0.2072, 'eval_samples_per_second': 965.368, 'eval_steps_per_second': 33.788, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:52,  7.66it/s]\n",
      "100% 7/7 [00:00<00:00, 43.49it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4352, 'learning_rate': 7.368421052631579e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:38,  9.15it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5592221021652222, 'eval_f1': 0.5168012730383982, 'eval_runtime': 0.2165, 'eval_samples_per_second': 923.908, 'eval_steps_per_second': 32.337, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:38,  9.15it/s]\n",
      "100% 7/7 [00:00<00:00, 39.17it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.3022, 'learning_rate': 6.31578947368421e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:51,  5.82it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5376015305519104, 'eval_f1': 0.5953399171334758, 'eval_runtime': 0.2206, 'eval_samples_per_second': 906.575, 'eval_steps_per_second': 31.73, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:51,  5.82it/s]\n",
      "100% 7/7 [00:00<00:00, 38.33it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.2035, 'learning_rate': 5.263157894736842e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:30,  8.26it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5229143500328064, 'eval_f1': 0.621687337799505, 'eval_runtime': 0.2126, 'eval_samples_per_second': 940.638, 'eval_steps_per_second': 32.922, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:30,  8.26it/s]\n",
      "100% 7/7 [00:00<00:00, 39.73it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1432, 'learning_rate': 4.210526315789474e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:31,  6.29it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5789942145347595, 'eval_f1': 0.6991078519725606, 'eval_runtime': 0.2107, 'eval_samples_per_second': 949.262, 'eval_steps_per_second': 33.224, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:31,  6.29it/s]\n",
      "100% 7/7 [00:00<00:00, 39.62it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.1116, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.74it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5650026798248291, 'eval_f1': 0.7357703259420817, 'eval_runtime': 0.2244, 'eval_samples_per_second': 891.377, 'eval_steps_per_second': 31.198, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.74it/s]\n",
      "100% 7/7 [00:00<00:00, 38.91it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0884, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:14,  6.78it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5943700671195984, 'eval_f1': 0.7096066063873933, 'eval_runtime': 0.2122, 'eval_samples_per_second': 942.642, 'eval_steps_per_second': 32.992, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:14,  6.78it/s]\n",
      "100% 7/7 [00:00<00:00, 39.45it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0792, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:06,  8.10it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6055459380149841, 'eval_f1': 0.6911054693009581, 'eval_runtime': 0.2132, 'eval_samples_per_second': 938.005, 'eval_steps_per_second': 32.83, 'epoch': 18.0}\n",
      " 90% 450/500 [01:26<00:06,  8.10it/s]\n",
      "100% 7/7 [00:00<00:00, 39.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0727, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.22it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6102156043052673, 'eval_f1': 0.6947409371512813, 'eval_runtime': 0.2324, 'eval_samples_per_second': 860.444, 'eval_steps_per_second': 30.116, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.22it/s]\n",
      "100% 7/7 [00:00<00:00, 39.49it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-350 (score: 0.7357703259420817).\n",
      "{'train_runtime': 99.4566, 'train_samples_per_second': 160.874, 'train_steps_per_second': 5.027, 'train_loss': 0.30081275033950805, 'epoch': 20.0}\n",
      "100% 500/500 [01:39<00:00,  8.22it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 68 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 68 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "100% 500/500 [01:39<00:00,  5.01it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 33.86it/s]\n",
      "f1 0.7357703259420817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7222    0.5417    0.6190        24\n",
      "           1     0.6735    0.7021    0.6875        47\n",
      "           2     0.8872    0.9147    0.9008       129\n",
      "\n",
      "    accuracy                         0.8200       200\n",
      "   macro avg     0.7610    0.7195    0.7358       200\n",
      "weighted avg     0.8172    0.8200    0.8168       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7316084002625197\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-22\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDz10QcMY6Pg",
    "outputId": "aa160f9f-a7c4-47b9-c3b8-3fc789830b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 18:57:34.623703: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 18:57:36.711237: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 18:57:36.711379: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 18:57:36.711399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "Downloading (…)lve/main/config.json: 100% 480/480 [00:00<00:00, 63.5kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 2.89MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 1.82MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.34MB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 41 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 41 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 158.19it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "Downloading pytorch_model.bin: 100% 331M/331M [00:01<00:00, 205MB/s]\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 82120707\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/500 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.026, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:05,  6.83it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00, 49.07it/s]\u001B[A\n",
      "{'eval_loss': 0.7628674507141113, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.2826, 'eval_samples_per_second': 707.727, 'eval_steps_per_second': 24.77, 'epoch': 2.0}\n",
      "\n",
      " 10% 50/500 [00:07<01:05,  6.83it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "{'loss': 0.6186, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:17<00:46,  8.64it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5459136962890625, 'eval_f1': 0.5181581404491621, 'eval_runtime': 0.2692, 'eval_samples_per_second': 742.973, 'eval_steps_per_second': 26.004, 'epoch': 4.0}\n",
      " 20% 100/500 [00:18<00:46,  8.64it/s]\n",
      "100% 7/7 [00:00<00:00, 49.24it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3812, 'learning_rate': 7.431578947368422e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:28<00:51,  6.77it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00, 49.22it/s]\u001B[A\n",
      "{'eval_loss': 0.524772047996521, 'eval_f1': 0.6047306925049872, 'eval_runtime': 0.2714, 'eval_samples_per_second': 737.049, 'eval_steps_per_second': 25.797, 'epoch': 6.0}\n",
      "\n",
      " 30% 150/500 [00:29<00:51,  6.77it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.257, 'learning_rate': 6.378947368421053e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:39<00:44,  6.74it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5569646954536438, 'eval_f1': 0.6924015541700551, 'eval_runtime': 0.2699, 'eval_samples_per_second': 741.105, 'eval_steps_per_second': 25.939, 'epoch': 8.0}\n",
      " 40% 200/500 [00:39<00:44,  6.74it/s]\n",
      "100% 7/7 [00:00<00:00, 49.51it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1806, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:50<00:31,  7.96it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6475585103034973, 'eval_f1': 0.627874032908358, 'eval_runtime': 0.2818, 'eval_samples_per_second': 709.634, 'eval_steps_per_second': 24.837, 'epoch': 10.0}\n",
      " 50% 250/500 [00:50<00:31,  7.96it/s]\n",
      "100% 7/7 [00:00<00:00, 48.57it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.1512, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:01<00:35,  5.60it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7196460962295532, 'eval_f1': 0.6753873383086866, 'eval_runtime': 0.2665, 'eval_samples_per_second': 750.355, 'eval_steps_per_second': 26.262, 'epoch': 12.0}\n",
      " 60% 300/500 [01:01<00:35,  5.60it/s]\n",
      "100% 7/7 [00:00<00:00, 26.92it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.1099, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:12<00:27,  5.53it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6975569725036621, 'eval_f1': 0.6584598052278662, 'eval_runtime': 0.2739, 'eval_samples_per_second': 730.158, 'eval_steps_per_second': 25.556, 'epoch': 14.0}\n",
      " 70% 350/500 [01:12<00:27,  5.53it/s]\n",
      "100% 7/7 [00:00<00:00, 48.79it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0888, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:22<00:14,  6.96it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7183378338813782, 'eval_f1': 0.6614411184089409, 'eval_runtime': 0.2861, 'eval_samples_per_second': 699.134, 'eval_steps_per_second': 24.47, 'epoch': 16.0}\n",
      " 80% 400/500 [01:23<00:14,  6.96it/s]\n",
      "100% 7/7 [00:00<00:00, 47.71it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0737, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:33<00:06,  7.44it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.728687047958374, 'eval_f1': 0.6565185088302506, 'eval_runtime': 0.2704, 'eval_samples_per_second': 739.57, 'eval_steps_per_second': 25.885, 'epoch': 18.0}\n",
      " 90% 450/500 [01:33<00:06,  7.44it/s]\n",
      "100% 7/7 [00:00<00:00, 26.59it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilroberta-base-config-default/checkpoint-200 (score: 0.6924015541700551).\n",
      "{'train_runtime': 98.4485, 'train_samples_per_second': 162.521, 'train_steps_per_second': 5.079, 'train_loss': 0.32076073434617786, 'epoch': 18.0}\n",
      " 90% 450/500 [01:38<00:06,  7.44it/s]Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      " 90% 450/500 [01:38<00:10,  4.55it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 21.40it/s]\n",
      "f1 0.6924015541700551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4211    0.6667    0.5161        12\n",
      "           1     0.7321    0.6508    0.6891        63\n",
      "           2     0.8720    0.8720    0.8720       125\n",
      "\n",
      "    accuracy                         0.7900       200\n",
      "   macro avg     0.6751    0.7298    0.6924       200\n",
      "weighted avg     0.8009    0.7900    0.7930       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 82120707\n",
      "{'loss': 0.9353, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<00:58,  7.68it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7580346465110779, 'eval_f1': 0.26506024096385544, 'eval_runtime': 0.212, 'eval_samples_per_second': 943.338, 'eval_steps_per_second': 33.017, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<00:58,  7.68it/s]\n",
      "100% 7/7 [00:00<00:00, 39.33it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.6292, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:17<01:07,  5.90it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5467346906661987, 'eval_f1': 0.5390753512132823, 'eval_runtime': 0.231, 'eval_samples_per_second': 865.757, 'eval_steps_per_second': 30.302, 'epoch': 4.0}\n",
      " 20% 100/500 [00:18<01:07,  5.90it/s]\n",
      "100% 7/7 [00:00<00:00, 37.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4016, 'learning_rate': 7.431578947368422e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:29<00:55,  6.25it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5049620270729065, 'eval_f1': 0.5877715667220986, 'eval_runtime': 0.2133, 'eval_samples_per_second': 937.684, 'eval_steps_per_second': 32.819, 'epoch': 6.0}\n",
      " 30% 150/500 [00:29<00:55,  6.25it/s]\n",
      "100% 7/7 [00:00<00:00, 37.76it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2865, 'learning_rate': 6.378947368421053e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:40<00:41,  7.21it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.49969130754470825, 'eval_f1': 0.6806246545052516, 'eval_runtime': 0.2156, 'eval_samples_per_second': 927.57, 'eval_steps_per_second': 32.465, 'epoch': 8.0}\n",
      " 40% 200/500 [00:40<00:41,  7.21it/s]\n",
      "100% 7/7 [00:00<00:00, 38.29it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1988, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:51<00:30,  8.10it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5884413123130798, 'eval_f1': 0.6834039103881039, 'eval_runtime': 0.2154, 'eval_samples_per_second': 928.685, 'eval_steps_per_second': 32.504, 'epoch': 10.0}\n",
      " 50% 250/500 [00:51<00:30,  8.10it/s]\n",
      "100% 7/7 [00:00<00:00, 37.71it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.148, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:02<00:26,  7.59it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6036009192466736, 'eval_f1': 0.6862434639152196, 'eval_runtime': 0.2244, 'eval_samples_per_second': 891.103, 'eval_steps_per_second': 31.189, 'epoch': 12.0}\n",
      " 60% 300/500 [01:02<00:26,  7.59it/s]\n",
      "100% 7/7 [00:00<00:00, 38.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.107, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:13<00:23,  6.34it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6713445782661438, 'eval_f1': 0.6843145104014668, 'eval_runtime': 0.2097, 'eval_samples_per_second': 953.755, 'eval_steps_per_second': 33.381, 'epoch': 14.0}\n",
      " 70% 350/500 [01:14<00:23,  6.34it/s]\n",
      "100% 7/7 [00:00<00:00, 39.23it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.085, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:24<00:12,  7.69it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6994601488113403, 'eval_f1': 0.6697346931080537, 'eval_runtime': 0.2133, 'eval_samples_per_second': 937.483, 'eval_steps_per_second': 32.812, 'epoch': 16.0}\n",
      " 80% 400/500 [01:24<00:12,  7.69it/s]\n",
      "100% 7/7 [00:00<00:00, 39.20it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0686, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:35<00:05,  8.81it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.745701014995575, 'eval_f1': 0.6594512328647133, 'eval_runtime': 0.2171, 'eval_samples_per_second': 921.054, 'eval_steps_per_second': 32.237, 'epoch': 18.0}\n",
      " 90% 450/500 [01:35<00:05,  8.81it/s]\n",
      "100% 7/7 [00:00<00:00, 37.66it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0565, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:46<00:00,  8.29it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7478196620941162, 'eval_f1': 0.6488071023475077, 'eval_runtime': 0.2238, 'eval_samples_per_second': 893.513, 'eval_steps_per_second': 31.273, 'epoch': 20.0}\n",
      "100% 500/500 [01:46<00:00,  8.29it/s]\n",
      "100% 7/7 [00:00<00:00, 37.91it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilroberta-base-config-default/checkpoint-300 (score: 0.6862434639152196).\n",
      "{'train_runtime': 111.3996, 'train_samples_per_second': 143.627, 'train_steps_per_second': 4.488, 'train_loss': 0.2916501054763794, 'epoch': 20.0}\n",
      "100% 500/500 [01:51<00:00,  8.29it/s]Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "100% 500/500 [01:51<00:00,  4.47it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 30.32it/s]\n",
      "f1 0.6862434639152196\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.4286    0.4615        14\n",
      "           1     0.6724    0.7222    0.6964        54\n",
      "           2     0.9077    0.8939    0.9008       132\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.6934    0.6816    0.6862       200\n",
      "weighted avg     0.8156    0.8150    0.8148       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 82120707\n",
      "{'loss': 0.943, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<00:57,  7.78it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7450036406517029, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2097, 'eval_samples_per_second': 953.599, 'eval_steps_per_second': 33.376, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<00:57,  7.78it/s]\n",
      "100% 7/7 [00:00<00:00, 36.10it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.5969, 'learning_rate': 8.48421052631579e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:17<01:06,  6.06it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5603165626525879, 'eval_f1': 0.5266909729669006, 'eval_runtime': 0.2108, 'eval_samples_per_second': 948.931, 'eval_steps_per_second': 33.213, 'epoch': 4.0}\n",
      " 20% 100/500 [00:18<01:06,  6.06it/s]\n",
      "100% 7/7 [00:00<00:00, 35.37it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4082, 'learning_rate': 7.431578947368422e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:28<00:59,  5.91it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5607764720916748, 'eval_f1': 0.5135982515804202, 'eval_runtime': 0.2262, 'eval_samples_per_second': 884.288, 'eval_steps_per_second': 30.95, 'epoch': 6.0}\n",
      " 30% 150/500 [00:28<00:59,  5.91it/s]\n",
      "100% 7/7 [00:00<00:00, 34.77it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "{'loss': 0.2918, 'learning_rate': 6.378947368421053e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:39<00:45,  6.52it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5954891443252563, 'eval_f1': 0.6475878775383981, 'eval_runtime': 0.2118, 'eval_samples_per_second': 944.232, 'eval_steps_per_second': 33.048, 'epoch': 8.0}\n",
      " 40% 200/500 [00:39<00:45,  6.52it/s]\n",
      "100% 7/7 [00:00<00:00, 36.05it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.2097, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:50<00:31,  7.91it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6670294404029846, 'eval_f1': 0.6740884363230576, 'eval_runtime': 0.2058, 'eval_samples_per_second': 971.822, 'eval_steps_per_second': 34.014, 'epoch': 10.0}\n",
      " 50% 250/500 [00:51<00:31,  7.91it/s]\n",
      "100% 7/7 [00:00<00:00, 36.97it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1705, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:01<00:29,  6.77it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6324403882026672, 'eval_f1': 0.6539168433905275, 'eval_runtime': 0.2122, 'eval_samples_per_second': 942.504, 'eval_steps_per_second': 32.988, 'epoch': 12.0}\n",
      " 60% 300/500 [01:02<00:29,  6.77it/s]\n",
      "100% 7/7 [00:00<00:00, 35.75it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.1308, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:12<00:24,  6.24it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.647419273853302, 'eval_f1': 0.6636764324604615, 'eval_runtime': 0.2234, 'eval_samples_per_second': 895.423, 'eval_steps_per_second': 31.34, 'epoch': 14.0}\n",
      " 70% 350/500 [01:12<00:24,  6.24it/s]\n",
      "100% 7/7 [00:00<00:00, 34.81it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0975, 'learning_rate': 2.1894736842105264e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:23<00:13,  7.64it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6873353719711304, 'eval_f1': 0.6636764324604615, 'eval_runtime': 0.2085, 'eval_samples_per_second': 959.174, 'eval_steps_per_second': 33.571, 'epoch': 16.0}\n",
      " 80% 400/500 [01:23<00:13,  7.64it/s]\n",
      "100% 7/7 [00:00<00:00, 36.08it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0849, 'learning_rate': 1.136842105263158e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:34<00:07,  6.90it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6749629974365234, 'eval_f1': 0.6849291580836097, 'eval_runtime': 0.2073, 'eval_samples_per_second': 964.875, 'eval_steps_per_second': 33.771, 'epoch': 18.0}\n",
      " 90% 450/500 [01:34<00:07,  6.90it/s]\n",
      "100% 7/7 [00:00<00:00, 36.88it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0724, 'learning_rate': 8.421052631578947e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:45<00:00,  6.36it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6925190687179565, 'eval_f1': 0.679719709220168, 'eval_runtime': 0.2308, 'eval_samples_per_second': 866.465, 'eval_steps_per_second': 30.326, 'epoch': 20.0}\n",
      "100% 500/500 [01:45<00:00,  6.36it/s]\n",
      "100% 7/7 [00:00<00:00, 35.16it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilroberta-base-config-default/checkpoint-450 (score: 0.6849291580836097).\n",
      "{'train_runtime': 109.8464, 'train_samples_per_second': 145.658, 'train_steps_per_second': 4.552, 'train_loss': 0.30056046962738037, 'epoch': 20.0}\n",
      "100% 500/500 [01:49<00:00,  6.36it/s]Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "100% 500/500 [01:50<00:00,  4.54it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 29.89it/s]\n",
      "f1 0.6849291580836097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5385    0.4375    0.4828        16\n",
      "           1     0.6852    0.6727    0.6789        55\n",
      "           2     0.8797    0.9070    0.8931       129\n",
      "\n",
      "    accuracy                         0.8050       200\n",
      "   macro avg     0.7011    0.6724    0.6849       200\n",
      "weighted avg     0.7989    0.8050    0.8014       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 82120707\n",
      "{'loss': 0.9431, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:08,  6.60it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7503912448883057, 'eval_f1': 0.25766871165644173, 'eval_runtime': 0.2408, 'eval_samples_per_second': 830.4, 'eval_steps_per_second': 29.064, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:08,  6.60it/s]\n",
      "100% 7/7 [00:00<00:00, 26.59it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "{'loss': 0.599, 'learning_rate': 8.48421052631579e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:17<00:55,  7.22it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5173614025115967, 'eval_f1': 0.5445898210092159, 'eval_runtime': 0.243, 'eval_samples_per_second': 822.893, 'eval_steps_per_second': 28.801, 'epoch': 4.0}\n",
      " 20% 100/500 [00:17<00:55,  7.22it/s]\n",
      "100% 7/7 [00:00<00:00, 26.11it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4332, 'learning_rate': 7.431578947368422e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:28<00:47,  7.45it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5018906593322754, 'eval_f1': 0.5464246289709309, 'eval_runtime': 0.2635, 'eval_samples_per_second': 759.116, 'eval_steps_per_second': 26.569, 'epoch': 6.0}\n",
      " 30% 150/500 [00:28<00:47,  7.45it/s]\n",
      "100% 7/7 [00:00<00:00, 25.54it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.3173, 'learning_rate': 6.378947368421053e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:39<00:40,  7.45it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.507133424282074, 'eval_f1': 0.7146714715418571, 'eval_runtime': 0.2423, 'eval_samples_per_second': 825.547, 'eval_steps_per_second': 28.894, 'epoch': 8.0}\n",
      " 40% 200/500 [00:39<00:40,  7.45it/s]\n",
      "100% 7/7 [00:00<00:00, 26.83it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.2314, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:50<00:32,  7.74it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5490217804908752, 'eval_f1': 0.682087118565106, 'eval_runtime': 0.2408, 'eval_samples_per_second': 830.713, 'eval_steps_per_second': 29.075, 'epoch': 10.0}\n",
      " 50% 250/500 [00:50<00:32,  7.74it/s]\n",
      "100% 7/7 [00:00<00:00, 26.45it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.1845, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:01<00:28,  6.97it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.523871898651123, 'eval_f1': 0.6856606174084209, 'eval_runtime': 0.2425, 'eval_samples_per_second': 824.762, 'eval_steps_per_second': 28.867, 'epoch': 12.0}\n",
      " 60% 300/500 [01:01<00:28,  6.97it/s]\n",
      "100% 7/7 [00:00<00:00, 26.19it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.1429, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:12<00:19,  7.74it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5240163207054138, 'eval_f1': 0.7352201257861634, 'eval_runtime': 0.2582, 'eval_samples_per_second': 774.651, 'eval_steps_per_second': 27.113, 'epoch': 14.0}\n",
      " 70% 350/500 [01:12<00:19,  7.74it/s]\n",
      "100% 7/7 [00:00<00:00, 25.82it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.1134, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:23<00:19,  5.26it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5252469778060913, 'eval_f1': 0.7186819497954007, 'eval_runtime': 0.2432, 'eval_samples_per_second': 822.384, 'eval_steps_per_second': 28.783, 'epoch': 16.0}\n",
      " 80% 400/500 [01:23<00:19,  5.26it/s]\n",
      "100% 7/7 [00:00<00:00, 26.24it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.099, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:33<00:06,  7.52it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5432515740394592, 'eval_f1': 0.7177033492822966, 'eval_runtime': 0.2425, 'eval_samples_per_second': 824.835, 'eval_steps_per_second': 28.869, 'epoch': 18.0}\n",
      " 90% 450/500 [01:34<00:06,  7.52it/s]\n",
      "100% 7/7 [00:00<00:00, 26.32it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0865, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:44<00:00,  7.66it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5505973100662231, 'eval_f1': 0.7169377143489323, 'eval_runtime': 0.2418, 'eval_samples_per_second': 827.201, 'eval_steps_per_second': 28.952, 'epoch': 20.0}\n",
      "100% 500/500 [01:45<00:00,  7.66it/s]\n",
      "100% 7/7 [00:00<00:00, 26.29it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilroberta-base-config-default/checkpoint-350 (score: 0.7352201257861634).\n",
      "{'train_runtime': 109.1919, 'train_samples_per_second': 146.531, 'train_steps_per_second': 4.579, 'train_loss': 0.31502464389801027, 'epoch': 20.0}\n",
      "100% 500/500 [01:49<00:00,  7.66it/s]Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "100% 500/500 [01:49<00:00,  4.56it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 26.28it/s]\n",
      "f1 0.7352201257861634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.3846    0.5000        13\n",
      "           1     0.8519    0.7541    0.8000        61\n",
      "           2     0.8633    0.9524    0.9057       126\n",
      "\n",
      "    accuracy                         0.8550       200\n",
      "   macro avg     0.8098    0.6970    0.7352       200\n",
      "weighted avg     0.8501    0.8550    0.8471       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilroberta-base/snapshots/d5411c3ee9e1793fd9ef58390b40a80a4c10df32/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 82120707\n",
      "{'loss': 0.9338, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:06,  6.73it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7894744873046875, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2149, 'eval_samples_per_second': 930.69, 'eval_steps_per_second': 32.574, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:06,  6.73it/s]\n",
      "100% 7/7 [00:00<00:00, 34.14it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.568, 'learning_rate': 8.463157894736843e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:17<00:50,  7.91it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5362988114356995, 'eval_f1': 0.5017491254372813, 'eval_runtime': 0.2183, 'eval_samples_per_second': 916.184, 'eval_steps_per_second': 32.066, 'epoch': 4.0}\n",
      " 20% 100/500 [00:18<00:50,  7.91it/s]\n",
      "100% 7/7 [00:00<00:00, 33.65it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4116, 'learning_rate': 7.431578947368422e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:28<00:42,  8.22it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5212156176567078, 'eval_f1': 0.528888888888889, 'eval_runtime': 0.2204, 'eval_samples_per_second': 907.528, 'eval_steps_per_second': 31.763, 'epoch': 6.0}\n",
      " 30% 150/500 [00:29<00:42,  8.22it/s]\n",
      "100% 7/7 [00:00<00:00, 32.93it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.3121, 'learning_rate': 6.378947368421053e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:39<01:03,  4.75it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.4904648959636688, 'eval_f1': 0.529357543450497, 'eval_runtime': 0.2259, 'eval_samples_per_second': 885.509, 'eval_steps_per_second': 30.993, 'epoch': 8.0}\n",
      " 40% 200/500 [00:40<01:03,  4.75it/s]\n",
      "100% 7/7 [00:00<00:00, 33.58it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.2556, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:50<00:32,  7.77it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5052139163017273, 'eval_f1': 0.5096738529574351, 'eval_runtime': 0.2134, 'eval_samples_per_second': 937.134, 'eval_steps_per_second': 32.8, 'epoch': 10.0}\n",
      " 50% 250/500 [00:51<00:32,  7.77it/s]\n",
      "100% 7/7 [00:00<00:00, 35.96it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.1984, 'learning_rate': 4.2947368421052635e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:01<00:28,  7.14it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5524148344993591, 'eval_f1': 0.6430149447693307, 'eval_runtime': 0.213, 'eval_samples_per_second': 939.043, 'eval_steps_per_second': 32.866, 'epoch': 12.0}\n",
      " 60% 300/500 [01:01<00:28,  7.14it/s]\n",
      "100% 7/7 [00:00<00:00, 34.56it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.1558, 'learning_rate': 3.2421052631578945e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:12<00:26,  5.66it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5096881985664368, 'eval_f1': 0.7056381836399656, 'eval_runtime': 0.2136, 'eval_samples_per_second': 936.235, 'eval_steps_per_second': 32.768, 'epoch': 14.0}\n",
      " 70% 350/500 [01:12<00:26,  5.66it/s]\n",
      "100% 7/7 [00:00<00:00, 34.60it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.1319, 'learning_rate': 2.1894736842105264e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:23<00:15,  6.25it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5438880324363708, 'eval_f1': 0.6234701492537313, 'eval_runtime': 0.2221, 'eval_samples_per_second': 900.429, 'eval_steps_per_second': 31.515, 'epoch': 16.0}\n",
      " 80% 400/500 [01:23<00:15,  6.25it/s]\n",
      "100% 7/7 [00:00<00:00, 34.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.1118, 'learning_rate': 1.136842105263158e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:33<00:06,  7.82it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.527534008026123, 'eval_f1': 0.7039141414141414, 'eval_runtime': 0.2158, 'eval_samples_per_second': 926.982, 'eval_steps_per_second': 32.444, 'epoch': 18.0}\n",
      " 90% 450/500 [01:34<00:06,  7.82it/s]\n",
      "100% 7/7 [00:00<00:00, 34.57it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0974, 'learning_rate': 8.421052631578947e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:44<00:00,  8.31it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5386494994163513, 'eval_f1': 0.6672003876643564, 'eval_runtime': 0.2147, 'eval_samples_per_second': 931.481, 'eval_steps_per_second': 32.602, 'epoch': 20.0}\n",
      "100% 500/500 [01:45<00:00,  8.31it/s]\n",
      "100% 7/7 [00:00<00:00, 34.86it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilroberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilroberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilroberta-base-config-default/checkpoint-350 (score: 0.7056381836399656).\n",
      "{'train_runtime': 109.4212, 'train_samples_per_second': 146.224, 'train_steps_per_second': 4.569, 'train_loss': 0.3176449546813965, 'epoch': 20.0}\n",
      "100% 500/500 [01:49<00:00,  8.31it/s]Deleting older checkpoint [/content/results/finetuning-distilroberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "100% 500/500 [01:49<00:00,  4.55it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 30.30it/s]\n",
      "f1 0.7056381836399656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6471    0.4583    0.5366        24\n",
      "           1     0.6596    0.6596    0.6596        47\n",
      "           2     0.8971    0.9457    0.9208       129\n",
      "\n",
      "    accuracy                         0.8200       200\n",
      "   macro avg     0.7346    0.6879    0.7056       200\n",
      "weighted avg     0.8113    0.8200    0.8133       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7029712976969481\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-23\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilroberta-base --cross-validation=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9jkr3JwZZXM",
    "outputId": "4e5bba6a-a239-4b46-ddb2-675bc0a1e433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 19:08:16.040620: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 19:08:17.516714: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 19:08:17.516885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 19:08:17.516918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "Downloading (…)lve/main/config.json: 100% 684/684 [00:00<00:00, 94.9kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100% 760k/760k [00:00<00:00, 2.44MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 1.31M/1.31M [00:00<00:00, 4.20MB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 36 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 36 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 200.29it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "Downloading pytorch_model.bin: 100% 47.4M/47.4M [00:00<00:00, 204MB/s]\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 11685891\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/500 [00:00<?, ?it/s]You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.8199, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:15<02:39,  2.83it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.81it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 13.44it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.6463589668273926, 'eval_f1': 0.4918704034983105, 'eval_runtime': 0.8449, 'eval_samples_per_second': 236.717, 'eval_steps_per_second': 8.285, 'epoch': 2.0}\n",
      " 10% 50/500 [00:16<02:39,  2.83it/s]\n",
      "100% 7/7 [00:00<00:00,  7.03it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/special_tokens_map.json\n",
      "{'loss': 0.482, 'learning_rate': 8.463157894736843e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:30<01:28,  4.53it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.26it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 13.22it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5512901544570923, 'eval_f1': 0.5330153051083284, 'eval_runtime': 0.8499, 'eval_samples_per_second': 235.331, 'eval_steps_per_second': 8.237, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:28,  4.53it/s]\n",
      "100% 7/7 [00:00<00:00,  6.98it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.294, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:46<02:06,  2.77it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.57it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 13.32it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5461330413818359, 'eval_f1': 0.5308627450980392, 'eval_runtime': 0.8486, 'eval_samples_per_second': 235.677, 'eval_steps_per_second': 8.249, 'epoch': 6.0}\n",
      " 30% 150/500 [00:47<02:06,  2.77it/s]\n",
      "100% 7/7 [00:00<00:00,  7.01it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/special_tokens_map.json\n",
      "{'loss': 0.1775, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:38,  3.05it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.96it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 13.53it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5547854900360107, 'eval_f1': 0.7217435897435897, 'eval_runtime': 0.8284, 'eval_samples_per_second': 241.422, 'eval_steps_per_second': 8.45, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:38,  3.05it/s]\n",
      "100% 7/7 [00:00<00:00,  7.07it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-100] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1095, 'learning_rate': 5.305263157894738e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:18<01:07,  3.70it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 15.47it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00, 15.23it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6314218640327454, 'eval_f1': 0.6896592382973706, 'eval_runtime': 0.8143, 'eval_samples_per_second': 245.615, 'eval_steps_per_second': 8.597, 'epoch': 10.0}\n",
      " 50% 250/500 [01:19<01:07,  3.70it/s]\n",
      "100% 7/7 [00:00<00:00,  8.21it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.0636, 'learning_rate': 4.252631578947369e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:34<01:09,  2.89it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.80it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 13.68it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.689368486404419, 'eval_f1': 0.6231783040738584, 'eval_runtime': 0.8186, 'eval_samples_per_second': 244.328, 'eval_steps_per_second': 8.551, 'epoch': 12.0}\n",
      " 60% 300/500 [01:35<01:09,  2.89it/s]\n",
      "100% 7/7 [00:00<00:00,  7.14it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0397, 'learning_rate': 3.2000000000000003e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:50<00:58,  2.57it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.94it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 13.47it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7486506700515747, 'eval_f1': 0.6926934133233346, 'eval_runtime': 0.8224, 'eval_samples_per_second': 243.183, 'eval_steps_per_second': 8.511, 'epoch': 14.0}\n",
      " 70% 350/500 [01:51<00:58,  2.57it/s]\n",
      "100% 7/7 [00:00<00:00,  7.10it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0244, 'learning_rate': 2.1473684210526317e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:06<00:35,  2.84it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 15.33it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00, 14.99it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8407524228096008, 'eval_f1': 0.6349481083152213, 'eval_runtime': 0.822, 'eval_samples_per_second': 243.317, 'eval_steps_per_second': 8.516, 'epoch': 16.0}\n",
      " 80% 400/500 [02:07<00:35,  2.84it/s]\n",
      "100% 7/7 [00:00<00:00,  8.16it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0187, 'learning_rate': 1.0947368421052632e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:22<00:13,  3.68it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 15.46it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00, 15.13it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8777430653572083, 'eval_f1': 0.609652421279796, 'eval_runtime': 0.8215, 'eval_samples_per_second': 243.452, 'eval_steps_per_second': 8.521, 'epoch': 18.0}\n",
      " 90% 450/500 [02:23<00:13,  3.68it/s]\n",
      "100% 7/7 [00:00<00:00,  8.16it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-albert-base-v2-config-default/checkpoint-200 (score: 0.7217435897435897).\n",
      "{'train_runtime': 144.1998, 'train_samples_per_second': 110.957, 'train_steps_per_second': 3.467, 'train_loss': 0.22547732830047607, 'epoch': 18.0}\n",
      " 90% 450/500 [02:24<00:13,  3.68it/s]Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-450] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 65 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 65 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      " 90% 450/500 [02:24<00:16,  3.11it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00,  9.49it/s]\n",
      "f1 0.7217435897435897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.4167    0.5000        12\n",
      "           1     0.7463    0.7937    0.7692        63\n",
      "           2     0.8960    0.8960    0.8960       125\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.7558    0.7021    0.7217       200\n",
      "weighted avg     0.8326    0.8350    0.8323       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 11685891\n",
      "{'loss': 0.9063, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:15<01:59,  3.76it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.45it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.72it/s]\u001B[A\n",
      " 86% 6/7 [00:00<00:00, 11.61it/s]\u001B[A\n",
      "{'eval_loss': 0.6814587116241455, 'eval_f1': 0.4520739630184907, 'eval_runtime': 0.6311, 'eval_samples_per_second': 316.911, 'eval_steps_per_second': 11.092, 'epoch': 2.0}\n",
      "\n",
      " 10% 50/500 [00:16<01:59,  3.76it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.6413, 'learning_rate': 8.463157894736843e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:38,  2.52it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.30it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.56it/s]\u001B[A\n",
      " 86% 6/7 [00:00<00:00, 11.48it/s]\u001B[A\n",
      "{'eval_loss': 0.5991685390472412, 'eval_f1': 0.5107800387596899, 'eval_runtime': 0.6355, 'eval_samples_per_second': 314.719, 'eval_steps_per_second': 11.015, 'epoch': 4.0}\n",
      "\n",
      " 20% 100/500 [00:33<02:38,  2.52it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3635, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:49<02:09,  2.71it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.55it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.75it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6920290589332581, 'eval_f1': 0.42528103439957543, 'eval_runtime': 0.6242, 'eval_samples_per_second': 320.415, 'eval_steps_per_second': 11.215, 'epoch': 6.0}\n",
      " 30% 150/500 [00:50<02:09,  2.71it/s]\n",
      "100% 7/7 [00:00<00:00, 11.69it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/special_tokens_map.json\n",
      "{'loss': 0.2725, 'learning_rate': 6.4000000000000006e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:06<01:25,  3.49it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.38it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.47it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5498380064964294, 'eval_f1': 0.5875276736590606, 'eval_runtime': 0.6522, 'eval_samples_per_second': 306.673, 'eval_steps_per_second': 10.734, 'epoch': 8.0}\n",
      " 40% 200/500 [01:07<01:25,  3.49it/s]\n",
      "100% 7/7 [00:00<00:00, 11.32it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-100] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1915, 'learning_rate': 5.36842105263158e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:23<01:09,  3.60it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.30it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.57it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5795081257820129, 'eval_f1': 0.5741414141414141, 'eval_runtime': 0.647, 'eval_samples_per_second': 309.121, 'eval_steps_per_second': 10.819, 'epoch': 10.0}\n",
      " 50% 250/500 [01:24<01:09,  3.60it/s]\n",
      "100% 7/7 [00:00<00:00, 11.42it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.1275, 'learning_rate': 4.315789473684211e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:40<00:52,  3.80it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.61it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.77it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5804418325424194, 'eval_f1': 0.6639021356002487, 'eval_runtime': 0.6269, 'eval_samples_per_second': 319.019, 'eval_steps_per_second': 11.166, 'epoch': 12.0}\n",
      " 60% 300/500 [01:41<00:52,  3.80it/s]\n",
      "100% 7/7 [00:00<00:00, 11.61it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0785, 'learning_rate': 3.2631578947368423e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:58<00:56,  2.65it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.58it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.71it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6133583784103394, 'eval_f1': 0.6419342539638258, 'eval_runtime': 0.6312, 'eval_samples_per_second': 316.879, 'eval_steps_per_second': 11.091, 'epoch': 14.0}\n",
      " 70% 350/500 [01:58<00:56,  2.65it/s]\n",
      "100% 7/7 [00:00<00:00, 11.63it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0521, 'learning_rate': 2.2105263157894738e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:24,  4.01it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.65it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.85it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6311362385749817, 'eval_f1': 0.6346472241209083, 'eval_runtime': 0.6273, 'eval_samples_per_second': 318.807, 'eval_steps_per_second': 11.158, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:24,  4.01it/s]\n",
      "100% 7/7 [00:00<00:00, 11.64it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0348, 'learning_rate': 1.1578947368421053e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:30<00:11,  4.47it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.74it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.83it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6486940979957581, 'eval_f1': 0.6540805392429766, 'eval_runtime': 0.6298, 'eval_samples_per_second': 317.563, 'eval_steps_per_second': 11.115, 'epoch': 18.0}\n",
      " 90% 450/500 [02:31<00:11,  4.47it/s]\n",
      "100% 7/7 [00:00<00:00, 11.59it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0277, 'learning_rate': 1.0526315789473685e-07, 'epoch': 20.0}\n",
      "100% 500/500 [02:46<00:00,  3.99it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 14.33it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 12.45it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6594390273094177, 'eval_f1': 0.6632582087716633, 'eval_runtime': 0.6572, 'eval_samples_per_second': 304.301, 'eval_steps_per_second': 10.651, 'epoch': 20.0}\n",
      "100% 500/500 [02:47<00:00,  3.99it/s]\n",
      "100% 7/7 [00:00<00:00, 11.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-albert-base-v2-config-default/checkpoint-300 (score: 0.6639021356002487).\n",
      "{'train_runtime': 168.0992, 'train_samples_per_second': 95.182, 'train_steps_per_second': 2.974, 'train_loss': 0.2695756869316101, 'epoch': 20.0}\n",
      "100% 500/500 [02:47<00:00,  3.99it/s]Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 50 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 50 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "100% 500/500 [02:48<00:00,  2.97it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 11.18it/s]\n",
      "f1 0.6639021356002487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.2857    0.3810        14\n",
      "           1     0.7308    0.7037    0.7170        54\n",
      "           2     0.8652    0.9242    0.8938       132\n",
      "\n",
      "    accuracy                         0.8200       200\n",
      "   macro avg     0.7225    0.6379    0.6639       200\n",
      "weighted avg     0.8084    0.8200    0.8101       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 11685891\n",
      "{'loss': 0.8904, 'learning_rate': 9.515789473684212e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:15<02:06,  3.56it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.06it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.6717590093612671, 'eval_f1': 0.4810327749295191, 'eval_runtime': 0.7361, 'eval_samples_per_second': 271.708, 'eval_steps_per_second': 9.51, 'epoch': 2.0}\n",
      " 10% 50/500 [00:16<02:06,  3.56it/s]\n",
      "100% 7/7 [00:00<00:00,  8.75it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.5262, 'learning_rate': 8.463157894736843e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<02:27,  2.71it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 19.69it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00,  8.95it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5160919427871704, 'eval_f1': 0.5787141229751883, 'eval_runtime': 0.733, 'eval_samples_per_second': 272.863, 'eval_steps_per_second': 9.55, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:27,  2.71it/s]\n",
      "100% 7/7 [00:00<00:00, 10.61it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3094, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:47<02:09,  2.70it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.04it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5287745594978333, 'eval_f1': 0.6380392156862745, 'eval_runtime': 0.7277, 'eval_samples_per_second': 274.851, 'eval_steps_per_second': 9.62, 'epoch': 6.0}\n",
      " 30% 150/500 [00:48<02:09,  2.70it/s]\n",
      "100% 7/7 [00:00<00:00,  8.86it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.194, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:37,  3.08it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.25it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5600897073745728, 'eval_f1': 0.7068008327550311, 'eval_runtime': 0.7272, 'eval_samples_per_second': 275.035, 'eval_steps_per_second': 9.626, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:37,  3.08it/s]\n",
      "100% 7/7 [00:00<00:00,  8.84it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.112, 'learning_rate': 5.305263157894738e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:19<01:09,  3.60it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 19.39it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00,  8.84it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6349733471870422, 'eval_f1': 0.6625485625485626, 'eval_runtime': 0.7492, 'eval_samples_per_second': 266.95, 'eval_steps_per_second': 9.343, 'epoch': 10.0}\n",
      " 50% 250/500 [01:20<01:09,  3.60it/s]\n",
      "100% 7/7 [00:00<00:00, 10.38it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.0623, 'learning_rate': 4.252631578947369e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:36<01:05,  3.04it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 19.52it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00,  8.84it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.691847562789917, 'eval_f1': 0.6890659499604453, 'eval_runtime': 0.7569, 'eval_samples_per_second': 264.227, 'eval_steps_per_second': 9.248, 'epoch': 12.0}\n",
      " 60% 300/500 [01:36<01:05,  3.04it/s]\n",
      "100% 7/7 [00:00<00:00, 10.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0321, 'learning_rate': 3.2000000000000003e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:52<00:51,  2.92it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 19.36it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00,  8.86it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8284808993339539, 'eval_f1': 0.6839074368486133, 'eval_runtime': 0.7653, 'eval_samples_per_second': 261.33, 'eval_steps_per_second': 9.147, 'epoch': 14.0}\n",
      " 70% 350/500 [01:52<00:51,  2.92it/s]\n",
      "100% 7/7 [00:00<00:00, 10.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.017, 'learning_rate': 2.1473684210526317e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:07<00:28,  3.48it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.07it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9137826561927795, 'eval_f1': 0.6711590162549507, 'eval_runtime': 0.7257, 'eval_samples_per_second': 275.579, 'eval_steps_per_second': 9.645, 'epoch': 16.0}\n",
      " 80% 400/500 [02:08<00:28,  3.48it/s]\n",
      "100% 7/7 [00:00<00:00,  8.86it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0101, 'learning_rate': 1.0947368421052632e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:23<00:15,  3.13it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.07it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9330825805664062, 'eval_f1': 0.678304854728271, 'eval_runtime': 0.7305, 'eval_samples_per_second': 273.771, 'eval_steps_per_second': 9.582, 'epoch': 18.0}\n",
      " 90% 450/500 [02:24<00:15,  3.13it/s]\n",
      "100% 7/7 [00:00<00:00,  8.81it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-albert-base-v2-config-default/checkpoint-200 (score: 0.7068008327550311).\n",
      "{'train_runtime': 144.6965, 'train_samples_per_second': 110.576, 'train_steps_per_second': 3.456, 'train_loss': 0.23927593496110705, 'epoch': 18.0}\n",
      " 90% 450/500 [02:24<00:15,  3.13it/s]Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-450] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 67 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 67 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      " 90% 450/500 [02:25<00:16,  3.10it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 10.92it/s]\n",
      "f1 0.7068008327550311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5833    0.4375    0.5000        16\n",
      "           1     0.7273    0.7273    0.7273        55\n",
      "           2     0.8797    0.9070    0.8931       129\n",
      "\n",
      "    accuracy                         0.8200       200\n",
      "   macro avg     0.7301    0.6906    0.7068       200\n",
      "weighted avg     0.8141    0.8200    0.8161       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 11685891\n",
      "{'loss': 0.9166, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:15<02:34,  2.91it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.05it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  7.97it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7110510468482971, 'eval_f1': 0.4522317188983856, 'eval_runtime': 0.7529, 'eval_samples_per_second': 265.636, 'eval_steps_per_second': 9.297, 'epoch': 2.0}\n",
      " 10% 50/500 [00:15<02:34,  2.91it/s]\n",
      "100% 7/7 [00:00<00:00,  9.16it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.5579, 'learning_rate': 8.463157894736843e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<02:07,  3.13it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.12it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  7.81it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5639836192131042, 'eval_f1': 0.5354096692111959, 'eval_runtime': 0.7621, 'eval_samples_per_second': 262.446, 'eval_steps_per_second': 9.186, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:07,  3.13it/s]\n",
      "100% 7/7 [00:00<00:00,  9.04it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3671, 'learning_rate': 7.431578947368422e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:48<01:34,  3.71it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.52it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  8.04it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5540385246276855, 'eval_f1': 0.5356539575289575, 'eval_runtime': 0.7346, 'eval_samples_per_second': 272.268, 'eval_steps_per_second': 9.529, 'epoch': 6.0}\n",
      " 30% 150/500 [00:49<01:34,  3.71it/s]\n",
      "100% 7/7 [00:00<00:00,  9.30it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2627, 'learning_rate': 6.378947368421053e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:04<01:26,  3.48it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.44it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  8.08it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.547025740146637, 'eval_f1': 0.6049475970163346, 'eval_runtime': 0.7301, 'eval_samples_per_second': 273.917, 'eval_steps_per_second': 9.587, 'epoch': 8.0}\n",
      " 40% 200/500 [01:05<01:26,  3.48it/s]\n",
      "100% 7/7 [00:00<00:00,  9.34it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.178, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<01:17,  3.25it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.18it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  7.96it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5896058678627014, 'eval_f1': 0.6344268476621417, 'eval_runtime': 0.7375, 'eval_samples_per_second': 271.172, 'eval_steps_per_second': 9.491, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<01:17,  3.25it/s]\n",
      "100% 7/7 [00:00<00:00,  9.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1121, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:38<01:05,  3.06it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.43it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  8.01it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5824869275093079, 'eval_f1': 0.6258642828371843, 'eval_runtime': 0.7344, 'eval_samples_per_second': 272.336, 'eval_steps_per_second': 9.532, 'epoch': 12.0}\n",
      " 60% 300/500 [01:38<01:05,  3.06it/s]\n",
      "100% 7/7 [00:00<00:00,  9.26it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0671, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:54<00:40,  3.75it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.31it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  8.04it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6189236640930176, 'eval_f1': 0.6329244751739113, 'eval_runtime': 0.7335, 'eval_samples_per_second': 272.655, 'eval_steps_per_second': 9.543, 'epoch': 14.0}\n",
      " 70% 350/500 [01:55<00:40,  3.75it/s]\n",
      "100% 7/7 [00:00<00:00,  9.27it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0417, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:10<00:45,  2.19it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.47it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  7.99it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.636690080165863, 'eval_f1': 0.6445022771633052, 'eval_runtime': 0.7456, 'eval_samples_per_second': 268.241, 'eval_steps_per_second': 9.388, 'epoch': 16.0}\n",
      " 80% 400/500 [02:11<00:45,  2.19it/s]\n",
      "100% 7/7 [00:00<00:00,  9.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-250] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0288, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:27<00:15,  3.18it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.27it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  8.02it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6426060199737549, 'eval_f1': 0.6526630895701607, 'eval_runtime': 0.7421, 'eval_samples_per_second': 269.496, 'eval_steps_per_second': 9.432, 'epoch': 18.0}\n",
      " 90% 450/500 [02:27<00:15,  3.18it/s]\n",
      "100% 7/7 [00:00<00:00,  9.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0234, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:43<00:00,  3.48it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 19.28it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00,  7.87it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6744217872619629, 'eval_f1': 0.6526630895701607, 'eval_runtime': 0.7594, 'eval_samples_per_second': 263.358, 'eval_steps_per_second': 9.218, 'epoch': 20.0}\n",
      "100% 500/500 [02:43<00:00,  3.48it/s]\n",
      "100% 7/7 [00:00<00:00,  9.10it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-albert-base-v2-config-default/checkpoint-450 (score: 0.6526630895701607).\n",
      "{'train_runtime': 164.5173, 'train_samples_per_second': 97.254, 'train_steps_per_second': 3.039, 'train_loss': 0.2555390622615814, 'epoch': 20.0}\n",
      "100% 500/500 [02:44<00:00,  3.48it/s]Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 64 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 64 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "100% 500/500 [02:44<00:00,  3.03it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 10.60it/s]\n",
      "f1 0.6526630895701607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3333    0.3846    0.3571        13\n",
      "           1     0.8261    0.6230    0.7103        61\n",
      "           2     0.8489    0.9365    0.8906       126\n",
      "\n",
      "    accuracy                         0.8050       200\n",
      "   macro avg     0.6694    0.6480    0.6527       200\n",
      "weighted avg     0.8084    0.8050    0.8009       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 11685891\n",
      "{'loss': 0.8641, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:15<02:22,  3.15it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.69it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.49it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7550542950630188, 'eval_f1': 0.38725146198830407, 'eval_runtime': 0.6388, 'eval_samples_per_second': 313.111, 'eval_steps_per_second': 10.959, 'epoch': 2.0}\n",
      " 10% 50/500 [00:16<02:22,  3.15it/s]\n",
      "100% 7/7 [00:00<00:00, 12.74it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-450] due to args.save_total_limit\n",
      "{'loss': 0.4731, 'learning_rate': 8.48421052631579e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<01:48,  3.69it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.64it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.50it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.599153459072113, 'eval_f1': 0.5168012730383982, 'eval_runtime': 0.6483, 'eval_samples_per_second': 308.519, 'eval_steps_per_second': 10.798, 'epoch': 4.0}\n",
      " 20% 100/500 [00:33<01:48,  3.69it/s]\n",
      "100% 7/7 [00:00<00:00, 12.59it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.3063, 'learning_rate': 7.4526315789473695e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:49<01:22,  4.23it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.75it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.38it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6082990765571594, 'eval_f1': 0.5129312169312169, 'eval_runtime': 0.6519, 'eval_samples_per_second': 306.781, 'eval_steps_per_second': 10.737, 'epoch': 6.0}\n",
      " 30% 150/500 [00:50<01:22,  4.23it/s]\n",
      "100% 7/7 [00:00<00:00, 12.42it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-150/special_tokens_map.json\n",
      "{'loss': 0.2093, 'learning_rate': 6.4000000000000006e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:06<02:28,  2.03it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.58it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.22it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5728663802146912, 'eval_f1': 0.5474461597103106, 'eval_runtime': 0.6471, 'eval_samples_per_second': 309.08, 'eval_steps_per_second': 10.818, 'epoch': 8.0}\n",
      " 40% 200/500 [01:06<02:28,  2.03it/s]\n",
      "100% 7/7 [00:00<00:00, 12.59it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-100] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.1482, 'learning_rate': 5.3473684210526325e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:22<01:08,  3.64it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.56it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.50it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5898761749267578, 'eval_f1': 0.5923004214371504, 'eval_runtime': 0.6462, 'eval_samples_per_second': 309.495, 'eval_steps_per_second': 10.832, 'epoch': 10.0}\n",
      " 50% 250/500 [01:23<01:08,  3.64it/s]\n",
      "100% 7/7 [00:00<00:00, 12.76it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0933, 'learning_rate': 4.2947368421052635e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:39<01:19,  2.51it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.77it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.65it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6136138439178467, 'eval_f1': 0.6234866343777234, 'eval_runtime': 0.6343, 'eval_samples_per_second': 315.305, 'eval_steps_per_second': 11.036, 'epoch': 12.0}\n",
      " 60% 300/500 [01:40<01:19,  2.51it/s]\n",
      "100% 7/7 [00:00<00:00, 12.86it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0574, 'learning_rate': 3.2421052631578945e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:57,  2.59it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.64it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.48it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6178897023200989, 'eval_f1': 0.6792532898009571, 'eval_runtime': 0.6373, 'eval_samples_per_second': 313.803, 'eval_steps_per_second': 10.983, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:57,  2.59it/s]\n",
      "100% 7/7 [00:00<00:00, 12.76it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0391, 'learning_rate': 2.1894736842105264e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:12<00:37,  2.65it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.58it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.58it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6583541631698608, 'eval_f1': 0.694665231798437, 'eval_runtime': 0.636, 'eval_samples_per_second': 314.479, 'eval_steps_per_second': 11.007, 'epoch': 16.0}\n",
      " 80% 400/500 [02:13<00:37,  2.65it/s]\n",
      "100% 7/7 [00:00<00:00, 12.73it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.025, 'learning_rate': 1.136842105263158e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:28<00:12,  3.89it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.72it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.62it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6968827843666077, 'eval_f1': 0.694665231798437, 'eval_runtime': 0.635, 'eval_samples_per_second': 314.971, 'eval_steps_per_second': 11.024, 'epoch': 18.0}\n",
      " 90% 450/500 [02:29<00:12,  3.89it/s]\n",
      "100% 7/7 [00:00<00:00, 12.89it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-450/special_tokens_map.json\n",
      "{'loss': 0.0179, 'learning_rate': 8.421052631578947e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:44<00:00,  3.81it/s]The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 29% 2/7 [00:00<00:00, 13.79it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 11.66it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7096290588378906, 'eval_f1': 0.6826073941026792, 'eval_runtime': 0.6338, 'eval_samples_per_second': 315.569, 'eval_steps_per_second': 11.045, 'epoch': 20.0}\n",
      "100% 500/500 [02:45<00:00,  3.81it/s]\n",
      "100% 7/7 [00:00<00:00, 12.88it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-albert-base-v2-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-albert-base-v2-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-albert-base-v2-config-default/checkpoint-400 (score: 0.694665231798437).\n",
      "{'train_runtime': 166.0191, 'train_samples_per_second': 96.374, 'train_steps_per_second': 3.012, 'train_loss': 0.22338427448272705, 'epoch': 20.0}\n",
      "100% 500/500 [02:45<00:00,  3.81it/s]Deleting older checkpoint [/content/results/finetuning-albert-base-v2-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 72 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 72 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "100% 500/500 [02:46<00:00,  3.01it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 13.13it/s]\n",
      "f1 0.694665231798437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.4167    0.5263        24\n",
      "           1     0.6111    0.7021    0.6535        47\n",
      "           2     0.8939    0.9147    0.9042       129\n",
      "\n",
      "    accuracy                         0.8050       200\n",
      "   macro avg     0.7398    0.6778    0.6947       200\n",
      "weighted avg     0.8059    0.8050    0.7999       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.6910568100725255\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-25\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=albert-base-v2 --cross-validation=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8zX2JgRZfuX",
    "outputId": "ee69604a-ace7-4a9f-8166-64d06b63c5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 19:40:07.123940: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 19:40:08.134542: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 19:40:08.134669: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 19:40:08.134691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "Downloading (…)lve/main/config.json: 100% 558/558 [00:00<00:00, 70.6kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 843k/843k [00:00<00:00, 2.71MB/s]\n",
      "Downloading (…)solve/main/bpe.codes: 100% 1.08M/1.08M [00:00<00:00, 3.46MB/s]\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 36 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 36 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 199.91it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "Downloading pytorch_model.bin: 100% 543M/543M [00:02<00:00, 244MB/s]\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 134902275\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "{'loss': 0.9648, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:11<01:39,  4.51it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.81it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.8016040325164795, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.3541, 'eval_samples_per_second': 564.851, 'eval_steps_per_second': 19.77, 'epoch': 2.0}\n",
      " 10% 50/500 [00:11<01:39,  4.51it/s]\n",
      "100% 7/7 [00:00<00:00, 21.51it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/added_tokens.json\n",
      "{'loss': 0.7267, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:19,  5.05it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.19it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6399438381195068, 'eval_f1': 0.48971807628524044, 'eval_runtime': 0.352, 'eval_samples_per_second': 568.136, 'eval_steps_per_second': 19.885, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:19,  5.05it/s]\n",
      "100% 7/7 [00:00<00:00, 21.28it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.5262, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:46<01:21,  4.31it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.86it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6122415065765381, 'eval_f1': 0.5046649703138253, 'eval_runtime': 0.3604, 'eval_samples_per_second': 554.991, 'eval_steps_per_second': 19.425, 'epoch': 6.0}\n",
      " 30% 150/500 [00:46<01:21,  4.31it/s]\n",
      "100% 7/7 [00:00<00:00, 20.75it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.4137, 'learning_rate': 6.378947368421053e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:04,  4.69it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.61it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.583749532699585, 'eval_f1': 0.5221984732824427, 'eval_runtime': 0.371, 'eval_samples_per_second': 539.142, 'eval_steps_per_second': 18.87, 'epoch': 8.0}\n",
      " 40% 200/500 [01:04<01:04,  4.69it/s]\n",
      "100% 7/7 [00:00<00:00, 20.84it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.336, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:54,  4.58it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.13it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6666074991226196, 'eval_f1': 0.5490093847758081, 'eval_runtime': 0.3723, 'eval_samples_per_second': 537.172, 'eval_steps_per_second': 18.801, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:54,  4.58it/s]\n",
      "100% 7/7 [00:00<00:00, 21.05it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.2578, 'learning_rate': 4.2947368421052635e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:39<00:42,  4.69it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.99it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6546488404273987, 'eval_f1': 0.6168847071896715, 'eval_runtime': 0.3595, 'eval_samples_per_second': 556.257, 'eval_steps_per_second': 19.469, 'epoch': 12.0}\n",
      " 60% 300/500 [01:39<00:42,  4.69it/s]\n",
      "100% 7/7 [00:00<00:00, 21.50it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.2111, 'learning_rate': 3.2631578947368423e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:57<00:34,  4.30it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.77it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6996365189552307, 'eval_f1': 0.6708485451930524, 'eval_runtime': 0.35, 'eval_samples_per_second': 571.43, 'eval_steps_per_second': 20.0, 'epoch': 14.0}\n",
      " 70% 350/500 [01:57<00:34,  4.30it/s]\n",
      "100% 7/7 [00:00<00:00, 21.65it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.1628, 'learning_rate': 2.2105263157894738e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:21,  4.70it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.30it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7194895148277283, 'eval_f1': 0.6842684902386394, 'eval_runtime': 0.3526, 'eval_samples_per_second': 567.224, 'eval_steps_per_second': 19.853, 'epoch': 16.0}\n",
      " 80% 400/500 [02:15<00:21,  4.70it/s]\n",
      "100% 7/7 [00:00<00:00, 21.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.1399, 'learning_rate': 1.1578947368421053e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:32<00:10,  4.90it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.81it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.722618043422699, 'eval_f1': 0.6845155096992593, 'eval_runtime': 0.362, 'eval_samples_per_second': 552.465, 'eval_steps_per_second': 19.336, 'epoch': 18.0}\n",
      " 90% 450/500 [02:32<00:10,  4.90it/s]\n",
      "100% 7/7 [00:00<00:00, 21.27it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.1181, 'learning_rate': 1.0526315789473685e-07, 'epoch': 20.0}\n",
      "100% 500/500 [02:50<00:00,  4.59it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.60it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7196663022041321, 'eval_f1': 0.7275273745861982, 'eval_runtime': 0.3719, 'eval_samples_per_second': 537.836, 'eval_steps_per_second': 18.824, 'epoch': 20.0}\n",
      "100% 500/500 [02:50<00:00,  4.59it/s]\n",
      "100% 7/7 [00:00<00:00, 20.78it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500 (score: 0.7275273745861982).\n",
      "{'train_runtime': 177.5393, 'train_samples_per_second': 90.121, 'train_steps_per_second': 2.816, 'train_loss': 0.38570705699920654, 'epoch': 20.0}\n",
      "100% 500/500 [02:57<00:00,  4.59it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "100% 500/500 [02:57<00:00,  2.81it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 20.74it/s]\n",
      "f1 0.7275273745861982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.5833    0.6364        12\n",
      "           1     0.7321    0.6508    0.6891        63\n",
      "           2     0.8284    0.8880    0.8571       125\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.7535    0.7074    0.7275       200\n",
      "weighted avg     0.7903    0.7950    0.7910       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 134902275\n",
      "{'loss': 0.951, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:10<01:31,  4.91it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 28.32it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.789935290813446, 'eval_f1': 0.26506024096385544, 'eval_runtime': 0.3584, 'eval_samples_per_second': 557.975, 'eval_steps_per_second': 19.529, 'epoch': 2.0}\n",
      " 10% 50/500 [00:11<01:31,  4.91it/s]\n",
      "100% 7/7 [00:00<00:00, 21.17it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.7189, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:24,  4.73it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.85it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5683495998382568, 'eval_f1': 0.5300660410149461, 'eval_runtime': 0.3614, 'eval_samples_per_second': 553.335, 'eval_steps_per_second': 19.367, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:24,  4.73it/s]\n",
      "100% 7/7 [00:00<00:00, 20.96it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.5189, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:45<01:13,  4.76it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.94it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5185080170631409, 'eval_f1': 0.5371008782730035, 'eval_runtime': 0.3639, 'eval_samples_per_second': 549.58, 'eval_steps_per_second': 19.235, 'epoch': 6.0}\n",
      " 30% 150/500 [00:46<01:13,  4.76it/s]\n",
      "100% 7/7 [00:00<00:00, 20.62it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.3833, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:03,  4.74it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.88it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5230919122695923, 'eval_f1': 0.5832285115303983, 'eval_runtime': 0.3817, 'eval_samples_per_second': 523.934, 'eval_steps_per_second': 18.338, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:03,  4.74it/s]\n",
      "100% 7/7 [00:00<00:00, 20.56it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.2777, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:54,  4.62it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.38it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.574370265007019, 'eval_f1': 0.6138468276811413, 'eval_runtime': 0.3826, 'eval_samples_per_second': 522.705, 'eval_steps_per_second': 18.295, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:54,  4.62it/s]\n",
      "100% 7/7 [00:00<00:00, 20.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.216, 'learning_rate': 4.252631578947369e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:38<00:44,  4.54it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.13it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6471328139305115, 'eval_f1': 0.5960771493952272, 'eval_runtime': 0.3661, 'eval_samples_per_second': 546.319, 'eval_steps_per_second': 19.121, 'epoch': 12.0}\n",
      " 60% 300/500 [01:39<00:44,  4.54it/s]\n",
      "100% 7/7 [00:00<00:00, 20.64it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/added_tokens.json\n",
      "{'loss': 0.1685, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:32,  4.59it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.52it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6561257839202881, 'eval_f1': 0.5916724600935127, 'eval_runtime': 0.3643, 'eval_samples_per_second': 549.036, 'eval_steps_per_second': 19.216, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:32,  4.59it/s]\n",
      "100% 7/7 [00:00<00:00, 20.80it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.1363, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:13<00:20,  4.92it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 28.15it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6526807546615601, 'eval_f1': 0.6150746710410804, 'eval_runtime': 0.3606, 'eval_samples_per_second': 554.65, 'eval_steps_per_second': 19.413, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:20,  4.92it/s]\n",
      "100% 7/7 [00:00<00:00, 20.97it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.1144, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:30<00:10,  4.90it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 27.99it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6435608863830566, 'eval_f1': 0.6386075499773807, 'eval_runtime': 0.3808, 'eval_samples_per_second': 525.266, 'eval_steps_per_second': 18.384, 'epoch': 18.0}\n",
      " 90% 450/500 [02:31<00:10,  4.90it/s]\n",
      "100% 7/7 [00:00<00:00, 20.17it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.1036, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:48<00:00,  4.86it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.79it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6592851877212524, 'eval_f1': 0.6642646625405245, 'eval_runtime': 0.383, 'eval_samples_per_second': 522.142, 'eval_steps_per_second': 18.275, 'epoch': 20.0}\n",
      "100% 500/500 [02:48<00:00,  4.86it/s]\n",
      "100% 7/7 [00:00<00:00, 20.14it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500 (score: 0.6642646625405245).\n",
      "{'train_runtime': 175.4558, 'train_samples_per_second': 91.191, 'train_steps_per_second': 2.85, 'train_loss': 0.35885368633270265, 'epoch': 20.0}\n",
      "100% 500/500 [02:55<00:00,  4.86it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "100% 500/500 [02:55<00:00,  2.85it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 21.45it/s]\n",
      "f1 0.6642646625405245\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.3571    0.4167        14\n",
      "           1     0.6613    0.7593    0.7069        54\n",
      "           2     0.8828    0.8561    0.8692       132\n",
      "\n",
      "    accuracy                         0.7950       200\n",
      "   macro avg     0.6814    0.6575    0.6643       200\n",
      "weighted avg     0.7962    0.7950    0.7937       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 134902275\n",
      "{'loss': 0.9487, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:10<01:32,  4.88it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 24.84it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.807879626750946, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.3589, 'eval_samples_per_second': 557.267, 'eval_steps_per_second': 19.504, 'epoch': 2.0}\n",
      " 10% 50/500 [00:11<01:32,  4.88it/s]\n",
      "100% 7/7 [00:00<00:00, 22.82it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.7103, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:28,  4.50it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.09it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6238598823547363, 'eval_f1': 0.4929735266813919, 'eval_runtime': 0.3569, 'eval_samples_per_second': 560.442, 'eval_steps_per_second': 19.615, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:28,  4.50it/s]\n",
      "100% 7/7 [00:00<00:00, 22.82it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.5128, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:46<01:15,  4.65it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.54it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5930789113044739, 'eval_f1': 0.5029098169151692, 'eval_runtime': 0.3538, 'eval_samples_per_second': 565.258, 'eval_steps_per_second': 19.784, 'epoch': 6.0}\n",
      " 30% 150/500 [00:46<01:15,  4.65it/s]\n",
      "100% 7/7 [00:00<00:00, 23.18it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.3725, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:08,  4.36it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 29.69it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.641587495803833, 'eval_f1': 0.6746958908173862, 'eval_runtime': 0.3741, 'eval_samples_per_second': 534.574, 'eval_steps_per_second': 18.71, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:08,  4.36it/s]\n",
      "100% 7/7 [00:00<00:00, 19.60it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.2697, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:54,  4.55it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.45it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6582737565040588, 'eval_f1': 0.6735802469135802, 'eval_runtime': 0.3719, 'eval_samples_per_second': 537.779, 'eval_steps_per_second': 18.822, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:54,  4.55it/s]\n",
      "100% 7/7 [00:00<00:00, 22.24it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/added_tokens.json\n",
      "{'loss': 0.217, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:38<00:47,  4.26it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 24.73it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7066656351089478, 'eval_f1': 0.6429720086436505, 'eval_runtime': 0.3681, 'eval_samples_per_second': 543.333, 'eval_steps_per_second': 19.017, 'epoch': 12.0}\n",
      " 60% 300/500 [01:39<00:47,  4.26it/s]\n",
      "100% 7/7 [00:00<00:00, 22.31it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.1645, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:33,  4.53it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.72it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7259703874588013, 'eval_f1': 0.6507961891694788, 'eval_runtime': 0.3491, 'eval_samples_per_second': 572.934, 'eval_steps_per_second': 20.053, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:33,  4.53it/s]\n",
      "100% 7/7 [00:00<00:00, 23.52it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.1329, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:23,  4.26it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.59it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7358020544052124, 'eval_f1': 0.6623266302165386, 'eval_runtime': 0.3522, 'eval_samples_per_second': 567.8, 'eval_steps_per_second': 19.873, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:23,  4.26it/s]\n",
      "100% 7/7 [00:00<00:00, 23.32it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.1175, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:31<00:12,  4.05it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.38it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.760959804058075, 'eval_f1': 0.6606677649471551, 'eval_runtime': 0.3528, 'eval_samples_per_second': 566.908, 'eval_steps_per_second': 19.842, 'epoch': 18.0}\n",
      " 90% 450/500 [02:31<00:12,  4.05it/s]\n",
      "100% 7/7 [00:00<00:00, 23.27it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200 (score: 0.6746958908173862).\n",
      "{'train_runtime': 161.606, 'train_samples_per_second': 99.006, 'train_steps_per_second': 3.094, 'train_loss': 0.38287124315897625, 'epoch': 18.0}\n",
      " 90% 450/500 [02:41<00:12,  4.05it/s]Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      " 90% 450/500 [02:42<00:18,  2.78it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 20.73it/s]\n",
      "f1 0.6746958908173862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.3125    0.4762        16\n",
      "           1     0.6923    0.6545    0.6729        55\n",
      "           2     0.8322    0.9225    0.8750       129\n",
      "\n",
      "    accuracy                         0.8000       200\n",
      "   macro avg     0.8415    0.6298    0.6747       200\n",
      "weighted avg     0.8071    0.8000    0.7875       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 134902275\n",
      "{'loss': 0.9609, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:10<01:39,  4.53it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.97it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.8188098073005676, 'eval_f1': 0.25766871165644173, 'eval_runtime': 0.3623, 'eval_samples_per_second': 552.032, 'eval_steps_per_second': 19.321, 'epoch': 2.0}\n",
      " 10% 50/500 [00:11<01:39,  4.53it/s]\n",
      "100% 7/7 [00:00<00:00, 20.43it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.7593, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:27,  4.57it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.16it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6465954780578613, 'eval_f1': 0.4565909996017523, 'eval_runtime': 0.3685, 'eval_samples_per_second': 542.798, 'eval_steps_per_second': 18.998, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:27,  4.57it/s]\n",
      "100% 7/7 [00:00<00:00, 20.08it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.5487, 'learning_rate': 7.410526315789475e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:45<01:18,  4.47it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.11it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6064639091491699, 'eval_f1': 0.5087534797401795, 'eval_runtime': 0.3766, 'eval_samples_per_second': 530.999, 'eval_steps_per_second': 18.585, 'epoch': 6.0}\n",
      " 30% 150/500 [00:46<01:18,  4.47it/s]\n",
      "100% 7/7 [00:00<00:00, 19.85it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.4129, 'learning_rate': 6.357894736842106e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:05,  4.59it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.09it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5799873471260071, 'eval_f1': 0.7020160464053214, 'eval_runtime': 0.3757, 'eval_samples_per_second': 532.326, 'eval_steps_per_second': 18.631, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:05,  4.59it/s]\n",
      "100% 7/7 [00:00<00:00, 19.85it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.3021, 'learning_rate': 5.305263157894738e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:56,  4.44it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.17it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6509487628936768, 'eval_f1': 0.6846888116081065, 'eval_runtime': 0.3824, 'eval_samples_per_second': 522.989, 'eval_steps_per_second': 18.305, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:56,  4.44it/s]\n",
      "100% 7/7 [00:00<00:00, 20.05it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/added_tokens.json\n",
      "{'loss': 0.2343, 'learning_rate': 4.252631578947369e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:38<00:44,  4.45it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.52it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5887817144393921, 'eval_f1': 0.7172883233489294, 'eval_runtime': 0.3652, 'eval_samples_per_second': 547.583, 'eval_steps_per_second': 19.165, 'epoch': 12.0}\n",
      " 60% 300/500 [01:39<00:44,  4.45it/s]\n",
      "100% 7/7 [00:00<00:00, 20.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.1636, 'learning_rate': 3.2000000000000003e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:31,  4.73it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.88it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6509289741516113, 'eval_f1': 0.7244366744366744, 'eval_runtime': 0.3632, 'eval_samples_per_second': 550.625, 'eval_steps_per_second': 19.272, 'epoch': 14.0}\n",
      " 70% 350/500 [01:57<00:31,  4.73it/s]\n",
      "100% 7/7 [00:00<00:00, 20.33it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.1348, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:22,  4.41it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.28it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6806368827819824, 'eval_f1': 0.718399085191538, 'eval_runtime': 0.3689, 'eval_samples_per_second': 542.165, 'eval_steps_per_second': 18.976, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:22,  4.41it/s]\n",
      "100% 7/7 [00:00<00:00, 19.99it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/added_tokens.json\n",
      "{'loss': 0.1092, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:31<00:10,  4.75it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.45it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6859005093574524, 'eval_f1': 0.6962150830075357, 'eval_runtime': 0.3636, 'eval_samples_per_second': 550.033, 'eval_steps_per_second': 19.251, 'epoch': 18.0}\n",
      " 90% 450/500 [02:32<00:10,  4.75it/s]\n",
      "100% 7/7 [00:00<00:00, 20.36it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0871, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:48<00:00,  4.61it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.14it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.699042797088623, 'eval_f1': 0.6998177261335156, 'eval_runtime': 0.3834, 'eval_samples_per_second': 521.66, 'eval_steps_per_second': 18.258, 'epoch': 20.0}\n",
      "100% 500/500 [02:49<00:00,  4.61it/s]\n",
      "100% 7/7 [00:00<00:00, 19.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350 (score: 0.7244366744366744).\n",
      "{'train_runtime': 179.0913, 'train_samples_per_second': 89.34, 'train_steps_per_second': 2.792, 'train_loss': 0.3712974376678467, 'epoch': 20.0}\n",
      "100% 500/500 [02:58<00:00,  4.61it/s]Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "100% 500/500 [02:59<00:00,  2.78it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 19.58it/s]\n",
      "f1 0.7244366744366744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5385    0.5385    0.5385        13\n",
      "           1     0.8571    0.6885    0.7636        61\n",
      "           2     0.8333    0.9127    0.8712       126\n",
      "\n",
      "    accuracy                         0.8200       200\n",
      "   macro avg     0.7430    0.7132    0.7244       200\n",
      "weighted avg     0.8214    0.8200    0.8168       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--bertweet-base/snapshots/118ab1d567653bec16bbb081eafb6f8942f72108/pytorch_model.bin\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 134902275\n",
      "{'loss': 0.9437, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:10<01:36,  4.67it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.56it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.8449133038520813, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.3559, 'eval_samples_per_second': 561.92, 'eval_steps_per_second': 19.667, 'epoch': 2.0}\n",
      " 10% 50/500 [00:11<01:36,  4.67it/s]\n",
      "100% 7/7 [00:00<00:00, 21.48it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.6935, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:20,  4.95it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.38it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.634932279586792, 'eval_f1': 0.5147907647907648, 'eval_runtime': 0.364, 'eval_samples_per_second': 549.403, 'eval_steps_per_second': 19.229, 'epoch': 4.0}\n",
      " 20% 100/500 [00:28<01:20,  4.95it/s]\n",
      "100% 7/7 [00:00<00:00, 21.16it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.4983, 'learning_rate': 7.431578947368422e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:45<01:04,  5.40it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.36it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5856579542160034, 'eval_f1': 0.5224715950514262, 'eval_runtime': 0.3546, 'eval_samples_per_second': 564.075, 'eval_steps_per_second': 19.743, 'epoch': 6.0}\n",
      " 30% 150/500 [00:46<01:04,  5.40it/s]\n",
      "100% 7/7 [00:00<00:00, 21.67it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.3686, 'learning_rate': 6.378947368421053e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:08,  4.37it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.07it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5768412947654724, 'eval_f1': 0.5110297568759973, 'eval_runtime': 0.3706, 'eval_samples_per_second': 539.67, 'eval_steps_per_second': 18.888, 'epoch': 8.0}\n",
      " 40% 200/500 [01:03<01:08,  4.37it/s]\n",
      "100% 7/7 [00:00<00:00, 20.99it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200/added_tokens.json\n",
      "{'loss': 0.2913, 'learning_rate': 5.326315789473685e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:20<00:52,  4.80it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.63it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5631358623504639, 'eval_f1': 0.5659152579224447, 'eval_runtime': 0.387, 'eval_samples_per_second': 516.761, 'eval_steps_per_second': 18.087, 'epoch': 10.0}\n",
      " 50% 250/500 [01:21<00:52,  4.80it/s]\n",
      "100% 7/7 [00:00<00:00, 20.75it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.2192, 'learning_rate': 4.273684210526316e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:38<00:45,  4.36it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.68it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6238204836845398, 'eval_f1': 0.6217521123181501, 'eval_runtime': 0.3646, 'eval_samples_per_second': 548.563, 'eval_steps_per_second': 19.2, 'epoch': 12.0}\n",
      " 60% 300/500 [01:38<00:45,  4.36it/s]\n",
      "100% 7/7 [00:00<00:00, 21.03it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.1766, 'learning_rate': 3.2210526315789476e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:33,  4.45it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.92it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6291244029998779, 'eval_f1': 0.6399274623522923, 'eval_runtime': 0.3614, 'eval_samples_per_second': 553.382, 'eval_steps_per_second': 19.368, 'epoch': 14.0}\n",
      " 70% 350/500 [01:56<00:33,  4.45it/s]\n",
      "100% 7/7 [00:00<00:00, 21.25it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.1386, 'learning_rate': 2.168421052631579e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:13<00:22,  4.50it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.92it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6496566534042358, 'eval_f1': 0.667630614999036, 'eval_runtime': 0.3566, 'eval_samples_per_second': 560.84, 'eval_steps_per_second': 19.629, 'epoch': 16.0}\n",
      " 80% 400/500 [02:14<00:22,  4.50it/s]\n",
      "100% 7/7 [00:00<00:00, 21.59it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.1184, 'learning_rate': 1.1157894736842106e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:31<00:11,  4.29it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.68it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6930952668190002, 'eval_f1': 0.655558024599912, 'eval_runtime': 0.3684, 'eval_samples_per_second': 542.837, 'eval_steps_per_second': 18.999, 'epoch': 18.0}\n",
      " 90% 450/500 [02:31<00:11,  4.29it/s]\n",
      "100% 7/7 [00:00<00:00, 21.07it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450/added_tokens.json\n",
      "{'loss': 0.1062, 'learning_rate': 6.315789473684211e-08, 'epoch': 20.0}\n",
      "100% 500/500 [02:48<00:00,  4.62it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.53it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6881864070892334, 'eval_f1': 0.667630614999036, 'eval_runtime': 0.3828, 'eval_samples_per_second': 522.497, 'eval_steps_per_second': 18.287, 'epoch': 20.0}\n",
      "100% 500/500 [02:48<00:00,  4.62it/s]\n",
      "100% 7/7 [00:00<00:00, 21.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "added tokens file saved in /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500/added_tokens.json\n",
      "Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-400 (score: 0.667630614999036).\n",
      "{'train_runtime': 178.3006, 'train_samples_per_second': 89.736, 'train_steps_per_second': 2.804, 'train_loss': 0.35543711185455323, 'epoch': 20.0}\n",
      "100% 500/500 [02:58<00:00,  4.62it/s]Deleting older checkpoint [/content/results/finetuning-vinai-bertweet-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "100% 500/500 [02:58<00:00,  2.80it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 21.56it/s]\n",
      "f1 0.667630614999036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.3750    0.4615        24\n",
      "           1     0.6250    0.6383    0.6316        47\n",
      "           2     0.8832    0.9380    0.9098       129\n",
      "\n",
      "    accuracy                         0.8000       200\n",
      "   macro avg     0.7027    0.6504    0.6676       200\n",
      "weighted avg     0.7885    0.8000    0.7906       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.6904356614958936\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-29\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=vinai/bertweet-base --cross-validation=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ub-EtlkTZ7-P",
    "outputId": "76e23719-8d61-4695-f57b-15a0634b3827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 19:22:43.369206: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 19:22:44.375502: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 19:22:44.375656: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 19:22:44.375679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 59.8kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 2.89MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 1.81MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.29MB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 41 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 41 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 425.73it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "Map:   0% 0/800 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "Downloading pytorch_model.bin: 100% 499M/499M [00:08<00:00, 56.4MB/s]\n",
      "Some weights of the model checkpoint at facebook/muppet-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at facebook/muppet-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/500 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.7931, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:01,  3.69it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.57it/s]\u001B[A\n",
      " 86% 6/7 [00:00<00:00, 12.57it/s]\u001B[A\n",
      "{'eval_loss': 0.5689961314201355, 'eval_f1': 0.5029325513196481, 'eval_runtime': 0.5262, 'eval_samples_per_second': 380.049, 'eval_steps_per_second': 13.302, 'epoch': 2.0}\n",
      "\n",
      " 10% 50/500 [00:13<02:01,  3.69it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "{'loss': 0.3923, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:19,  5.01it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.08it/s]\u001B[A\n",
      " 86% 6/7 [00:00<00:00, 12.47it/s]\u001B[A\n",
      "{'eval_loss': 0.5178271532058716, 'eval_f1': 0.6713220609772333, 'eval_runtime': 0.5304, 'eval_samples_per_second': 377.062, 'eval_steps_per_second': 13.197, 'epoch': 4.0}\n",
      "\n",
      " 20% 100/500 [00:32<01:19,  5.01it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.2332, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:50<01:34,  3.71it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.51it/s]\u001B[A\n",
      " 86% 6/7 [00:00<00:00, 12.59it/s]\u001B[A\n",
      "{'eval_loss': 0.5567357540130615, 'eval_f1': 0.6846340822493758, 'eval_runtime': 0.5184, 'eval_samples_per_second': 385.8, 'eval_steps_per_second': 13.503, 'epoch': 6.0}\n",
      "\n",
      " 30% 150/500 [00:50<01:34,  3.71it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1462, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:09<01:12,  4.13it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.06it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6164059638977051, 'eval_f1': 0.7052858941231035, 'eval_runtime': 0.5089, 'eval_samples_per_second': 393.028, 'eval_steps_per_second': 13.756, 'epoch': 8.0}\n",
      " 40% 200/500 [01:09<01:12,  4.13it/s]\n",
      "100% 7/7 [00:00<00:00, 12.71it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.0662, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:27<00:56,  4.43it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.17it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6752532720565796, 'eval_f1': 0.7237112950554022, 'eval_runtime': 0.5248, 'eval_samples_per_second': 381.076, 'eval_steps_per_second': 13.338, 'epoch': 10.0}\n",
      " 50% 250/500 [01:28<00:56,  4.43it/s]\n",
      "100% 7/7 [00:00<00:00, 12.50it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0295, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:46<00:58,  3.42it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.47it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7462007999420166, 'eval_f1': 0.6981172170512985, 'eval_runtime': 0.5296, 'eval_samples_per_second': 377.675, 'eval_steps_per_second': 13.219, 'epoch': 12.0}\n",
      " 60% 300/500 [01:47<00:58,  3.42it/s]\n",
      "100% 7/7 [00:00<00:00, 12.54it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0142, 'learning_rate': 3.2000000000000003e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:05<00:50,  2.95it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.86it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8222774267196655, 'eval_f1': 0.6892166851236619, 'eval_runtime': 0.5181, 'eval_samples_per_second': 385.998, 'eval_steps_per_second': 13.51, 'epoch': 14.0}\n",
      " 70% 350/500 [02:05<00:50,  2.95it/s]\n",
      "100% 7/7 [00:00<00:00, 12.40it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0064, 'learning_rate': 2.1473684210526317e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:24<00:25,  3.85it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.14it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8402358889579773, 'eval_f1': 0.7007557042759612, 'eval_runtime': 0.5151, 'eval_samples_per_second': 388.274, 'eval_steps_per_second': 13.59, 'epoch': 16.0}\n",
      " 80% 400/500 [02:24<00:25,  3.85it/s]\n",
      "100% 7/7 [00:00<00:00, 12.62it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0057, 'learning_rate': 1.0947368421052632e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:43<00:11,  4.36it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 25.82it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8653754591941833, 'eval_f1': 0.6926418957360164, 'eval_runtime': 0.5288, 'eval_samples_per_second': 378.222, 'eval_steps_per_second': 13.238, 'epoch': 18.0}\n",
      " 90% 450/500 [02:43<00:11,  4.36it/s]\n",
      "100% 7/7 [00:00<00:00, 12.38it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0039, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 500/500 [03:02<00:00,  4.17it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 26.23it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8722280859947205, 'eval_f1': 0.6926418957360164, 'eval_runtime': 0.514, 'eval_samples_per_second': 389.087, 'eval_steps_per_second': 13.618, 'epoch': 20.0}\n",
      "100% 500/500 [03:02<00:00,  4.17it/s]\n",
      "100% 7/7 [00:00<00:00, 12.58it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250 (score: 0.7237112950554022).\n",
      "{'train_runtime': 191.7052, 'train_samples_per_second': 83.461, 'train_steps_per_second': 2.608, 'train_loss': 0.16907121190428734, 'epoch': 20.0}\n",
      "100% 500/500 [03:11<00:00,  4.17it/s]Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "100% 500/500 [03:12<00:00,  2.60it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 14.09it/s]\n",
      "f1 0.7237112950554022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.5000    0.5000        12\n",
      "           1     0.8364    0.7302    0.7797        63\n",
      "           2     0.8647    0.9200    0.8915       125\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.7337    0.7167    0.7237       200\n",
      "weighted avg     0.8339    0.8350    0.8328       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--muppet-roberta-base/snapshots/caf238c63db946bdfbd00575713462838e823997/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"facebook/muppet-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--muppet-roberta-base/snapshots/caf238c63db946bdfbd00575713462838e823997/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/muppet-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at facebook/muppet-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "{'loss': 0.7844, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<01:42,  4.37it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.87it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.5135870575904846, 'eval_f1': 0.5315729974160207, 'eval_runtime': 0.4048, 'eval_samples_per_second': 494.113, 'eval_steps_per_second': 17.294, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<01:42,  4.37it/s]\n",
      "100% 7/7 [00:00<00:00, 18.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.3954, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:07,  3.13it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.15it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.43654876947402954, 'eval_f1': 0.6592370481425953, 'eval_runtime': 0.42, 'eval_samples_per_second': 476.177, 'eval_steps_per_second': 16.666, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:07,  3.13it/s]\n",
      "100% 7/7 [00:00<00:00, 17.74it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.2291, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:43,  3.40it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.58it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.48916369676589966, 'eval_f1': 0.6881256170729855, 'eval_runtime': 0.4034, 'eval_samples_per_second': 495.735, 'eval_steps_per_second': 17.351, 'epoch': 6.0}\n",
      " 30% 150/500 [00:52<01:43,  3.40it/s]\n",
      "100% 7/7 [00:00<00:00, 18.37it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.123, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:11<01:17,  3.89it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.75it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5346733331680298, 'eval_f1': 0.6712647002120686, 'eval_runtime': 0.4019, 'eval_samples_per_second': 497.664, 'eval_steps_per_second': 17.418, 'epoch': 8.0}\n",
      " 40% 200/500 [01:11<01:17,  3.89it/s]\n",
      "100% 7/7 [00:00<00:00, 18.51it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.0563, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:29<00:56,  4.43it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.26it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6930212378501892, 'eval_f1': 0.6099906629318395, 'eval_runtime': 0.4174, 'eval_samples_per_second': 479.126, 'eval_steps_per_second': 16.769, 'epoch': 10.0}\n",
      " 50% 250/500 [01:30<00:56,  4.43it/s]\n",
      "100% 7/7 [00:00<00:00, 17.79it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.025, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:49<00:48,  4.13it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.58it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7639399766921997, 'eval_f1': 0.6547087302123443, 'eval_runtime': 0.413, 'eval_samples_per_second': 484.25, 'eval_steps_per_second': 16.949, 'epoch': 12.0}\n",
      " 60% 300/500 [01:50<00:48,  4.13it/s]\n",
      "100% 7/7 [00:00<00:00, 17.98it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0121, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:08<00:45,  3.29it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.42it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8335505127906799, 'eval_f1': 0.6504580040701714, 'eval_runtime': 0.4042, 'eval_samples_per_second': 494.78, 'eval_steps_per_second': 17.317, 'epoch': 14.0}\n",
      " 70% 350/500 [02:09<00:45,  3.29it/s]\n",
      "100% 7/7 [00:00<00:00, 18.41it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0064, 'learning_rate': 2.1473684210526317e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:27<00:21,  4.60it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 20.99it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8682904839515686, 'eval_f1': 0.6952693241108493, 'eval_runtime': 0.4082, 'eval_samples_per_second': 489.972, 'eval_steps_per_second': 17.149, 'epoch': 16.0}\n",
      " 80% 400/500 [02:28<00:21,  4.60it/s]\n",
      "100% 7/7 [00:00<00:00, 18.07it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0041, 'learning_rate': 1.0947368421052632e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:46<00:10,  4.71it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.24it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8889829516410828, 'eval_f1': 0.7043412579534253, 'eval_runtime': 0.4207, 'eval_samples_per_second': 475.386, 'eval_steps_per_second': 16.639, 'epoch': 18.0}\n",
      " 90% 450/500 [02:47<00:10,  4.71it/s]\n",
      "100% 7/7 [00:00<00:00, 17.98it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0035, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 500/500 [03:05<00:00,  4.75it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.64it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8983260989189148, 'eval_f1': 0.700654853326609, 'eval_runtime': 0.4026, 'eval_samples_per_second': 496.818, 'eval_steps_per_second': 17.389, 'epoch': 20.0}\n",
      "100% 500/500 [03:06<00:00,  4.75it/s]\n",
      "100% 7/7 [00:00<00:00, 18.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450 (score: 0.7043412579534253).\n",
      "{'train_runtime': 192.5268, 'train_samples_per_second': 83.105, 'train_steps_per_second': 2.597, 'train_loss': 0.16392786154150962, 'epoch': 20.0}\n",
      "100% 500/500 [03:12<00:00,  4.75it/s]Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "100% 500/500 [03:12<00:00,  2.59it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 17.43it/s]\n",
      "f1 0.7043412579534253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4615    0.4286    0.4444        14\n",
      "           1     0.7500    0.7778    0.7636        54\n",
      "           2     0.9084    0.9015    0.9049       132\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.7066    0.7026    0.7043       200\n",
      "weighted avg     0.8343    0.8350    0.8346       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--muppet-roberta-base/snapshots/caf238c63db946bdfbd00575713462838e823997/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"facebook/muppet-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--muppet-roberta-base/snapshots/caf238c63db946bdfbd00575713462838e823997/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/muppet-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at facebook/muppet-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "{'loss': 0.7754, 'learning_rate': 9.494736842105265e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<01:44,  4.31it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.73it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.5579957365989685, 'eval_f1': 0.5070707070707071, 'eval_runtime': 0.3945, 'eval_samples_per_second': 506.96, 'eval_steps_per_second': 17.744, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<01:44,  4.31it/s]\n",
      "100% 7/7 [00:00<00:00, 19.38it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "{'loss': 0.3835, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:05,  3.19it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 24.78it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5202851891517639, 'eval_f1': 0.6449851042701091, 'eval_runtime': 0.414, 'eval_samples_per_second': 483.039, 'eval_steps_per_second': 16.906, 'epoch': 4.0}\n",
      " 20% 100/500 [00:32<02:05,  3.19it/s]\n",
      "100% 7/7 [00:00<00:00, 18.44it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.2193, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:51,  3.15it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.75it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5444384813308716, 'eval_f1': 0.7037029963500552, 'eval_runtime': 0.4164, 'eval_samples_per_second': 480.301, 'eval_steps_per_second': 16.811, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:51,  3.15it/s]\n",
      "100% 7/7 [00:00<00:00, 18.43it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.109, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:25,  3.52it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.98it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6419703960418701, 'eval_f1': 0.68882627586538, 'eval_runtime': 0.3954, 'eval_samples_per_second': 505.774, 'eval_steps_per_second': 17.702, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:25,  3.52it/s]\n",
      "100% 7/7 [00:00<00:00, 19.25it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.0541, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:29<00:57,  4.38it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.82it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7560569643974304, 'eval_f1': 0.7001790594665912, 'eval_runtime': 0.3936, 'eval_samples_per_second': 508.164, 'eval_steps_per_second': 17.786, 'epoch': 10.0}\n",
      " 50% 250/500 [01:29<00:57,  4.38it/s]\n",
      "100% 7/7 [00:00<00:00, 19.39it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0242, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:49<00:55,  3.59it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.44it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8646524548530579, 'eval_f1': 0.6814225614977331, 'eval_runtime': 0.4127, 'eval_samples_per_second': 484.63, 'eval_steps_per_second': 16.962, 'epoch': 12.0}\n",
      " 60% 300/500 [01:49<00:55,  3.59it/s]\n",
      "100% 7/7 [00:00<00:00, 18.54it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0131, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:08<00:45,  3.29it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.61it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9321581125259399, 'eval_f1': 0.6742779476724431, 'eval_runtime': 0.3959, 'eval_samples_per_second': 505.14, 'eval_steps_per_second': 17.68, 'epoch': 14.0}\n",
      " 70% 350/500 [02:08<00:45,  3.29it/s]\n",
      "100% 7/7 [00:00<00:00, 19.25it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0077, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:27<00:24,  4.07it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 57% 4/7 [00:00<00:00, 25.95it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9601556658744812, 'eval_f1': 0.6742779476724431, 'eval_runtime': 0.3939, 'eval_samples_per_second': 507.777, 'eval_steps_per_second': 17.772, 'epoch': 16.0}\n",
      " 80% 400/500 [02:27<00:24,  4.07it/s]\n",
      "100% 7/7 [00:00<00:00, 19.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150 (score: 0.7037029963500552).\n",
      "{'train_runtime': 156.165, 'train_samples_per_second': 102.456, 'train_steps_per_second': 3.202, 'train_loss': 0.1982875268906355, 'epoch': 16.0}\n",
      " 80% 400/500 [02:35<00:24,  4.07it/s]Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      " 80% 400/500 [02:36<00:39,  2.55it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 16.73it/s]\n",
      "f1 0.7037029963500552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4706    0.5000    0.4848        16\n",
      "           1     0.7193    0.7455    0.7321        55\n",
      "           2     0.9048    0.8837    0.8941       129\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.6982    0.7097    0.7037       200\n",
      "weighted avg     0.8190    0.8150    0.8168       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--muppet-roberta-base/snapshots/caf238c63db946bdfbd00575713462838e823997/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"facebook/muppet-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--muppet-roberta-base/snapshots/caf238c63db946bdfbd00575713462838e823997/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/muppet-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at facebook/muppet-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "{'loss': 0.7888, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:12<02:10,  3.45it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.29it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.5613141059875488, 'eval_f1': 0.5107937289665928, 'eval_runtime': 0.4812, 'eval_samples_per_second': 415.623, 'eval_steps_per_second': 14.547, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:10,  3.45it/s]\n",
      "100% 7/7 [00:00<00:00, 14.06it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.4142, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:31<01:45,  3.80it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.91it/s]\u001B[A\n",
      " 86% 6/7 [00:00<00:00, 13.92it/s]\u001B[A\n",
      "{'eval_loss': 0.5013135671615601, 'eval_f1': 0.6652410652410653, 'eval_runtime': 0.4912, 'eval_samples_per_second': 407.179, 'eval_steps_per_second': 14.251, 'epoch': 4.0}\n",
      "\n",
      " 20% 100/500 [00:32<01:45,  3.80it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.23, 'learning_rate': 7.368421052631579e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:50<01:23,  4.18it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.42it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5367233753204346, 'eval_f1': 0.6676672300907306, 'eval_runtime': 0.4626, 'eval_samples_per_second': 432.351, 'eval_steps_per_second': 15.132, 'epoch': 6.0}\n",
      " 30% 150/500 [00:51<01:23,  4.18it/s]\n",
      "100% 7/7 [00:00<00:00, 14.29it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1234, 'learning_rate': 6.31578947368421e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:09<01:12,  4.14it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.95it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6116347312927246, 'eval_f1': 0.6795713952946657, 'eval_runtime': 0.4591, 'eval_samples_per_second': 435.605, 'eval_steps_per_second': 15.246, 'epoch': 8.0}\n",
      " 40% 200/500 [01:10<01:12,  4.14it/s]\n",
      "100% 7/7 [00:00<00:00, 14.39it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.0631, 'learning_rate': 5.263157894736842e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:28<01:01,  4.05it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.35it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7392375469207764, 'eval_f1': 0.6644504342617551, 'eval_runtime': 0.4818, 'eval_samples_per_second': 415.137, 'eval_steps_per_second': 14.53, 'epoch': 10.0}\n",
      " 50% 250/500 [01:29<01:01,  4.05it/s]\n",
      "100% 7/7 [00:00<00:00, 14.17it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.0424, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:47<00:54,  3.70it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.42it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7191550731658936, 'eval_f1': 0.6978960811991666, 'eval_runtime': 0.4647, 'eval_samples_per_second': 430.427, 'eval_steps_per_second': 15.065, 'epoch': 12.0}\n",
      " 60% 300/500 [01:48<00:54,  3.70it/s]\n",
      "100% 7/7 [00:00<00:00, 14.17it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0289, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:07<00:34,  4.39it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.29it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8463461995124817, 'eval_f1': 0.6721445221445222, 'eval_runtime': 0.4663, 'eval_samples_per_second': 428.954, 'eval_steps_per_second': 15.013, 'epoch': 14.0}\n",
      " 70% 350/500 [02:07<00:34,  4.39it/s]\n",
      "100% 7/7 [00:00<00:00, 14.09it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "{'loss': 0.0179, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:26<00:37,  2.68it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.62it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8237763047218323, 'eval_f1': 0.6942028985507247, 'eval_runtime': 0.4759, 'eval_samples_per_second': 420.262, 'eval_steps_per_second': 14.709, 'epoch': 16.0}\n",
      " 80% 400/500 [02:26<00:37,  2.68it/s]\n",
      "100% 7/7 [00:00<00:00, 14.17it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0085, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:45<00:12,  3.95it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.14it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.877743661403656, 'eval_f1': 0.6760991626352704, 'eval_runtime': 0.4732, 'eval_samples_per_second': 422.674, 'eval_steps_per_second': 14.794, 'epoch': 18.0}\n",
      " 90% 450/500 [02:45<00:12,  3.95it/s]\n",
      "100% 7/7 [00:00<00:00, 14.04it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0068, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 500/500 [03:03<00:00,  4.11it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.80it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8896337747573853, 'eval_f1': 0.6713192332661359, 'eval_runtime': 0.4601, 'eval_samples_per_second': 434.677, 'eval_steps_per_second': 15.214, 'epoch': 20.0}\n",
      "100% 500/500 [03:04<00:00,  4.11it/s]\n",
      "100% 7/7 [00:00<00:00, 14.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300 (score: 0.6978960811991666).\n",
      "{'train_runtime': 193.0086, 'train_samples_per_second': 82.898, 'train_steps_per_second': 2.591, 'train_loss': 0.17239426511526107, 'epoch': 20.0}\n",
      "100% 500/500 [03:12<00:00,  4.11it/s]Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "100% 500/500 [03:13<00:00,  2.58it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 14.50it/s]\n",
      "f1 0.6978960811991666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4167    0.3846    0.4000        13\n",
      "           1     0.8491    0.7377    0.7895        61\n",
      "           2     0.8741    0.9365    0.9042       126\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7133    0.6863    0.6979       200\n",
      "weighted avg     0.8367    0.8400    0.8364       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--muppet-roberta-base/snapshots/caf238c63db946bdfbd00575713462838e823997/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"facebook/muppet-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--muppet-roberta-base/snapshots/caf238c63db946bdfbd00575713462838e823997/pytorch_model.bin\n",
      "Some weights of the model checkpoint at facebook/muppet-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at facebook/muppet-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 124647939\n",
      "{'loss': 0.7667, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:04,  3.61it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.34it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.5531938076019287, 'eval_f1': 0.47254424884308954, 'eval_runtime': 0.4082, 'eval_samples_per_second': 489.929, 'eval_steps_per_second': 17.148, 'epoch': 2.0}\n",
      " 10% 50/500 [00:13<02:04,  3.61it/s]\n",
      "100% 7/7 [00:00<00:00, 18.30it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.3795, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:33<01:35,  4.20it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 22.58it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5047919750213623, 'eval_f1': 0.6161157611008653, 'eval_runtime': 0.5142, 'eval_samples_per_second': 388.981, 'eval_steps_per_second': 13.614, 'epoch': 4.0}\n",
      " 20% 100/500 [00:33<01:35,  4.20it/s]\n",
      "100% 7/7 [00:00<00:00, 15.88it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.2232, 'learning_rate': 7.368421052631579e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:52<01:14,  4.71it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 24.18it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5484918355941772, 'eval_f1': 0.677063400943998, 'eval_runtime': 0.4054, 'eval_samples_per_second': 493.357, 'eval_steps_per_second': 17.268, 'epoch': 6.0}\n",
      " 30% 150/500 [00:52<01:14,  4.71it/s]\n",
      "100% 7/7 [00:00<00:00, 18.33it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1228, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [01:11<01:56,  2.58it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.01it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6230142116546631, 'eval_f1': 0.6748142103478035, 'eval_runtime': 0.4064, 'eval_samples_per_second': 492.184, 'eval_steps_per_second': 17.226, 'epoch': 8.0}\n",
      " 40% 200/500 [01:11<01:56,  2.58it/s]\n",
      "100% 7/7 [00:00<00:00, 18.42it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.0547, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [01:31<00:58,  4.25it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 21.89it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7605464458465576, 'eval_f1': 0.6876543209876543, 'eval_runtime': 0.4289, 'eval_samples_per_second': 466.276, 'eval_steps_per_second': 16.32, 'epoch': 10.0}\n",
      " 50% 250/500 [01:31<00:58,  4.25it/s]\n",
      "100% 7/7 [00:00<00:00, 17.50it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-150] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0215, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [01:50<00:56,  3.52it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.71it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8463386297225952, 'eval_f1': 0.6488852604418641, 'eval_runtime': 0.4043, 'eval_samples_per_second': 494.682, 'eval_steps_per_second': 17.314, 'epoch': 12.0}\n",
      " 60% 300/500 [01:50<00:56,  3.52it/s]\n",
      "100% 7/7 [00:00<00:00, 18.41it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0113, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [02:09<00:48,  3.12it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.92it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8830703496932983, 'eval_f1': 0.669044072491605, 'eval_runtime': 0.4066, 'eval_samples_per_second': 491.854, 'eval_steps_per_second': 17.215, 'epoch': 14.0}\n",
      " 70% 350/500 [02:09<00:48,  3.12it/s]\n",
      "100% 7/7 [00:00<00:00, 18.20it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0091, 'learning_rate': 2.1473684210526317e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [02:28<00:29,  3.35it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.14it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9691161513328552, 'eval_f1': 0.6421193921193921, 'eval_runtime': 0.4206, 'eval_samples_per_second': 475.539, 'eval_steps_per_second': 16.644, 'epoch': 16.0}\n",
      " 80% 400/500 [02:28<00:29,  3.35it/s]\n",
      "100% 7/7 [00:00<00:00, 18.01it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.007, 'learning_rate': 1.0947368421052632e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [02:47<00:11,  4.46it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.31it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9820410013198853, 'eval_f1': 0.6362898530896631, 'eval_runtime': 0.4212, 'eval_samples_per_second': 474.833, 'eval_steps_per_second': 16.619, 'epoch': 18.0}\n",
      " 90% 450/500 [02:47<00:11,  4.46it/s]\n",
      "100% 7/7 [00:00<00:00, 17.90it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0046, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 500/500 [03:06<00:00,  4.57it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 43% 3/7 [00:00<00:00, 23.61it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.9820024967193604, 'eval_f1': 0.6410293720686978, 'eval_runtime': 0.4059, 'eval_samples_per_second': 492.741, 'eval_steps_per_second': 17.246, 'epoch': 20.0}\n",
      "100% 500/500 [03:06<00:00,  4.57it/s]\n",
      "100% 7/7 [00:00<00:00, 18.27it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-250 (score: 0.6876543209876543).\n",
      "{'train_runtime': 195.2046, 'train_samples_per_second': 81.965, 'train_steps_per_second': 2.561, 'train_loss': 0.16003314310312272, 'epoch': 20.0}\n",
      "100% 500/500 [03:15<00:00,  4.57it/s]Deleting older checkpoint [/content/results/finetuning-facebook-muppet-roberta-base-config-default/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "100% 500/500 [03:18<00:00,  2.51it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 17.17it/s]\n",
      "f1 0.6876543209876543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.4167    0.5000        24\n",
      "           1     0.6977    0.6383    0.6667        47\n",
      "           2     0.8582    0.9380    0.8963       129\n",
      "\n",
      "    accuracy                         0.8050       200\n",
      "   macro avg     0.7269    0.6643    0.6877       200\n",
      "weighted avg     0.7925    0.8050    0.7948       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.706012967517874\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-27\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=facebook/muppet-roberta-base --cross-validation=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nLloV6skbO2"
   },
   "source": [
    "## Selected Baseline params searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i03CdYm-igzz",
    "outputId": "a85d2a94-a948-4af5-d346-33656c4cc123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 20:35:30.430753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 20:35:31.430075: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 20:35:31.430201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 20:35:31.430234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 142.95it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2dd56238ea05cc5c.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/500 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.0591, 'learning_rate': 1.894736842105263e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:03,  7.09it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.9778344631195068, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.2199, 'eval_samples_per_second': 909.559, 'eval_steps_per_second': 31.835, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:03,  7.09it/s]\n",
      "100% 7/7 [00:00<00:00, 33.81it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/special_tokens_map.json\n",
      "{'loss': 0.9043, 'learning_rate': 1.6842105263157893e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:43,  9.27it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8459216356277466, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.2377, 'eval_samples_per_second': 841.306, 'eval_steps_per_second': 29.446, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:43,  9.27it/s]\n",
      "100% 7/7 [00:00<00:00, 48.67it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 0.8172, 'learning_rate': 1.4736842105263156e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:47,  7.29it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8043310642242432, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.221, 'eval_samples_per_second': 905.088, 'eval_steps_per_second': 31.678, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:47,  7.29it/s]\n",
      "100% 7/7 [00:00<00:00, 33.23it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.7797, 'learning_rate': 1.263157894736842e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:47,  6.28it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7700231671333313, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.2215, 'eval_samples_per_second': 902.937, 'eval_steps_per_second': 31.603, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:47,  6.28it/s]\n",
      "100% 7/7 [00:00<00:00, 48.93it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.7432, 'learning_rate': 1.0526315789473683e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:32,  7.66it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7375561594963074, 'eval_f1': 0.25641025641025644, 'eval_runtime': 0.2227, 'eval_samples_per_second': 898.227, 'eval_steps_per_second': 31.438, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:32,  7.66it/s]\n",
      "100% 7/7 [00:00<00:00, 49.25it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.71, 'learning_rate': 8.421052631578947e-07, 'epoch': 12.0}\n",
      " 60% 300/500 [00:54<00:33,  5.94it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7109186053276062, 'eval_f1': 0.2998803628488786, 'eval_runtime': 0.2206, 'eval_samples_per_second': 906.787, 'eval_steps_per_second': 31.738, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:33,  5.94it/s]\n",
      "100% 7/7 [00:00<00:00, 49.49it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.6795, 'learning_rate': 6.31578947368421e-07, 'epoch': 14.0}\n",
      " 70% 350/500 [01:04<00:22,  6.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6872662305831909, 'eval_f1': 0.37584356235719224, 'eval_runtime': 0.2408, 'eval_samples_per_second': 830.734, 'eval_steps_per_second': 29.076, 'epoch': 14.0}\n",
      " 70% 350/500 [01:04<00:22,  6.73it/s]\n",
      "100% 7/7 [00:00<00:00, 46.54it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.6542, 'learning_rate': 4.2105263157894733e-07, 'epoch': 16.0}\n",
      " 80% 400/500 [01:14<00:14,  6.98it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6730303764343262, 'eval_f1': 0.395554186559885, 'eval_runtime': 0.2247, 'eval_samples_per_second': 889.952, 'eval_steps_per_second': 31.148, 'epoch': 16.0}\n",
      " 80% 400/500 [01:14<00:14,  6.98it/s]\n",
      "100% 7/7 [00:00<00:00, 49.93it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.6403, 'learning_rate': 2.1052631578947366e-07, 'epoch': 18.0}\n",
      " 90% 450/500 [01:23<00:06,  8.16it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6644842624664307, 'eval_f1': 0.40684929790938584, 'eval_runtime': 0.2239, 'eval_samples_per_second': 893.346, 'eval_steps_per_second': 31.267, 'epoch': 18.0}\n",
      " 90% 450/500 [01:24<00:06,  8.16it/s]\n",
      "100% 7/7 [00:00<00:00, 49.24it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.6345, 'learning_rate': 0.0, 'epoch': 20.0}\n",
      "100% 500/500 [01:33<00:00,  7.60it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6612676978111267, 'eval_f1': 0.40122840122840125, 'eval_runtime': 0.234, 'eval_samples_per_second': 854.788, 'eval_steps_per_second': 29.918, 'epoch': 20.0}\n",
      "100% 500/500 [01:33<00:00,  7.60it/s]\n",
      "100% 7/7 [00:00<00:00, 48.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450 (score: 0.40684929790938584).\n",
      "{'train_runtime': 97.1925, 'train_samples_per_second': 164.622, 'train_steps_per_second': 5.144, 'train_loss': 0.7622118606567383, 'epoch': 20.0}\n",
      "100% 500/500 [01:37<00:00,  7.60it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 67 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 67 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "100% 500/500 [01:37<00:00,  5.12it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 27.30it/s]\n",
      "f1 0.40684929790938584\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        12\n",
      "           1     0.6923    0.2857    0.4045        63\n",
      "           2     0.7011    0.9760    0.8161       125\n",
      "\n",
      "    accuracy                         0.7000       200\n",
      "   macro avg     0.4645    0.4206    0.4068       200\n",
      "weighted avg     0.6563    0.7000    0.6374       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-198219fae9dafda2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1d63a5e99dc01e6f.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 1.0237, 'learning_rate': 1.894736842105263e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:54,  8.20it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.9653222560882568, 'eval_f1': 0.26506024096385544, 'eval_runtime': 0.2128, 'eval_samples_per_second': 939.696, 'eval_steps_per_second': 32.889, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:54,  8.20it/s]\n",
      "100% 7/7 [00:00<00:00, 37.83it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450] due to args.save_total_limit\n",
      "{'loss': 0.9243, 'learning_rate': 1.6842105263157893e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:59,  6.76it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8478857278823853, 'eval_f1': 0.26506024096385544, 'eval_runtime': 0.2282, 'eval_samples_per_second': 876.299, 'eval_steps_per_second': 30.67, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:59,  6.76it/s]\n",
      "100% 7/7 [00:00<00:00, 35.95it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 0.8336, 'learning_rate': 1.4736842105263156e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:46,  7.52it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7858374118804932, 'eval_f1': 0.26506024096385544, 'eval_runtime': 0.2266, 'eval_samples_per_second': 882.627, 'eval_steps_per_second': 30.892, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:46,  7.52it/s]\n",
      "100% 7/7 [00:00<00:00, 35.86it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.7917, 'learning_rate': 1.263157894736842e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:39,  7.60it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7554004192352295, 'eval_f1': 0.26506024096385544, 'eval_runtime': 0.2133, 'eval_samples_per_second': 937.761, 'eval_steps_per_second': 32.822, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:39,  7.60it/s]\n",
      "100% 7/7 [00:00<00:00, 37.43it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.7553, 'learning_rate': 1.0526315789473683e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:31,  7.89it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7262151837348938, 'eval_f1': 0.3142696944210821, 'eval_runtime': 0.2303, 'eval_samples_per_second': 868.492, 'eval_steps_per_second': 30.397, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:31,  7.89it/s]\n",
      "100% 7/7 [00:00<00:00, 36.85it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.723, 'learning_rate': 8.421052631578947e-07, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:27,  7.36it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7012963891029358, 'eval_f1': 0.3980711271850512, 'eval_runtime': 0.2142, 'eval_samples_per_second': 933.64, 'eval_steps_per_second': 32.677, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:27,  7.36it/s]\n",
      "100% 7/7 [00:00<00:00, 37.83it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.6889, 'learning_rate': 6.31578947368421e-07, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.59it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6817278861999512, 'eval_f1': 0.44924047924689775, 'eval_runtime': 0.2179, 'eval_samples_per_second': 917.74, 'eval_steps_per_second': 32.121, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.59it/s]\n",
      "100% 7/7 [00:00<00:00, 36.95it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.6661, 'learning_rate': 4.2105263157894733e-07, 'epoch': 16.0}\n",
      " 80% 400/500 [01:15<00:12,  7.85it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6683325171470642, 'eval_f1': 0.4673173617272996, 'eval_runtime': 0.2292, 'eval_samples_per_second': 872.765, 'eval_steps_per_second': 30.547, 'epoch': 16.0}\n",
      " 80% 400/500 [01:15<00:12,  7.85it/s]\n",
      "100% 7/7 [00:00<00:00, 36.40it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.6498, 'learning_rate': 2.1052631578947366e-07, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:05,  9.03it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6592987179756165, 'eval_f1': 0.4700281744875158, 'eval_runtime': 0.2213, 'eval_samples_per_second': 903.914, 'eval_steps_per_second': 31.637, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:05,  9.03it/s]\n",
      "100% 7/7 [00:00<00:00, 36.89it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.6428, 'learning_rate': 0.0, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  9.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6572259664535522, 'eval_f1': 0.48074293392311446, 'eval_runtime': 0.2197, 'eval_samples_per_second': 910.371, 'eval_steps_per_second': 31.863, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  9.02it/s]\n",
      "100% 7/7 [00:00<00:00, 36.24it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500 (score: 0.48074293392311446).\n",
      "{'train_runtime': 98.8541, 'train_samples_per_second': 161.855, 'train_steps_per_second': 5.058, 'train_loss': 0.7699236602783203, 'epoch': 20.0}\n",
      "100% 500/500 [01:38<00:00,  9.02it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 64 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 64 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "100% 500/500 [01:39<00:00,  5.04it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 27.95it/s]\n",
      "f1 0.48074293392311446\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        14\n",
      "           1     0.6744    0.5370    0.5979        54\n",
      "           2     0.7771    0.9242    0.8443       132\n",
      "\n",
      "    accuracy                         0.7550       200\n",
      "   macro avg     0.4838    0.4871    0.4807       200\n",
      "weighted avg     0.6950    0.7550    0.7187       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-21c7b8108a755fc8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e422d9a57c9fe578.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 1.0228, 'learning_rate': 1.894736842105263e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:56,  8.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.9699975848197937, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2484, 'eval_samples_per_second': 805.308, 'eval_steps_per_second': 28.186, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:56,  8.02it/s]\n",
      "100% 7/7 [00:00<00:00, 30.95it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.9195, 'learning_rate': 1.6842105263157893e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:53,  7.43it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8555383086204529, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2569, 'eval_samples_per_second': 778.543, 'eval_steps_per_second': 27.249, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:53,  7.43it/s]\n",
      "100% 7/7 [00:00<00:00, 29.21it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 0.8278, 'learning_rate': 1.4736842105263156e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:25<00:48,  7.29it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8023645281791687, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.265, 'eval_samples_per_second': 754.7, 'eval_steps_per_second': 26.415, 'epoch': 6.0}\n",
      " 30% 150/500 [00:25<00:48,  7.29it/s]\n",
      "100% 7/7 [00:00<00:00, 29.36it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.786, 'learning_rate': 1.263157894736842e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:38,  7.75it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7746423482894897, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2551, 'eval_samples_per_second': 783.956, 'eval_steps_per_second': 27.438, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:38,  7.75it/s]\n",
      "100% 7/7 [00:00<00:00, 30.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.7454, 'learning_rate': 1.0526315789473683e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:30,  8.11it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.744111955165863, 'eval_f1': 0.30595071128030454, 'eval_runtime': 0.2504, 'eval_samples_per_second': 798.78, 'eval_steps_per_second': 27.957, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:30,  8.11it/s]\n",
      "100% 7/7 [00:00<00:00, 30.33it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.7119, 'learning_rate': 8.421052631578947e-07, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:28,  7.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.718031644821167, 'eval_f1': 0.4082534147050276, 'eval_runtime': 0.2666, 'eval_samples_per_second': 750.191, 'eval_steps_per_second': 26.257, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:28,  7.02it/s]\n",
      "100% 7/7 [00:00<00:00, 28.98it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.6818, 'learning_rate': 6.31578947368421e-07, 'epoch': 14.0}\n",
      " 70% 350/500 [01:04<00:21,  6.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6971838474273682, 'eval_f1': 0.42503569093650356, 'eval_runtime': 0.2506, 'eval_samples_per_second': 798.09, 'eval_steps_per_second': 27.933, 'epoch': 14.0}\n",
      " 70% 350/500 [01:04<00:21,  6.87it/s]\n",
      "100% 7/7 [00:00<00:00, 30.27it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.6561, 'learning_rate': 4.2105263157894733e-07, 'epoch': 16.0}\n",
      " 80% 400/500 [01:14<00:13,  7.68it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.683441162109375, 'eval_f1': 0.44467665157320324, 'eval_runtime': 0.2529, 'eval_samples_per_second': 790.799, 'eval_steps_per_second': 27.678, 'epoch': 16.0}\n",
      " 80% 400/500 [01:14<00:13,  7.68it/s]\n",
      "100% 7/7 [00:00<00:00, 30.02it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.6414, 'learning_rate': 2.1052631578947366e-07, 'epoch': 18.0}\n",
      " 90% 450/500 [01:23<00:07,  6.60it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.674591064453125, 'eval_f1': 0.4436936936936937, 'eval_runtime': 0.2663, 'eval_samples_per_second': 750.933, 'eval_steps_per_second': 26.283, 'epoch': 18.0}\n",
      " 90% 450/500 [01:24<00:07,  6.60it/s]\n",
      "100% 7/7 [00:00<00:00, 29.40it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/special_tokens_map.json\n",
      "{'loss': 0.6363, 'learning_rate': 0.0, 'epoch': 20.0}\n",
      "100% 500/500 [01:33<00:00,  6.95it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6717303395271301, 'eval_f1': 0.44608633687131977, 'eval_runtime': 0.2499, 'eval_samples_per_second': 800.382, 'eval_steps_per_second': 28.013, 'epoch': 20.0}\n",
      "100% 500/500 [01:33<00:00,  6.95it/s]\n",
      "100% 7/7 [00:00<00:00, 30.23it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500 (score: 0.44608633687131977).\n",
      "{'train_runtime': 96.9351, 'train_samples_per_second': 165.059, 'train_steps_per_second': 5.158, 'train_loss': 0.7628933715820313, 'epoch': 20.0}\n",
      "100% 500/500 [01:36<00:00,  6.95it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "100% 500/500 [01:37<00:00,  5.14it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 22.77it/s]\n",
      "f1 0.44608633687131977\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        16\n",
      "           1     0.6389    0.4182    0.5055        55\n",
      "           2     0.7439    0.9457    0.8328       129\n",
      "\n",
      "    accuracy                         0.7250       200\n",
      "   macro avg     0.4609    0.4546    0.4461       200\n",
      "weighted avg     0.6555    0.7250    0.6761       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fc4b439212f8d707.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-922ddddff4b607c8.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 1.0244, 'learning_rate': 1.894736842105263e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:03,  7.06it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.9708960056304932, 'eval_f1': 0.25766871165644173, 'eval_runtime': 0.254, 'eval_samples_per_second': 787.462, 'eval_steps_per_second': 27.561, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:03,  7.06it/s]\n",
      "100% 7/7 [00:00<00:00, 25.34it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.9154, 'learning_rate': 1.6842105263157893e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<01:04,  6.24it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.861828625202179, 'eval_f1': 0.25766871165644173, 'eval_runtime': 0.2541, 'eval_samples_per_second': 787.027, 'eval_steps_per_second': 27.546, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<01:04,  6.24it/s]\n",
      "100% 7/7 [00:00<00:00, 25.24it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 0.8323, 'learning_rate': 1.4736842105263156e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:25<00:43,  8.03it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8115258812904358, 'eval_f1': 0.25766871165644173, 'eval_runtime': 0.2713, 'eval_samples_per_second': 737.163, 'eval_steps_per_second': 25.801, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:43,  8.03it/s]\n",
      "100% 7/7 [00:00<00:00, 24.50it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.7862, 'learning_rate': 1.263157894736842e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:37,  8.04it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7838171124458313, 'eval_f1': 0.2804232804232804, 'eval_runtime': 0.2501, 'eval_samples_per_second': 799.835, 'eval_steps_per_second': 27.994, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:37,  8.04it/s]\n",
      "100% 7/7 [00:00<00:00, 25.65it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.7573, 'learning_rate': 1.0526315789473683e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:33,  7.39it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7572643756866455, 'eval_f1': 0.3658494023209144, 'eval_runtime': 0.2543, 'eval_samples_per_second': 786.321, 'eval_steps_per_second': 27.521, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:33,  7.39it/s]\n",
      "100% 7/7 [00:00<00:00, 25.50it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.7233, 'learning_rate': 8.421052631578947e-07, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:27,  7.37it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7325378656387329, 'eval_f1': 0.4048780487804878, 'eval_runtime': 0.2728, 'eval_samples_per_second': 733.144, 'eval_steps_per_second': 25.66, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:27,  7.37it/s]\n",
      "100% 7/7 [00:00<00:00, 24.16it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.6951, 'learning_rate': 6.31578947368421e-07, 'epoch': 14.0}\n",
      " 70% 350/500 [01:05<00:18,  8.08it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7125127911567688, 'eval_f1': 0.4192204934779192, 'eval_runtime': 0.2502, 'eval_samples_per_second': 799.377, 'eval_steps_per_second': 27.978, 'epoch': 14.0}\n",
      " 70% 350/500 [01:05<00:18,  8.08it/s]\n",
      "100% 7/7 [00:00<00:00, 25.77it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.6708, 'learning_rate': 4.2105263157894733e-07, 'epoch': 16.0}\n",
      " 80% 400/500 [01:15<00:15,  6.44it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6984527707099915, 'eval_f1': 0.4261784183872224, 'eval_runtime': 0.2554, 'eval_samples_per_second': 782.951, 'eval_steps_per_second': 27.403, 'epoch': 16.0}\n",
      " 80% 400/500 [01:15<00:15,  6.44it/s]\n",
      "100% 7/7 [00:00<00:00, 25.21it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.6583, 'learning_rate': 2.1052631578947366e-07, 'epoch': 18.0}\n",
      " 90% 450/500 [01:24<00:07,  7.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6898132562637329, 'eval_f1': 0.4396934865900383, 'eval_runtime': 0.2795, 'eval_samples_per_second': 715.447, 'eval_steps_per_second': 25.041, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:07,  7.12it/s]\n",
      "100% 7/7 [00:00<00:00, 23.51it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.6521, 'learning_rate': 0.0, 'epoch': 20.0}\n",
      "100% 500/500 [01:34<00:00,  8.18it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.686887800693512, 'eval_f1': 0.4396934865900383, 'eval_runtime': 0.2602, 'eval_samples_per_second': 768.632, 'eval_steps_per_second': 26.902, 'epoch': 20.0}\n",
      "100% 500/500 [01:34<00:00,  8.18it/s]\n",
      "100% 7/7 [00:00<00:00, 25.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450 (score: 0.4396934865900383).\n",
      "{'train_runtime': 98.2461, 'train_samples_per_second': 162.856, 'train_steps_per_second': 5.089, 'train_loss': 0.7715242767333984, 'epoch': 20.0}\n",
      "100% 500/500 [01:38<00:00,  8.18it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 59 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 59 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "100% 500/500 [01:38<00:00,  5.06it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 23.62it/s]\n",
      "f1 0.4396934865900383\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        13\n",
      "           1     0.8462    0.3607    0.5057        61\n",
      "           2     0.7011    0.9683    0.8133       126\n",
      "\n",
      "    accuracy                         0.7200       200\n",
      "   macro avg     0.5158    0.4430    0.4397       200\n",
      "weighted avg     0.6998    0.7200    0.6667       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ae00c3c78557fb9d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1075e5fb4da15629.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 1.0172, 'learning_rate': 1.894736842105263e-06, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:06,  6.79it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.9752343893051147, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2195, 'eval_samples_per_second': 911.219, 'eval_steps_per_second': 31.893, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:06,  6.79it/s]\n",
      "100% 7/7 [00:00<00:00, 39.32it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450] due to args.save_total_limit\n",
      "{'loss': 0.916, 'learning_rate': 1.6842105263157893e-06, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:52,  7.63it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8758373856544495, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2186, 'eval_samples_per_second': 915.058, 'eval_steps_per_second': 32.027, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:52,  7.63it/s]\n",
      "100% 7/7 [00:00<00:00, 39.32it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100/special_tokens_map.json\n",
      "{'loss': 0.8196, 'learning_rate': 1.4736842105263156e-06, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:38,  9.16it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.836529552936554, 'eval_f1': 0.26139817629179335, 'eval_runtime': 0.2177, 'eval_samples_per_second': 918.762, 'eval_steps_per_second': 32.157, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:38,  9.16it/s]\n",
      "100% 7/7 [00:00<00:00, 38.86it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.7759, 'learning_rate': 1.263157894736842e-06, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:51,  5.86it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8068798780441284, 'eval_f1': 0.26016260162601623, 'eval_runtime': 0.22, 'eval_samples_per_second': 909.26, 'eval_steps_per_second': 31.824, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:51,  5.86it/s]\n",
      "100% 7/7 [00:00<00:00, 39.20it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.7399, 'learning_rate': 1.0526315789473683e-06, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:30,  8.28it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7764990329742432, 'eval_f1': 0.34408027232707766, 'eval_runtime': 0.2151, 'eval_samples_per_second': 929.631, 'eval_steps_per_second': 32.537, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:30,  8.28it/s]\n",
      "100% 7/7 [00:00<00:00, 39.14it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-50] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.7065, 'learning_rate': 8.421052631578947e-07, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:31,  6.28it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.747084379196167, 'eval_f1': 0.407843137254902, 'eval_runtime': 0.207, 'eval_samples_per_second': 966.171, 'eval_steps_per_second': 33.816, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:31,  6.28it/s]\n",
      "100% 7/7 [00:00<00:00, 43.22it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.6757, 'learning_rate': 6.31578947368421e-07, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.75it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7240838408470154, 'eval_f1': 0.4269005847953216, 'eval_runtime': 0.2093, 'eval_samples_per_second': 955.561, 'eval_steps_per_second': 33.445, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.75it/s]\n",
      "100% 7/7 [00:00<00:00, 42.58it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.6525, 'learning_rate': 4.2105263157894733e-07, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:14,  6.71it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7082672119140625, 'eval_f1': 0.43208481551231076, 'eval_runtime': 0.2192, 'eval_samples_per_second': 912.54, 'eval_steps_per_second': 31.939, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:14,  6.71it/s]\n",
      "100% 7/7 [00:00<00:00, 38.12it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.6335, 'learning_rate': 2.1052631578947366e-07, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:06,  8.07it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6986773610115051, 'eval_f1': 0.45314198738856276, 'eval_runtime': 0.2137, 'eval_samples_per_second': 935.985, 'eval_steps_per_second': 32.759, 'epoch': 18.0}\n",
      " 90% 450/500 [01:26<00:06,  8.07it/s]\n",
      "100% 7/7 [00:00<00:00, 39.42it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.628, 'learning_rate': 0.0, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.25it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6956250071525574, 'eval_f1': 0.4587543437583534, 'eval_runtime': 0.2162, 'eval_samples_per_second': 925.018, 'eval_steps_per_second': 32.376, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.25it/s]\n",
      "100% 7/7 [00:00<00:00, 42.95it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-6/checkpoint-500 (score: 0.4587543437583534).\n",
      "{'train_runtime': 99.0035, 'train_samples_per_second': 161.611, 'train_steps_per_second': 5.05, 'train_loss': 0.7564843978881836, 'epoch': 20.0}\n",
      "100% 500/500 [01:38<00:00,  8.25it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 65 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 65 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "100% 500/500 [01:39<00:00,  5.04it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 33.94it/s]\n",
      "f1 0.4587543437583534\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        24\n",
      "           1     0.5897    0.4894    0.5349        47\n",
      "           2     0.7578    0.9457    0.8414       129\n",
      "\n",
      "    accuracy                         0.7250       200\n",
      "   macro avg     0.4492    0.4784    0.4588       200\n",
      "weighted avg     0.6273    0.7250    0.6684       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.446826296044515\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-33\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=default-lr2e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bOMS3iD0jOm",
    "outputId": "5893f42b-de8e-4187-e599-8262847ec6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 20:44:46.196478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 20:44:47.948514: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 20:44:47.948654: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 20:44:47.948675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 229.81it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2dd56238ea05cc5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6305b363f5daf9a8.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/500 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.889, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:05,  6.84it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.6701074242591858, 'eval_f1': 0.40122840122840125, 'eval_runtime': 0.2547, 'eval_samples_per_second': 785.315, 'eval_steps_per_second': 27.486, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:05,  6.84it/s]\n",
      "100% 7/7 [00:00<00:00, 48.18it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/special_tokens_map.json\n",
      "{'loss': 0.4836, 'learning_rate': 1.688421052631579e-05, 'epoch': 4.0}\n",
      " 20% 100/500 [00:17<00:44,  9.09it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00, 45.53it/s]\u001B[A\n",
      "{'eval_loss': 0.5068950057029724, 'eval_f1': 0.5950011845534233, 'eval_runtime': 0.2467, 'eval_samples_per_second': 810.794, 'eval_steps_per_second': 28.378, 'epoch': 4.0}\n",
      "\n",
      " 20% 100/500 [00:17<00:44,  9.09it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.243, 'learning_rate': 1.4778947368421055e-05, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:51,  6.84it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5014026761054993, 'eval_f1': 0.7371933347866969, 'eval_runtime': 0.2301, 'eval_samples_per_second': 869.27, 'eval_steps_per_second': 30.424, 'epoch': 6.0}\n",
      " 30% 150/500 [00:27<00:51,  6.84it/s]\n",
      "100% 7/7 [00:00<00:00, 47.59it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1072, 'learning_rate': 1.2673684210526315e-05, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:48,  6.13it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5355548858642578, 'eval_f1': 0.7649255749030562, 'eval_runtime': 0.2338, 'eval_samples_per_second': 855.405, 'eval_steps_per_second': 29.939, 'epoch': 8.0}\n",
      " 40% 200/500 [00:37<00:48,  6.13it/s]\n",
      "100% 7/7 [00:00<00:00, 47.09it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.0559, 'learning_rate': 1.0568421052631579e-05, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:32,  7.77it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6224360466003418, 'eval_f1': 0.7837775169195543, 'eval_runtime': 0.2476, 'eval_samples_per_second': 807.688, 'eval_steps_per_second': 28.269, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:32,  7.77it/s]\n",
      "100% 7/7 [00:00<00:00, 44.73it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0319, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:34,  5.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6344271302223206, 'eval_f1': 0.759143665120574, 'eval_runtime': 0.2305, 'eval_samples_per_second': 867.602, 'eval_steps_per_second': 30.366, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:34,  5.73it/s]\n",
      "100% 7/7 [00:00<00:00, 46.73it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0185, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6817283630371094, 'eval_f1': 0.7546806024607293, 'eval_runtime': 0.2296, 'eval_samples_per_second': 870.98, 'eval_steps_per_second': 30.484, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:22,  6.73it/s]\n",
      "100% 7/7 [00:00<00:00, 47.13it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0088, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:14,  6.67it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6839727163314819, 'eval_f1': 0.7518851810190393, 'eval_runtime': 0.242, 'eval_samples_per_second': 826.331, 'eval_steps_per_second': 28.922, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:14,  6.67it/s]\n",
      "100% 7/7 [00:00<00:00, 46.23it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0058, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:06,  7.99it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7094723582267761, 'eval_f1': 0.7599469895485077, 'eval_runtime': 0.2319, 'eval_samples_per_second': 862.34, 'eval_steps_per_second': 30.182, 'epoch': 18.0}\n",
      " 90% 450/500 [01:26<00:06,  7.99it/s]\n",
      "100% 7/7 [00:00<00:00, 47.54it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0053, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  7.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7076950073242188, 'eval_f1': 0.7668405273086597, 'eval_runtime': 0.2273, 'eval_samples_per_second': 879.852, 'eval_steps_per_second': 30.795, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  7.69it/s]\n",
      "100% 7/7 [00:00<00:00, 47.84it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250 (score: 0.7837775169195543).\n",
      "{'train_runtime': 99.1947, 'train_samples_per_second': 161.299, 'train_steps_per_second': 5.041, 'train_loss': 0.18489150017499922, 'epoch': 20.0}\n",
      "100% 500/500 [01:39<00:00,  7.69it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "100% 500/500 [01:39<00:00,  5.02it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 26.83it/s]\n",
      "f1 0.7837775169195543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.5833    0.6364        12\n",
      "           1     0.8333    0.7937    0.8130        63\n",
      "           2     0.8846    0.9200    0.9020       125\n",
      "\n",
      "    accuracy                         0.8600       200\n",
      "   macro avg     0.8060    0.7657    0.7838       200\n",
      "weighted avg     0.8574    0.8600    0.8580       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1d63a5e99dc01e6f.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8792, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:55,  8.04it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.6416821479797363, 'eval_f1': 0.4986334579802623, 'eval_runtime': 0.217, 'eval_samples_per_second': 921.578, 'eval_steps_per_second': 32.255, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:55,  8.04it/s]\n",
      "100% 7/7 [00:00<00:00, 36.89it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.4623, 'learning_rate': 1.6842105263157896e-05, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:58,  6.79it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.48339325189590454, 'eval_f1': 0.6515594541910331, 'eval_runtime': 0.2235, 'eval_samples_per_second': 894.682, 'eval_steps_per_second': 31.314, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:58,  6.79it/s]\n",
      "100% 7/7 [00:00<00:00, 36.12it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.2124, 'learning_rate': 1.4736842105263159e-05, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:48,  7.19it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.4717286229133606, 'eval_f1': 0.700250986366655, 'eval_runtime': 0.2201, 'eval_samples_per_second': 908.849, 'eval_steps_per_second': 31.81, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:48,  7.19it/s]\n",
      "100% 7/7 [00:00<00:00, 36.37it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.0929, 'learning_rate': 1.263157894736842e-05, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:40,  7.44it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.581473171710968, 'eval_f1': 0.7096285709424395, 'eval_runtime': 0.2376, 'eval_samples_per_second': 841.736, 'eval_steps_per_second': 29.461, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:40,  7.44it/s]\n",
      "100% 7/7 [00:00<00:00, 35.37it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.0426, 'learning_rate': 1.0526315789473684e-05, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:30,  8.08it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7889661192893982, 'eval_f1': 0.6333099698398539, 'eval_runtime': 0.2182, 'eval_samples_per_second': 916.584, 'eval_steps_per_second': 32.08, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:30,  8.08it/s]\n",
      "100% 7/7 [00:00<00:00, 36.58it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.0358, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:27,  7.38it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7678724527359009, 'eval_f1': 0.6519278096800657, 'eval_runtime': 0.2198, 'eval_samples_per_second': 909.908, 'eval_steps_per_second': 31.847, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:27,  7.38it/s]\n",
      "100% 7/7 [00:00<00:00, 35.96it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0191, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:23,  6.38it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8037693500518799, 'eval_f1': 0.6905332393704486, 'eval_runtime': 0.2296, 'eval_samples_per_second': 870.927, 'eval_steps_per_second': 30.482, 'epoch': 14.0}\n",
      " 70% 350/500 [01:06<00:23,  6.38it/s]\n",
      "100% 7/7 [00:00<00:00, 36.58it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0124, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:12,  8.05it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8269514441490173, 'eval_f1': 0.6905332393704486, 'eval_runtime': 0.22, 'eval_samples_per_second': 908.934, 'eval_steps_per_second': 31.813, 'epoch': 16.0}\n",
      " 80% 400/500 [01:16<00:12,  8.05it/s]\n",
      "100% 7/7 [00:00<00:00, 36.24it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0088, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:26<00:05,  9.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.8292783498764038, 'eval_f1': 0.6850918250343052, 'eval_runtime': 0.2202, 'eval_samples_per_second': 908.284, 'eval_steps_per_second': 31.79, 'epoch': 18.0}\n",
      " 90% 450/500 [01:26<00:05,  9.00it/s]\n",
      "100% 7/7 [00:00<00:00, 36.15it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200 (score: 0.7096285709424395).\n",
      "{'train_runtime': 89.7345, 'train_samples_per_second': 178.304, 'train_steps_per_second': 5.572, 'train_loss': 0.19615168472131092, 'epoch': 18.0}\n",
      " 90% 450/500 [01:29<00:05,  9.00it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      " 90% 450/500 [01:30<00:10,  4.99it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 28.85it/s]\n",
      "f1 0.7096285709424395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.3571    0.4545        14\n",
      "           1     0.8000    0.7407    0.7692        54\n",
      "           2     0.8732    0.9394    0.9051       132\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.7661    0.6791    0.7096       200\n",
      "weighted avg     0.8361    0.8450    0.8369       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-21c7b8108a755fc8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e422d9a57c9fe578.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9018, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:56,  8.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7015802264213562, 'eval_f1': 0.37099708109480095, 'eval_runtime': 0.2559, 'eval_samples_per_second': 781.419, 'eval_steps_per_second': 27.35, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<00:56,  8.00it/s]\n",
      "100% 7/7 [00:00<00:00, 29.44it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.5256, 'learning_rate': 1.6842105263157896e-05, 'epoch': 4.0}\n",
      " 20% 100/500 [00:15<00:54,  7.36it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5028218030929565, 'eval_f1': 0.5316520844055584, 'eval_runtime': 0.2512, 'eval_samples_per_second': 796.044, 'eval_steps_per_second': 27.862, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:54,  7.36it/s]\n",
      "100% 7/7 [00:00<00:00, 30.20it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.2749, 'learning_rate': 1.4736842105263159e-05, 'epoch': 6.0}\n",
      " 30% 150/500 [00:25<00:47,  7.38it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      " 71% 5/7 [00:00<00:00, 28.46it/s]\u001B[A\n",
      "{'eval_loss': 0.4616805911064148, 'eval_f1': 0.6913983070884759, 'eval_runtime': 0.2667, 'eval_samples_per_second': 749.999, 'eval_steps_per_second': 26.25, 'epoch': 6.0}\n",
      "\n",
      " 30% 150/500 [00:25<00:47,  7.38it/s]\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1146, 'learning_rate': 1.263157894736842e-05, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:38,  7.71it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.526252269744873, 'eval_f1': 0.6897800716045938, 'eval_runtime': 0.2589, 'eval_samples_per_second': 772.462, 'eval_steps_per_second': 27.036, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:38,  7.71it/s]\n",
      "100% 7/7 [00:00<00:00, 28.83it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.044, 'learning_rate': 1.0526315789473684e-05, 'epoch': 10.0}\n",
      " 50% 250/500 [00:44<00:31,  8.06it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5743996500968933, 'eval_f1': 0.7013436249285306, 'eval_runtime': 0.2548, 'eval_samples_per_second': 784.86, 'eval_steps_per_second': 27.47, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:31,  8.06it/s]\n",
      "100% 7/7 [00:00<00:00, 29.41it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0168, 'learning_rate': 8.421052631578948e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:54<00:28,  7.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6542747020721436, 'eval_f1': 0.6967670011148271, 'eval_runtime': 0.2624, 'eval_samples_per_second': 762.215, 'eval_steps_per_second': 26.678, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:28,  7.12it/s]\n",
      "100% 7/7 [00:00<00:00, 29.56it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0078, 'learning_rate': 6.31578947368421e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:04<00:21,  6.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7249257564544678, 'eval_f1': 0.6977491229517759, 'eval_runtime': 0.2526, 'eval_samples_per_second': 791.742, 'eval_steps_per_second': 27.711, 'epoch': 14.0}\n",
      " 70% 350/500 [01:04<00:21,  6.87it/s]\n",
      "100% 7/7 [00:00<00:00, 30.04it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0056, 'learning_rate': 4.210526315789474e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:13<00:13,  7.43it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7218897938728333, 'eval_f1': 0.7049530790160475, 'eval_runtime': 0.2566, 'eval_samples_per_second': 779.34, 'eval_steps_per_second': 27.277, 'epoch': 16.0}\n",
      " 80% 400/500 [01:14<00:13,  7.43it/s]\n",
      "100% 7/7 [00:00<00:00, 29.46it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0047, 'learning_rate': 2.105263157894737e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:23<00:07,  6.68it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7558961510658264, 'eval_f1': 0.6977491229517759, 'eval_runtime': 0.2557, 'eval_samples_per_second': 782.118, 'eval_steps_per_second': 27.374, 'epoch': 18.0}\n",
      " 90% 450/500 [01:23<00:07,  6.68it/s]\n",
      "100% 7/7 [00:00<00:00, 29.56it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/special_tokens_map.json\n",
      "{'loss': 0.0044, 'learning_rate': 0.0, 'epoch': 20.0}\n",
      "100% 500/500 [01:32<00:00,  6.94it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7489901185035706, 'eval_f1': 0.6977491229517759, 'eval_runtime': 0.2504, 'eval_samples_per_second': 798.751, 'eval_steps_per_second': 27.956, 'epoch': 20.0}\n",
      "100% 500/500 [01:32<00:00,  6.94it/s]\n",
      "100% 7/7 [00:00<00:00, 30.21it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400 (score: 0.7049530790160475).\n",
      "{'train_runtime': 96.0589, 'train_samples_per_second': 166.565, 'train_steps_per_second': 5.205, 'train_loss': 0.1900219670534134, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  6.94it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "100% 500/500 [01:36<00:00,  5.18it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 23.01it/s]\n",
      "f1 0.7049530790160475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.3125    0.4348        16\n",
      "           1     0.7377    0.8182    0.7759        55\n",
      "           2     0.8939    0.9147    0.9042       129\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7820    0.6818    0.7050       200\n",
      "weighted avg     0.8366    0.8400    0.8314       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fc4b439212f8d707.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-922ddddff4b607c8.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8795, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:03,  7.05it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.6769989132881165, 'eval_f1': 0.44802188552188554, 'eval_runtime': 0.2536, 'eval_samples_per_second': 788.765, 'eval_steps_per_second': 27.607, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:03,  7.05it/s]\n",
      "100% 7/7 [00:00<00:00, 25.41it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.4914, 'learning_rate': 1.6842105263157896e-05, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<01:06,  6.03it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.47358453273773193, 'eval_f1': 0.6376863389033774, 'eval_runtime': 0.2526, 'eval_samples_per_second': 791.678, 'eval_steps_per_second': 27.709, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<01:06,  6.03it/s]\n",
      "100% 7/7 [00:00<00:00, 25.32it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.2414, 'learning_rate': 1.4778947368421055e-05, 'epoch': 6.0}\n",
      " 30% 150/500 [00:25<00:46,  7.55it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.453681081533432, 'eval_f1': 0.6819412080620321, 'eval_runtime': 0.2594, 'eval_samples_per_second': 771.077, 'eval_steps_per_second': 26.988, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:46,  7.55it/s]\n",
      "100% 7/7 [00:00<00:00, 24.97it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1113, 'learning_rate': 1.2673684210526315e-05, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:37,  8.07it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.47272422909736633, 'eval_f1': 0.749908299791982, 'eval_runtime': 0.2609, 'eval_samples_per_second': 766.432, 'eval_steps_per_second': 26.825, 'epoch': 8.0}\n",
      " 40% 200/500 [00:35<00:37,  8.07it/s]\n",
      "100% 7/7 [00:00<00:00, 25.15it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.0468, 'learning_rate': 1.0568421052631579e-05, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:33,  7.39it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6043790578842163, 'eval_f1': 0.7122697563874034, 'eval_runtime': 0.2638, 'eval_samples_per_second': 758.165, 'eval_steps_per_second': 26.536, 'epoch': 10.0}\n",
      " 50% 250/500 [00:45<00:33,  7.39it/s]\n",
      "100% 7/7 [00:00<00:00, 25.42it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/special_tokens_map.json\n",
      "{'loss': 0.0301, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:27,  7.31it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6733376979827881, 'eval_f1': 0.694104701139386, 'eval_runtime': 0.2565, 'eval_samples_per_second': 779.592, 'eval_steps_per_second': 27.286, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:27,  7.31it/s]\n",
      "100% 7/7 [00:00<00:00, 25.72it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250] due to args.save_total_limit\n",
      "{'loss': 0.0231, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:05<00:19,  7.58it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6292640566825867, 'eval_f1': 0.7276424618378642, 'eval_runtime': 0.2765, 'eval_samples_per_second': 723.365, 'eval_steps_per_second': 25.318, 'epoch': 14.0}\n",
      " 70% 350/500 [01:05<00:19,  7.58it/s]\n",
      "100% 7/7 [00:00<00:00, 24.96it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0146, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:14<00:15,  6.50it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6603840589523315, 'eval_f1': 0.7368582387593795, 'eval_runtime': 0.2521, 'eval_samples_per_second': 793.44, 'eval_steps_per_second': 27.77, 'epoch': 16.0}\n",
      " 80% 400/500 [01:15<00:15,  6.50it/s]\n",
      "100% 7/7 [00:00<00:00, 25.54it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0109, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:24<00:07,  7.14it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.6799907684326172, 'eval_f1': 0.7265512265512265, 'eval_runtime': 0.2544, 'eval_samples_per_second': 786.303, 'eval_steps_per_second': 27.521, 'epoch': 18.0}\n",
      " 90% 450/500 [01:24<00:07,  7.14it/s]\n",
      "100% 7/7 [00:00<00:00, 25.03it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200 (score: 0.749908299791982).\n",
      "{'train_runtime': 88.1959, 'train_samples_per_second': 181.414, 'train_steps_per_second': 5.669, 'train_loss': 0.2054443014992608, 'epoch': 18.0}\n",
      " 90% 450/500 [01:28<00:07,  7.14it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 61 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 61 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      " 90% 450/500 [01:28<00:09,  5.08it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 22.38it/s]\n",
      "f1 0.749908299791982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.5385    0.5185        13\n",
      "           1     0.8421    0.7869    0.8136        61\n",
      "           2     0.9070    0.9286    0.9176       126\n",
      "\n",
      "    accuracy                         0.8600       200\n",
      "   macro avg     0.7497    0.7513    0.7499       200\n",
      "weighted avg     0.8607    0.8600    0.8600       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ae00c3c78557fb9d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1075e5fb4da15629.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8977, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 50/500 [00:06<01:04,  7.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                    \n",
      "\u001B[A{'eval_loss': 0.7599383592605591, 'eval_f1': 0.3017797888386124, 'eval_runtime': 0.2142, 'eval_samples_per_second': 933.662, 'eval_steps_per_second': 32.678, 'epoch': 2.0}\n",
      " 10% 50/500 [00:07<01:04,  7.00it/s]\n",
      "100% 7/7 [00:00<00:00, 38.88it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.5205, 'learning_rate': 1.6842105263157896e-05, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:50,  7.92it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5465052723884583, 'eval_f1': 0.5370351281082133, 'eval_runtime': 0.2175, 'eval_samples_per_second': 919.384, 'eval_steps_per_second': 32.178, 'epoch': 4.0}\n",
      " 20% 100/500 [00:16<00:50,  7.92it/s]\n",
      "100% 7/7 [00:00<00:00, 39.45it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-50] due to args.save_total_limit\n",
      "{'loss': 0.2745, 'learning_rate': 1.4778947368421055e-05, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:38,  9.11it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.546812117099762, 'eval_f1': 0.6394554589896795, 'eval_runtime': 0.2076, 'eval_samples_per_second': 963.365, 'eval_steps_per_second': 33.718, 'epoch': 6.0}\n",
      " 30% 150/500 [00:26<00:38,  9.11it/s]\n",
      "100% 7/7 [00:00<00:00, 43.13it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1392, 'learning_rate': 1.2673684210526315e-05, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:50,  6.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.556412935256958, 'eval_f1': 0.6959480547963693, 'eval_runtime': 0.2145, 'eval_samples_per_second': 932.196, 'eval_steps_per_second': 32.627, 'epoch': 8.0}\n",
      " 40% 200/500 [00:36<00:50,  6.00it/s]\n",
      "100% 7/7 [00:00<00:00, 38.69it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.0564, 'learning_rate': 1.0568421052631579e-05, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:31,  8.03it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.5941243767738342, 'eval_f1': 0.7089787476822137, 'eval_runtime': 0.2154, 'eval_samples_per_second': 928.295, 'eval_steps_per_second': 32.49, 'epoch': 10.0}\n",
      " 50% 250/500 [00:46<00:31,  8.03it/s]\n",
      "100% 7/7 [00:00<00:00, 39.12it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0279, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 300/500 [00:55<00:31,  6.28it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7134459614753723, 'eval_f1': 0.7075304086374197, 'eval_runtime': 0.211, 'eval_samples_per_second': 947.969, 'eval_steps_per_second': 33.179, 'epoch': 12.0}\n",
      " 60% 300/500 [00:56<00:31,  6.28it/s]\n",
      "100% 7/7 [00:00<00:00, 39.72it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.0133, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 350/500 [01:05<00:21,  6.88it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7505763173103333, 'eval_f1': 0.7076779026217229, 'eval_runtime': 0.2076, 'eval_samples_per_second': 963.413, 'eval_steps_per_second': 33.719, 'epoch': 14.0}\n",
      " 70% 350/500 [01:05<00:21,  6.88it/s]\n",
      "100% 7/7 [00:00<00:00, 43.08it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0085, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 400/500 [01:15<00:15,  6.42it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7086980938911438, 'eval_f1': 0.7216902634805815, 'eval_runtime': 0.2067, 'eval_samples_per_second': 967.51, 'eval_steps_per_second': 33.863, 'epoch': 16.0}\n",
      " 80% 400/500 [01:15<00:15,  6.42it/s]\n",
      "100% 7/7 [00:00<00:00, 43.35it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-250] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-350] due to args.save_total_limit\n",
      "{'loss': 0.0058, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:06,  8.08it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7755416631698608, 'eval_f1': 0.6941574180380151, 'eval_runtime': 0.2071, 'eval_samples_per_second': 965.534, 'eval_steps_per_second': 33.794, 'epoch': 18.0}\n",
      " 90% 450/500 [01:25<00:06,  8.08it/s]\n",
      "100% 7/7 [00:00<00:00, 43.61it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450/special_tokens_map.json\n",
      "{'loss': 0.0049, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 500/500 [01:34<00:00,  8.48it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "                                     \n",
      "\u001B[A{'eval_loss': 0.7580114603042603, 'eval_f1': 0.7043421562035169, 'eval_runtime': 0.2138, 'eval_samples_per_second': 935.635, 'eval_steps_per_second': 32.747, 'epoch': 20.0}\n",
      "100% 500/500 [01:35<00:00,  8.48it/s]\n",
      "100% 7/7 [00:00<00:00, 39.31it/s]\u001B[A\n",
      "                                 \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-450] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-400 (score: 0.7216902634805815).\n",
      "{'train_runtime': 98.3589, 'train_samples_per_second': 162.67, 'train_steps_per_second': 5.083, 'train_loss': 0.19486760824918747, 'epoch': 20.0}\n",
      "100% 500/500 [01:38<00:00,  8.48it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5/checkpoint-500] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 59 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 59 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "100% 500/500 [01:38<00:00,  5.06it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "100% 7/7 [00:00<00:00, 33.80it/s]\n",
      "f1 0.7216902634805815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6471    0.4583    0.5366        24\n",
      "           1     0.6939    0.7234    0.7083        47\n",
      "           2     0.9030    0.9380    0.9202       129\n",
      "\n",
      "    accuracy                         0.8300       200\n",
      "   macro avg     0.7480    0.7066    0.7217       200\n",
      "weighted avg     0.8231    0.8300    0.8243       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7358261994858221\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-34\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=default-lr2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eh3jB2jd036k",
    "outputId": "d704a155-3faf-41d0-b593-67330cd2c827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 21:35:35.007322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 21:35:37.012740: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 21:35:37.012885: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 21:35:37.012906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 156.82it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2dd56238ea05cc5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6305b363f5daf9a8.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/1000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.8875, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:15, 11.85it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6723046898841858, 'eval_f1': 0.4136713895103157, 'eval_runtime': 0.2187, 'eval_samples_per_second': 914.596, 'eval_steps_per_second': 59.449, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:15, 11.85it/s]\n",
      "100% 13/13 [00:00<00:00, 88.66it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.5033, 'learning_rate': 8.442105263157896e-06, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:19<00:57, 13.95it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.525564432144165, 'eval_f1': 0.5244379276637341, 'eval_runtime': 0.235, 'eval_samples_per_second': 851.081, 'eval_steps_per_second': 55.32, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:19<00:57, 13.95it/s]\n",
      "100% 13/13 [00:00<00:00, 75.85it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2935, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:30<00:54, 12.88it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5092772841453552, 'eval_f1': 0.721606182795699, 'eval_runtime': 0.207, 'eval_samples_per_second': 966.133, 'eval_steps_per_second': 62.799, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:30<00:54, 12.88it/s]\n",
      "100% 13/13 [00:00<00:00, 87.57it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1575, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:41<00:50, 11.83it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5664268732070923, 'eval_f1': 0.7262952743835677, 'eval_runtime': 0.2077, 'eval_samples_per_second': 962.983, 'eval_steps_per_second': 62.594, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:41<00:50, 11.83it/s]\n",
      "100% 13/13 [00:00<00:00, 85.28it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.094, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:52<00:38, 12.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.633449375629425, 'eval_f1': 0.7316457197799796, 'eval_runtime': 0.2436, 'eval_samples_per_second': 821.181, 'eval_steps_per_second': 53.377, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:52<00:38, 12.87it/s]\n",
      "100% 13/13 [00:00<00:00, 72.18it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0665, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:31, 12.76it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.637489914894104, 'eval_f1': 0.7433695630416941, 'eval_runtime': 0.2057, 'eval_samples_per_second': 972.455, 'eval_steps_per_second': 63.21, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:03<00:31, 12.76it/s]\n",
      "100% 13/13 [00:00<00:00, 86.14it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.0577, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:14<00:25, 11.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6551926136016846, 'eval_f1': 0.7494791666666667, 'eval_runtime': 0.2084, 'eval_samples_per_second': 959.626, 'eval_steps_per_second': 62.376, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:14<00:25, 11.57it/s]\n",
      "100% 13/13 [00:00<00:00, 85.73it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.046, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:25<00:17, 11.49it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6463432312011719, 'eval_f1': 0.7326445794102209, 'eval_runtime': 0.2468, 'eval_samples_per_second': 810.321, 'eval_steps_per_second': 52.671, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:25<00:17, 11.49it/s]\n",
      "100% 13/13 [00:00<00:00, 73.33it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0387, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:07, 14.11it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6617882251739502, 'eval_f1': 0.7480544324159819, 'eval_runtime': 0.2076, 'eval_samples_per_second': 963.449, 'eval_steps_per_second': 62.624, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:07, 14.11it/s]\n",
      "100% 13/13 [00:00<00:00, 89.16it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0355, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:46<00:00, 13.92it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6645162105560303, 'eval_f1': 0.7326445794102209, 'eval_runtime': 0.2067, 'eval_samples_per_second': 967.421, 'eval_steps_per_second': 62.882, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:46<00:00, 13.92it/s]\n",
      "100% 13/13 [00:00<00:00, 88.01it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700 (score: 0.7494791666666667).\n",
      "{'train_runtime': 110.2875, 'train_samples_per_second': 145.075, 'train_steps_per_second': 9.067, 'train_loss': 0.2180278663635254, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:50<00:00, 13.92it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "100% 1000/1000 [01:50<00:00,  9.04it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 46.30it/s]\n",
      "f1 0.7494791666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5833    0.5833    0.5833        12\n",
      "           1     0.8070    0.7302    0.7667        63\n",
      "           2     0.8779    0.9200    0.8984       125\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7561    0.7445    0.7495       200\n",
      "weighted avg     0.8379    0.8400    0.8380       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d5256619d5ecb4f8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-468186b3490e9025.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8779, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:05, 13.67it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6617602705955505, 'eval_f1': 0.4843045199300377, 'eval_runtime': 0.2018, 'eval_samples_per_second': 991.132, 'eval_steps_per_second': 64.424, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:05, 13.67it/s]\n",
      "100% 13/13 [00:00<00:00, 73.35it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.4955, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:02, 12.76it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5114169120788574, 'eval_f1': 0.5388309322735553, 'eval_runtime': 0.2088, 'eval_samples_per_second': 958.0, 'eval_steps_per_second': 62.27, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:02, 12.76it/s]\n",
      "100% 13/13 [00:00<00:00, 71.04it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2746, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:58, 12.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 46% 6/13 [00:00<00:00, 57.20it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.46627020835876465, 'eval_f1': 0.6500439290107187, 'eval_runtime': 0.2603, 'eval_samples_per_second': 768.243, 'eval_steps_per_second': 49.936, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:30<00:58, 12.02it/s]\n",
      "100% 13/13 [00:00<00:00, 56.42it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1362, 'learning_rate': 6.326315789473685e-06, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:44, 13.51it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5610765218734741, 'eval_f1': 0.6868686868686869, 'eval_runtime': 0.222, 'eval_samples_per_second': 900.857, 'eval_steps_per_second': 58.556, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:44, 13.51it/s]\n",
      "100% 13/13 [00:00<00:00, 68.69it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0873, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:35, 13.99it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6895059943199158, 'eval_f1': 0.6728617720984134, 'eval_runtime': 0.2056, 'eval_samples_per_second': 972.862, 'eval_steps_per_second': 63.236, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:35, 13.99it/s]\n",
      "100% 13/13 [00:00<00:00, 71.30it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.0615, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:30, 13.15it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7604440450668335, 'eval_f1': 0.6821231056525173, 'eval_runtime': 0.2038, 'eval_samples_per_second': 981.173, 'eval_steps_per_second': 63.776, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:03<00:30, 13.15it/s]\n",
      "100% 13/13 [00:00<00:00, 71.81it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.0424, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:25, 11.58it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7338058948516846, 'eval_f1': 0.680665796195123, 'eval_runtime': 0.2232, 'eval_samples_per_second': 895.926, 'eval_steps_per_second': 58.235, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:14<00:25, 11.58it/s]\n",
      "100% 13/13 [00:00<00:00, 70.13it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0327, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:14, 13.95it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7708603143692017, 'eval_f1': 0.7124739883360572, 'eval_runtime': 0.2083, 'eval_samples_per_second': 960.35, 'eval_steps_per_second': 62.423, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:14, 13.95it/s]\n",
      "100% 13/13 [00:00<00:00, 72.14it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.0256, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:06, 14.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.788145124912262, 'eval_f1': 0.7033988404659001, 'eval_runtime': 0.207, 'eval_samples_per_second': 966.245, 'eval_steps_per_second': 62.806, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:06, 14.73it/s]\n",
      "100% 13/13 [00:00<00:00, 70.91it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/special_tokens_map.json\n",
      "{'loss': 0.0224, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:46<00:00, 13.49it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.779112696647644, 'eval_f1': 0.7023988005997003, 'eval_runtime': 0.2244, 'eval_samples_per_second': 891.3, 'eval_steps_per_second': 57.935, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:46<00:00, 13.49it/s]\n",
      "100% 13/13 [00:00<00:00, 69.79it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800 (score: 0.7124739883360572).\n",
      "{'train_runtime': 110.4907, 'train_samples_per_second': 144.809, 'train_steps_per_second': 9.051, 'train_loss': 0.20561454820632935, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:50<00:00, 13.49it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 62 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 62 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "100% 1000/1000 [01:51<00:00,  9.01it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 50.77it/s]\n",
      "f1 0.7124739883360572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.3571    0.4545        14\n",
      "           1     0.7302    0.8519    0.7863        54\n",
      "           2     0.9070    0.8864    0.8966       132\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7540    0.6985    0.7125       200\n",
      "weighted avg     0.8395    0.8400    0.8359       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-88aa98cae448eb95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-80d401ff1a69a5b8.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8707, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:10, 12.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 54% 7/13 [00:00<00:00, 67.88it/s]\u001B[A\n",
      "{'eval_loss': 0.6706286668777466, 'eval_f1': 0.46750399421567596, 'eval_runtime': 0.2663, 'eval_samples_per_second': 750.898, 'eval_steps_per_second': 48.808, 'epoch': 2.0}\n",
      "\n",
      " 10% 100/1000 [00:08<01:10, 12.73it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.4915, 'learning_rate': 8.421052631578948e-06, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:04, 12.48it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 62% 8/13 [00:00<00:00, 71.00it/s]\u001B[A\n",
      "{'eval_loss': 0.49501433968544006, 'eval_f1': 0.5943032926751596, 'eval_runtime': 0.2498, 'eval_samples_per_second': 800.524, 'eval_steps_per_second': 52.034, 'epoch': 4.0}\n",
      "\n",
      " 20% 200/1000 [00:18<01:04, 12.48it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2695, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:58, 12.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 69% 9/13 [00:00<00:00, 66.76it/s]\u001B[A\n",
      "{'eval_loss': 0.4645159840583801, 'eval_f1': 0.701171408430682, 'eval_runtime': 0.226, 'eval_samples_per_second': 884.804, 'eval_steps_per_second': 57.512, 'epoch': 6.0}\n",
      "\n",
      " 30% 300/1000 [00:29<00:58, 12.00it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1427, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:50, 11.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4896792471408844, 'eval_f1': 0.7038662629665885, 'eval_runtime': 0.2562, 'eval_samples_per_second': 780.697, 'eval_steps_per_second': 50.745, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:50, 11.87it/s]\n",
      "100% 13/13 [00:00<00:00, 68.10it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.082, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:50<00:37, 13.30it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5533660054206848, 'eval_f1': 0.7150383466172939, 'eval_runtime': 0.2328, 'eval_samples_per_second': 859.249, 'eval_steps_per_second': 55.851, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:50<00:37, 13.30it/s]\n",
      "100% 13/13 [00:00<00:00, 64.78it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0611, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:01<00:30, 13.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5850265622138977, 'eval_f1': 0.7162869216101155, 'eval_runtime': 0.2162, 'eval_samples_per_second': 924.909, 'eval_steps_per_second': 60.119, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:30, 13.00it/s]\n",
      "100% 13/13 [00:00<00:00, 68.20it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.0515, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:12<00:25, 11.60it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6265882253646851, 'eval_f1': 0.7194563614797077, 'eval_runtime': 0.251, 'eval_samples_per_second': 796.808, 'eval_steps_per_second': 51.792, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:25, 11.60it/s]\n",
      "100% 13/13 [00:00<00:00, 75.45it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0393, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:23<00:14, 13.62it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.628965437412262, 'eval_f1': 0.7158164928292047, 'eval_runtime': 0.224, 'eval_samples_per_second': 892.734, 'eval_steps_per_second': 58.028, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:23<00:14, 13.62it/s]\n",
      "100% 13/13 [00:00<00:00, 67.86it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0364, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:34<00:08, 12.36it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6113020181655884, 'eval_f1': 0.7113432947608883, 'eval_runtime': 0.2204, 'eval_samples_per_second': 907.361, 'eval_steps_per_second': 58.978, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:34<00:08, 12.36it/s]\n",
      "100% 13/13 [00:00<00:00, 66.49it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0326, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:44<00:00, 11.68it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6114072203636169, 'eval_f1': 0.7076767496061739, 'eval_runtime': 0.257, 'eval_samples_per_second': 778.173, 'eval_steps_per_second': 50.581, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:45<00:00, 11.68it/s]\n",
      "100% 13/13 [00:00<00:00, 71.63it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700 (score: 0.7194563614797077).\n",
      "{'train_runtime': 108.5567, 'train_samples_per_second': 147.388, 'train_steps_per_second': 9.212, 'train_loss': 0.20773514556884765, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:48<00:00, 11.68it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "100% 1000/1000 [01:48<00:00,  9.18it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 51.08it/s]\n",
      "f1 0.7194563614797077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.3750    0.4615        16\n",
      "           1     0.7419    0.8364    0.7863        55\n",
      "           2     0.9141    0.9070    0.9105       129\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.7520    0.7061    0.7195       200\n",
      "weighted avg     0.8416    0.8450    0.8404       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9ee582664d5bea23.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8821, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:11, 12.66it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6875726580619812, 'eval_f1': 0.4527059296684513, 'eval_runtime': 0.2071, 'eval_samples_per_second': 965.537, 'eval_steps_per_second': 62.76, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:11, 12.66it/s]\n",
      "100% 13/13 [00:00<00:00, 60.64it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.5235, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:08, 11.75it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.489044189453125, 'eval_f1': 0.6187952658540894, 'eval_runtime': 0.251, 'eval_samples_per_second': 796.933, 'eval_steps_per_second': 51.801, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:08, 11.75it/s]\n",
      "100% 13/13 [00:00<00:00, 55.82it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.292, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:50, 13.83it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.48110145330429077, 'eval_f1': 0.7164846077457795, 'eval_runtime': 0.2142, 'eval_samples_per_second': 933.864, 'eval_steps_per_second': 60.701, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:50, 13.83it/s]\n",
      "100% 13/13 [00:00<00:00, 58.82it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1568, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:43, 13.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.530264139175415, 'eval_f1': 0.7494798733604705, 'eval_runtime': 0.2307, 'eval_samples_per_second': 866.885, 'eval_steps_per_second': 56.347, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:43, 13.69it/s]\n",
      "100% 13/13 [00:00<00:00, 59.89it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0758, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:41, 12.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.555901288986206, 'eval_f1': 0.6876572687884321, 'eval_runtime': 0.2561, 'eval_samples_per_second': 781.041, 'eval_steps_per_second': 50.768, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:41, 12.12it/s]\n",
      "100% 13/13 [00:00<00:00, 54.11it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.048, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:30, 13.24it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5492088794708252, 'eval_f1': 0.7243853139047274, 'eval_runtime': 0.2131, 'eval_samples_per_second': 938.428, 'eval_steps_per_second': 60.998, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:30, 13.24it/s]\n",
      "100% 13/13 [00:00<00:00, 59.49it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.0312, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:22, 13.58it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6105934977531433, 'eval_f1': 0.6912382122783317, 'eval_runtime': 0.2109, 'eval_samples_per_second': 948.48, 'eval_steps_per_second': 61.651, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:22, 13.58it/s]\n",
      "100% 13/13 [00:00<00:00, 60.71it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.028, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:17, 11.45it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6301087737083435, 'eval_f1': 0.7442992803108689, 'eval_runtime': 0.2626, 'eval_samples_per_second': 761.541, 'eval_steps_per_second': 49.5, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:17, 11.45it/s]\n",
      "100% 13/13 [00:00<00:00, 49.71it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.0235, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:34<00:07, 12.62it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6079453825950623, 'eval_f1': 0.7564933985167448, 'eval_runtime': 0.2163, 'eval_samples_per_second': 924.591, 'eval_steps_per_second': 60.098, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:34<00:07, 12.62it/s]\n",
      "100% 13/13 [00:00<00:00, 59.51it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0207, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:45<00:00, 13.56it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6310765743255615, 'eval_f1': 0.7247693147693148, 'eval_runtime': 0.2134, 'eval_samples_per_second': 937.058, 'eval_steps_per_second': 60.909, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:45<00:00, 13.56it/s]\n",
      "100% 13/13 [00:00<00:00, 59.99it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900 (score: 0.7564933985167448).\n",
      "{'train_runtime': 109.1027, 'train_samples_per_second': 146.651, 'train_steps_per_second': 9.166, 'train_loss': 0.2081737675666809, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:48<00:00, 13.56it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "100% 1000/1000 [01:49<00:00,  9.13it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 49.56it/s]\n",
      "f1 0.7564933985167448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5385    0.5385    0.5385        13\n",
      "           1     0.8571    0.7869    0.8205        61\n",
      "           2     0.8931    0.9286    0.9105       126\n",
      "\n",
      "    accuracy                         0.8600       200\n",
      "   macro avg     0.7629    0.7513    0.7565       200\n",
      "weighted avg     0.8591    0.8600    0.8589       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ae00c3c78557fb9d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1075e5fb4da15629.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.865, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:09, 12.90it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7043817043304443, 'eval_f1': 0.4173095227947548, 'eval_runtime': 0.195, 'eval_samples_per_second': 1025.596, 'eval_steps_per_second': 66.664, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:09, 12.90it/s]\n",
      "100% 13/13 [00:00<00:00, 79.25it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900] due to args.save_total_limit\n",
      "{'loss': 0.4822, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<00:57, 14.01it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5212464928627014, 'eval_f1': 0.5137930658644781, 'eval_runtime': 0.2015, 'eval_samples_per_second': 992.422, 'eval_steps_per_second': 64.507, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:19<00:57, 14.01it/s]\n",
      "100% 13/13 [00:00<00:00, 76.81it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.2633, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:53, 13.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4997776746749878, 'eval_f1': 0.709201388888889, 'eval_runtime': 0.2302, 'eval_samples_per_second': 868.895, 'eval_steps_per_second': 56.478, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:53, 13.02it/s]\n",
      "100% 13/13 [00:00<00:00, 67.08it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1422, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:52, 11.44it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5771934986114502, 'eval_f1': 0.7221153968678721, 'eval_runtime': 0.1945, 'eval_samples_per_second': 1028.119, 'eval_steps_per_second': 66.828, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:52, 11.44it/s]\n",
      "100% 13/13 [00:00<00:00, 78.67it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0895, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:35, 14.08it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6373857855796814, 'eval_f1': 0.7234014189001207, 'eval_runtime': 0.1952, 'eval_samples_per_second': 1024.574, 'eval_steps_per_second': 66.597, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:35, 14.08it/s]\n",
      "100% 13/13 [00:00<00:00, 78.98it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0674, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:37, 10.53it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6226428747177124, 'eval_f1': 0.7798368298368298, 'eval_runtime': 0.2316, 'eval_samples_per_second': 863.527, 'eval_steps_per_second': 56.129, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:37, 10.53it/s]\n",
      "100% 13/13 [00:00<00:00, 69.52it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.0553, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:25, 11.83it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6760039329528809, 'eval_f1': 0.767041046542365, 'eval_runtime': 0.1935, 'eval_samples_per_second': 1033.693, 'eval_steps_per_second': 67.19, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:25, 11.83it/s]\n",
      "100% 13/13 [00:00<00:00, 83.04it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700/special_tokens_map.json\n",
      "{'loss': 0.0418, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:16, 12.19it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6751596331596375, 'eval_f1': 0.767041046542365, 'eval_runtime': 0.1959, 'eval_samples_per_second': 1020.768, 'eval_steps_per_second': 66.35, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:16, 12.19it/s]\n",
      "100% 13/13 [00:00<00:00, 76.61it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.0375, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:07, 12.90it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7171981334686279, 'eval_f1': 0.7559052077201702, 'eval_runtime': 0.2253, 'eval_samples_per_second': 887.577, 'eval_steps_per_second': 57.693, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:07, 12.90it/s]\n",
      "100% 13/13 [00:00<00:00, 68.57it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0384, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:45<00:00, 14.20it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7192548513412476, 'eval_f1': 0.7559052077201702, 'eval_runtime': 0.1969, 'eval_samples_per_second': 1015.751, 'eval_steps_per_second': 66.024, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:45<00:00, 14.20it/s]\n",
      "100% 13/13 [00:00<00:00, 82.44it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-600 (score: 0.7798368298368298).\n",
      "{'train_runtime': 109.6143, 'train_samples_per_second': 145.966, 'train_steps_per_second': 9.123, 'train_loss': 0.20826417922973633, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:49<00:00, 14.20it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "100% 1000/1000 [01:50<00:00,  9.09it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 54.07it/s]\n",
      "f1 0.7798368298368298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.6250    0.6818        24\n",
      "           1     0.7347    0.7660    0.7500        47\n",
      "           2     0.9008    0.9147    0.9077       129\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.7952    0.7686    0.7798       200\n",
      "weighted avg     0.8436    0.8450    0.8435       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7503913613745858\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-40\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=default-b16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYL4RgTX1BaL",
    "outputId": "2c6126ab-6fcd-42e5-9b77-4ee35b046bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 21:11:56.772887: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 21:11:57.767201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 21:11:57.767349: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 21:11:57.767370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 159.02it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2dd56238ea05cc5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6305b363f5daf9a8.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/1000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.8133, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:15, 11.98it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5411573648452759, 'eval_f1': 0.5474654377880185, 'eval_runtime': 0.2313, 'eval_samples_per_second': 864.78, 'eval_steps_per_second': 56.211, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:15, 11.98it/s]\n",
      "100% 13/13 [00:00<00:00, 91.02it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-150] due to args.save_total_limit\n",
      "{'loss': 0.3342, 'learning_rate': 1.688421052631579e-05, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:19<00:55, 14.53it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 77% 10/13 [00:00<00:00, 90.87it/s]\u001B[A\n",
      "{'eval_loss': 0.5604848265647888, 'eval_f1': 0.735632602575485, 'eval_runtime': 0.2005, 'eval_samples_per_second': 997.294, 'eval_steps_per_second': 64.824, 'epoch': 4.0}\n",
      "\n",
      " 20% 200/1000 [00:19<00:55, 14.53it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1231, 'learning_rate': 1.4778947368421055e-05, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:30<00:58, 11.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6220368146896362, 'eval_f1': 0.7647058823529411, 'eval_runtime': 0.2332, 'eval_samples_per_second': 857.805, 'eval_steps_per_second': 55.757, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:30<00:58, 11.87it/s]\n",
      "100% 13/13 [00:00<00:00, 76.70it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0583, 'learning_rate': 1.2673684210526315e-05, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:41<00:50, 11.94it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6623622179031372, 'eval_f1': 0.7741618101267592, 'eval_runtime': 0.2018, 'eval_samples_per_second': 991.028, 'eval_steps_per_second': 64.417, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:41<00:50, 11.94it/s]\n",
      "100% 13/13 [00:00<00:00, 89.34it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0306, 'learning_rate': 1.0568421052631579e-05, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:52<00:36, 13.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6768113970756531, 'eval_f1': 0.7604223571879988, 'eval_runtime': 0.2033, 'eval_samples_per_second': 983.769, 'eval_steps_per_second': 63.945, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:52<00:36, 13.57it/s]\n",
      "100% 13/13 [00:00<00:00, 89.31it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.0199, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:32, 12.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7090690732002258, 'eval_f1': 0.8011369394768605, 'eval_runtime': 0.2313, 'eval_samples_per_second': 864.717, 'eval_steps_per_second': 56.207, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:03<00:32, 12.12it/s]\n",
      "100% 13/13 [00:00<00:00, 80.13it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.009, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:26, 11.48it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7515295147895813, 'eval_f1': 0.7558746924446914, 'eval_runtime': 0.2026, 'eval_samples_per_second': 987.32, 'eval_steps_per_second': 64.176, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:14<00:26, 11.48it/s]\n",
      "100% 13/13 [00:00<00:00, 88.50it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/special_tokens_map.json\n",
      "{'loss': 0.0044, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:16, 12.49it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7822007536888123, 'eval_f1': 0.7629200268817203, 'eval_runtime': 0.2035, 'eval_samples_per_second': 982.787, 'eval_steps_per_second': 63.881, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:25<00:16, 12.49it/s]\n",
      "100% 13/13 [00:00<00:00, 87.74it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.0028, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:07, 12.84it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.8066153526306152, 'eval_f1': 0.7694794838335507, 'eval_runtime': 0.2284, 'eval_samples_per_second': 875.479, 'eval_steps_per_second': 56.906, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:07, 12.84it/s]\n",
      "100% 13/13 [00:00<00:00, 78.80it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0021, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:46<00:00, 13.35it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.789683997631073, 'eval_f1': 0.7555380129694154, 'eval_runtime': 0.204, 'eval_samples_per_second': 980.288, 'eval_steps_per_second': 63.719, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:46<00:00, 13.35it/s]\n",
      "100% 13/13 [00:00<00:00, 87.74it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600 (score: 0.8011369394768605).\n",
      "{'train_runtime': 110.1744, 'train_samples_per_second': 145.224, 'train_steps_per_second': 9.077, 'train_loss': 0.13978184321522713, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:50<00:00, 13.35it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "100% 1000/1000 [01:50<00:00,  9.04it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 49.78it/s]\n",
      "f1 0.8011369394768605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6429    0.7500    0.6923        12\n",
      "           1     0.8448    0.7778    0.8099        63\n",
      "           2     0.8906    0.9120    0.9012       125\n",
      "\n",
      "    accuracy                         0.8600       200\n",
      "   macro avg     0.7928    0.8133    0.8011       200\n",
      "weighted avg     0.8613    0.8600    0.8599       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d5256619d5ecb4f8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-468186b3490e9025.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.7968, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:10, 12.81it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5289852619171143, 'eval_f1': 0.5532281437793248, 'eval_runtime': 0.233, 'eval_samples_per_second': 858.375, 'eval_steps_per_second': 55.794, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:10, 12.81it/s]\n",
      "100% 13/13 [00:00<00:00, 65.46it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.3218, 'learning_rate': 1.6863157894736844e-05, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:04, 12.36it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5058420300483704, 'eval_f1': 0.6668961019767609, 'eval_runtime': 0.205, 'eval_samples_per_second': 975.677, 'eval_steps_per_second': 63.419, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:04, 12.36it/s]\n",
      "100% 13/13 [00:00<00:00, 72.04it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1035, 'learning_rate': 1.4757894736842106e-05, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:52, 13.46it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6068741083145142, 'eval_f1': 0.7060515543754841, 'eval_runtime': 0.2086, 'eval_samples_per_second': 958.61, 'eval_steps_per_second': 62.31, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:52, 13.46it/s]\n",
      "100% 13/13 [00:00<00:00, 71.61it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0515, 'learning_rate': 1.265263157894737e-05, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:47, 12.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 46% 6/13 [00:00<00:00, 59.55it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7318421006202698, 'eval_f1': 0.7124739883360572, 'eval_runtime': 0.2468, 'eval_samples_per_second': 810.269, 'eval_steps_per_second': 52.667, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:47, 12.69it/s]\n",
      "100% 13/13 [00:00<00:00, 62.33it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0205, 'learning_rate': 1.0547368421052633e-05, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:37, 13.48it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.9233716726303101, 'eval_f1': 0.6740915032679738, 'eval_runtime': 0.2068, 'eval_samples_per_second': 967.318, 'eval_steps_per_second': 62.876, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:37, 13.48it/s]\n",
      "100% 13/13 [00:00<00:00, 72.01it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.0076, 'learning_rate': 8.442105263157896e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:28, 13.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.9578434824943542, 'eval_f1': 0.6757839480591241, 'eval_runtime': 0.2029, 'eval_samples_per_second': 985.832, 'eval_steps_per_second': 64.079, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:28, 13.87it/s]\n",
      "100% 13/13 [00:00<00:00, 73.55it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.0029, 'learning_rate': 6.336842105263158e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:26, 11.25it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 46% 6/13 [00:00<00:00, 58.02it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.933927595615387, 'eval_f1': 0.7023234852884244, 'eval_runtime': 0.254, 'eval_samples_per_second': 787.452, 'eval_steps_per_second': 51.184, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:26, 11.25it/s]\n",
      "100% 13/13 [00:00<00:00, 56.86it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0019, 'learning_rate': 4.2315789473684215e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:14, 13.82it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.966346025466919, 'eval_f1': 0.6897364205294046, 'eval_runtime': 0.2068, 'eval_samples_per_second': 967.286, 'eval_steps_per_second': 62.874, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:14, 13.82it/s]\n",
      "100% 13/13 [00:00<00:00, 70.68it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.0016, 'learning_rate': 2.1263157894736844e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:06, 14.82it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.9724034070968628, 'eval_f1': 0.6951688801706489, 'eval_runtime': 0.2072, 'eval_samples_per_second': 965.425, 'eval_steps_per_second': 62.753, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:06, 14.82it/s]\n",
      "100% 13/13 [00:00<00:00, 71.75it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400 (score: 0.7124739883360572).\n",
      "{'train_runtime': 98.8949, 'train_samples_per_second': 161.788, 'train_steps_per_second': 10.112, 'train_loss': 0.14535280916425916, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:38<00:06, 14.82it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      " 90% 900/1000 [01:39<00:11,  9.06it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 52.98it/s]\n",
      "f1 0.7124739883360572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.3571    0.4545        14\n",
      "           1     0.7302    0.8519    0.7863        54\n",
      "           2     0.9070    0.8864    0.8966       132\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7540    0.6985    0.7125       200\n",
      "weighted avg     0.8395    0.8400    0.8359       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-88aa98cae448eb95.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8207, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:04, 13.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 69% 9/13 [00:00<00:00, 66.65it/s]\u001B[A\n",
      "{'eval_loss': 0.5786779522895813, 'eval_f1': 0.5093632958801498, 'eval_runtime': 0.2235, 'eval_samples_per_second': 894.754, 'eval_steps_per_second': 58.159, 'epoch': 2.0}\n",
      "\n",
      " 10% 100/1000 [00:07<01:04, 13.87it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.3869, 'learning_rate': 1.6863157894736844e-05, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:02, 12.88it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 69% 9/13 [00:00<00:00, 66.99it/s]\u001B[A\n",
      "{'eval_loss': 0.43898463249206543, 'eval_f1': 0.6555064401530902, 'eval_runtime': 0.2183, 'eval_samples_per_second': 916.157, 'eval_steps_per_second': 59.55, 'epoch': 4.0}\n",
      "\n",
      " 20% 200/1000 [00:18<01:02, 12.88it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1169, 'learning_rate': 1.4757894736842106e-05, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<01:03, 10.98it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 62% 8/13 [00:00<00:00, 67.25it/s]\u001B[A\n",
      "{'eval_loss': 0.5730740427970886, 'eval_f1': 0.756811815635345, 'eval_runtime': 0.2695, 'eval_samples_per_second': 742.059, 'eval_steps_per_second': 48.234, 'epoch': 6.0}\n",
      "\n",
      " 30% 300/1000 [00:29<01:03, 10.98it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0306, 'learning_rate': 1.265263157894737e-05, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:39<00:45, 13.09it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.629077136516571, 'eval_f1': 0.7542346542346543, 'eval_runtime': 0.2202, 'eval_samples_per_second': 908.463, 'eval_steps_per_second': 59.05, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:45, 13.09it/s]\n",
      "100% 13/13 [00:00<00:00, 67.88it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0106, 'learning_rate': 1.0547368421052633e-05, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:50<00:37, 13.39it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6717721819877625, 'eval_f1': 0.7440890484368744, 'eval_runtime': 0.2193, 'eval_samples_per_second': 912.121, 'eval_steps_per_second': 59.288, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:50<00:37, 13.39it/s]\n",
      "100% 13/13 [00:00<00:00, 67.64it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0034, 'learning_rate': 8.442105263157896e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:01<00:33, 11.78it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6935648322105408, 'eval_f1': 0.7411374500048775, 'eval_runtime': 0.2614, 'eval_samples_per_second': 764.99, 'eval_steps_per_second': 49.724, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:01<00:33, 11.78it/s]\n",
      "100% 13/13 [00:00<00:00, 70.81it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.0021, 'learning_rate': 6.336842105263158e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:12<00:23, 12.54it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7245903015136719, 'eval_f1': 0.7719014170163595, 'eval_runtime': 0.2253, 'eval_samples_per_second': 887.733, 'eval_steps_per_second': 57.703, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:12<00:23, 12.54it/s]\n",
      "100% 13/13 [00:00<00:00, 66.87it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0016, 'learning_rate': 4.2315789473684215e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:22<00:14, 13.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7486124634742737, 'eval_f1': 0.7719014170163595, 'eval_runtime': 0.2228, 'eval_samples_per_second': 897.725, 'eval_steps_per_second': 58.352, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:23<00:14, 13.69it/s]\n",
      "100% 13/13 [00:00<00:00, 66.82it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0023, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:33<00:08, 11.37it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7462376952171326, 'eval_f1': 0.7719014170163595, 'eval_runtime': 0.2746, 'eval_samples_per_second': 728.453, 'eval_steps_per_second': 47.349, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:34<00:08, 11.37it/s]\n",
      "100% 13/13 [00:00<00:00, 67.54it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0014, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:44<00:00, 12.55it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7507029175758362, 'eval_f1': 0.7719014170163595, 'eval_runtime': 0.2126, 'eval_samples_per_second': 940.594, 'eval_steps_per_second': 61.139, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:44<00:00, 12.55it/s]\n",
      "100% 13/13 [00:00<00:00, 68.33it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700 (score: 0.7719014170163595).\n",
      "{'train_runtime': 107.9849, 'train_samples_per_second': 148.169, 'train_steps_per_second': 9.261, 'train_loss': 0.13764278437197208, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:47<00:00, 12.55it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "100% 1000/1000 [01:48<00:00,  9.23it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 45.99it/s]\n",
      "f1 0.7719014170163595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7273    0.5000    0.5926        16\n",
      "           1     0.7895    0.8182    0.8036        55\n",
      "           2     0.9091    0.9302    0.9195       129\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.8086    0.7495    0.7719       200\n",
      "weighted avg     0.8617    0.8650    0.8615       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fc4b439212f8d707.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-922ddddff4b607c8.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8032, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:07<01:16, 11.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4979116916656494, 'eval_f1': 0.5432875318066157, 'eval_runtime': 0.2575, 'eval_samples_per_second': 776.589, 'eval_steps_per_second': 50.478, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:16, 11.73it/s]\n",
      "100% 13/13 [00:00<00:00, 52.12it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.3716, 'learning_rate': 1.688421052631579e-05, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:03, 12.66it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.41721484065055847, 'eval_f1': 0.705546193311613, 'eval_runtime': 0.2118, 'eval_samples_per_second': 944.276, 'eval_steps_per_second': 61.378, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:03, 12.66it/s]\n",
      "100% 13/13 [00:00<00:00, 59.47it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1162, 'learning_rate': 1.4778947368421055e-05, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:51, 13.58it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5895342230796814, 'eval_f1': 0.688398515299656, 'eval_runtime': 0.2218, 'eval_samples_per_second': 901.863, 'eval_steps_per_second': 58.621, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:51, 13.58it/s]\n",
      "100% 13/13 [00:00<00:00, 59.94it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/special_tokens_map.json\n",
      "{'loss': 0.037, 'learning_rate': 1.2673684210526315e-05, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:48, 12.40it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6177664399147034, 'eval_f1': 0.7078534578534579, 'eval_runtime': 0.2589, 'eval_samples_per_second': 772.51, 'eval_steps_per_second': 50.213, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:48, 12.40it/s]\n",
      "100% 13/13 [00:00<00:00, 53.39it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0192, 'learning_rate': 1.0568421052631579e-05, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:38, 13.10it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.657917320728302, 'eval_f1': 0.7167725515338734, 'eval_runtime': 0.215, 'eval_samples_per_second': 930.449, 'eval_steps_per_second': 60.479, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:38, 13.10it/s]\n",
      "100% 13/13 [00:00<00:00, 59.92it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0115, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:30, 13.14it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6927239298820496, 'eval_f1': 0.7293082556240451, 'eval_runtime': 0.211, 'eval_samples_per_second': 947.843, 'eval_steps_per_second': 61.61, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:30, 13.14it/s]\n",
      "100% 13/13 [00:00<00:00, 60.54it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500] due to args.save_total_limit\n",
      "{'loss': 0.0076, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:24, 12.18it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6851460337638855, 'eval_f1': 0.7429488893001323, 'eval_runtime': 0.2493, 'eval_samples_per_second': 802.239, 'eval_steps_per_second': 52.146, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:24, 12.18it/s]\n",
      "100% 13/13 [00:00<00:00, 54.99it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0064, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:23<00:16, 12.44it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7608384490013123, 'eval_f1': 0.7291917160699585, 'eval_runtime': 0.2144, 'eval_samples_per_second': 932.73, 'eval_steps_per_second': 60.627, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:16, 12.44it/s]\n",
      "100% 13/13 [00:00<00:00, 60.17it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.004, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:34<00:07, 13.21it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7507171630859375, 'eval_f1': 0.7106937190304005, 'eval_runtime': 0.2157, 'eval_samples_per_second': 927.372, 'eval_steps_per_second': 60.279, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:34<00:07, 13.21it/s]\n",
      "100% 13/13 [00:00<00:00, 60.54it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0029, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:45<00:00, 12.01it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      " 54% 7/13 [00:00<00:00, 48.62it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7255470752716064, 'eval_f1': 0.7162654707731538, 'eval_runtime': 0.2777, 'eval_samples_per_second': 720.116, 'eval_steps_per_second': 46.808, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:45<00:00, 12.01it/s]\n",
      "100% 13/13 [00:00<00:00, 53.96it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700 (score: 0.7429488893001323).\n",
      "{'train_runtime': 109.1608, 'train_samples_per_second': 146.573, 'train_steps_per_second': 9.161, 'train_loss': 0.13796554961800575, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:49<00:00, 12.01it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "100% 1000/1000 [01:49<00:00,  9.13it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 49.72it/s]\n",
      "f1 0.7429488893001323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4615    0.4615    0.4615        13\n",
      "           1     0.8909    0.8033    0.8448        61\n",
      "           2     0.9015    0.9444    0.9225       126\n",
      "\n",
      "    accuracy                         0.8700       200\n",
      "   macro avg     0.7513    0.7364    0.7429       200\n",
      "weighted avg     0.8697    0.8700    0.8688       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ae00c3c78557fb9d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1075e5fb4da15629.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.7795, 'learning_rate': 1.894736842105263e-05, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:09, 12.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5739431977272034, 'eval_f1': 0.5198094802123183, 'eval_runtime': 0.1937, 'eval_samples_per_second': 1032.594, 'eval_steps_per_second': 67.119, 'epoch': 2.0}\n",
      " 10% 100/1000 [00:08<01:09, 12.87it/s]\n",
      "100% 13/13 [00:00<00:00, 78.68it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700] due to args.save_total_limit\n",
      "{'loss': 0.3229, 'learning_rate': 1.6863157894736844e-05, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:18<01:04, 12.44it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5005933046340942, 'eval_f1': 0.6543437020867441, 'eval_runtime': 0.2224, 'eval_samples_per_second': 899.228, 'eval_steps_per_second': 58.45, 'epoch': 4.0}\n",
      " 20% 200/1000 [00:19<01:04, 12.44it/s]\n",
      "100% 13/13 [00:00<00:00, 69.14it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-100] due to args.save_total_limit\n",
      "{'loss': 0.1089, 'learning_rate': 1.4778947368421055e-05, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:47, 14.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5299685001373291, 'eval_f1': 0.7541907479795679, 'eval_runtime': 0.1955, 'eval_samples_per_second': 1022.89, 'eval_steps_per_second': 66.488, 'epoch': 6.0}\n",
      " 30% 300/1000 [00:29<00:47, 14.73it/s]\n",
      "100% 13/13 [00:00<00:00, 77.16it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0575, 'learning_rate': 1.2673684210526315e-05, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:40<00:52, 11.50it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6222032308578491, 'eval_f1': 0.7722437406647934, 'eval_runtime': 0.1948, 'eval_samples_per_second': 1026.618, 'eval_steps_per_second': 66.73, 'epoch': 8.0}\n",
      " 40% 400/1000 [00:41<00:52, 11.50it/s]\n",
      "100% 13/13 [00:00<00:00, 79.81it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-300] due to args.save_total_limit\n",
      "{'loss': 0.0335, 'learning_rate': 1.0568421052631579e-05, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:39, 12.74it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.662405788898468, 'eval_f1': 0.7762577228596647, 'eval_runtime': 0.2328, 'eval_samples_per_second': 859.242, 'eval_steps_per_second': 55.851, 'epoch': 10.0}\n",
      " 50% 500/1000 [00:51<00:39, 12.74it/s]\n",
      "100% 13/13 [00:00<00:00, 67.80it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.011, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:33, 11.77it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.784401535987854, 'eval_f1': 0.7643636080386353, 'eval_runtime': 0.1924, 'eval_samples_per_second': 1039.617, 'eval_steps_per_second': 67.575, 'epoch': 12.0}\n",
      " 60% 600/1000 [01:02<00:33, 11.77it/s]\n",
      "100% 13/13 [00:00<00:00, 79.75it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 0.0049, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:23, 12.55it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7701503038406372, 'eval_f1': 0.7790511076225362, 'eval_runtime': 0.1993, 'eval_samples_per_second': 1003.552, 'eval_steps_per_second': 65.231, 'epoch': 14.0}\n",
      " 70% 700/1000 [01:13<00:23, 12.55it/s]\n",
      "100% 13/13 [00:00<00:00, 76.76it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-500] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0042, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:18, 10.98it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.8324058651924133, 'eval_f1': 0.7712144702842377, 'eval_runtime': 0.2263, 'eval_samples_per_second': 883.66, 'eval_steps_per_second': 57.438, 'epoch': 16.0}\n",
      " 80% 800/1000 [01:24<00:18, 10.98it/s]\n",
      "100% 13/13 [00:00<00:00, 67.17it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0036, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:07, 14.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.8440954685211182, 'eval_f1': 0.7712144702842377, 'eval_runtime': 0.1991, 'eval_samples_per_second': 1004.425, 'eval_steps_per_second': 65.288, 'epoch': 18.0}\n",
      " 90% 900/1000 [01:35<00:07, 14.00it/s]\n",
      "100% 13/13 [00:00<00:00, 73.29it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0034, 'learning_rate': 4.2105263157894737e-08, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:46<00:00, 14.06it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "\n",
      "  0% 0/13 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.846476674079895, 'eval_f1': 0.7712144702842377, 'eval_runtime': 0.1933, 'eval_samples_per_second': 1034.878, 'eval_steps_per_second': 67.267, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:46<00:00, 14.06it/s]\n",
      "100% 13/13 [00:00<00:00, 83.23it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-700 (score: 0.7790511076225362).\n",
      "{'train_runtime': 109.6419, 'train_samples_per_second': 145.93, 'train_steps_per_second': 9.121, 'train_loss': 0.13297181046009063, 'epoch': 20.0}\n",
      "100% 1000/1000 [01:49<00:00, 14.06it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b16/checkpoint-1000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "100% 1000/1000 [01:50<00:00,  9.09it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 16\n",
      "100% 13/13 [00:00<00:00, 53.01it/s]\n",
      "f1 0.7790511076225362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7778    0.5833    0.6667        24\n",
      "           1     0.7255    0.7872    0.7551        47\n",
      "           2     0.9084    0.9225    0.9154       129\n",
      "\n",
      "    accuracy                         0.8500       200\n",
      "   macro avg     0.8039    0.7643    0.7791       200\n",
      "weighted avg     0.8497    0.8500    0.8479       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7666301928532269\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-38\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=default-lr2e-5-b16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOv3NXGV8pkY",
    "outputId": "9006e46c-c22c-4838-d6ae-71c551063db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 21:21:56.113900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 21:21:58.040026: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 21:21:58.040176: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 21:21:58.040196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 180.53it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2dd56238ea05cc5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6305b363f5daf9a8.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/2000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.8211, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:52, 16.06it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 88.78it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5514624118804932, 'eval_f1': 0.5387991908088096, 'eval_runtime': 0.3626, 'eval_samples_per_second': 551.573, 'eval_steps_per_second': 68.947, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:52, 16.06it/s]\n",
      "100% 25/25 [00:00<00:00, 84.73it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.3833, 'learning_rate': 8.43684210526316e-06, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:27<01:28, 18.04it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.75it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5409477353096008, 'eval_f1': 0.7230585147444805, 'eval_runtime': 0.257, 'eval_samples_per_second': 778.298, 'eval_steps_per_second': 97.287, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:27<01:28, 18.04it/s]\n",
      "100% 25/25 [00:00<00:00, 103.34it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1687, 'learning_rate': 7.384210526315791e-06, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:21, 17.22it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 113.93it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6216580867767334, 'eval_f1': 0.7506012506012505, 'eval_runtime': 0.2549, 'eval_samples_per_second': 784.616, 'eval_steps_per_second': 98.077, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:21, 17.22it/s]\n",
      "100% 25/25 [00:00<00:00, 103.82it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0941, 'learning_rate': 6.331578947368422e-06, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:09, 17.23it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.662159264087677, 'eval_f1': 0.7644468533574225, 'eval_runtime': 0.2484, 'eval_samples_per_second': 805.135, 'eval_steps_per_second': 100.642, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:09, 17.23it/s]\n",
      "100% 25/25 [00:00<00:00, 122.51it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0757, 'learning_rate': 5.278947368421054e-06, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:14<00:56, 17.81it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6891369819641113, 'eval_f1': 0.7958391102762498, 'eval_runtime': 0.2502, 'eval_samples_per_second': 799.314, 'eval_steps_per_second': 99.914, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:14<00:56, 17.81it/s]\n",
      "100% 25/25 [00:00<00:00, 124.64it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0535, 'learning_rate': 4.226315789473685e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:29<00:45, 17.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.28it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6885795593261719, 'eval_f1': 0.7764364184597646, 'eval_runtime': 0.2659, 'eval_samples_per_second': 752.111, 'eval_steps_per_second': 94.014, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:30<00:45, 17.69it/s]\n",
      "100% 25/25 [00:00<00:00, 102.30it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/special_tokens_map.json\n",
      "{'loss': 0.0375, 'learning_rate': 3.173684210526316e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:45<00:37, 15.91it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6971844434738159, 'eval_f1': 0.7966766367563974, 'eval_runtime': 0.256, 'eval_samples_per_second': 781.358, 'eval_steps_per_second': 97.67, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:45<00:37, 15.91it/s]\n",
      "100% 25/25 [00:00<00:00, 120.46it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0277, 'learning_rate': 2.1210526315789476e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:00<00:23, 16.72it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 113.32it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7356889843940735, 'eval_f1': 0.7929670555886394, 'eval_runtime': 0.2579, 'eval_samples_per_second': 775.518, 'eval_steps_per_second': 96.94, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:01<00:23, 16.72it/s]\n",
      "100% 25/25 [00:00<00:00, 102.66it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/special_tokens_map.json\n",
      "{'loss': 0.0219, 'learning_rate': 1.068421052631579e-06, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:16<00:11, 17.84it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7290483117103577, 'eval_f1': 0.8048287233326604, 'eval_runtime': 0.2484, 'eval_samples_per_second': 805.087, 'eval_steps_per_second': 100.636, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:16<00:11, 17.84it/s]\n",
      "100% 25/25 [00:00<00:00, 120.56it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.0143, 'learning_rate': 1.578947368421053e-08, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:32<00:00, 17.79it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7327249646186829, 'eval_f1': 0.8009722897423431, 'eval_runtime': 0.246, 'eval_samples_per_second': 813.097, 'eval_steps_per_second': 101.637, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:32<00:00, 17.79it/s]\n",
      "100% 25/25 [00:00<00:00, 122.72it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800 (score: 0.8048287233326604).\n",
      "{'train_runtime': 155.664, 'train_samples_per_second': 102.785, 'train_steps_per_second': 12.848, 'train_loss': 0.1697722922563553, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:35<00:00, 17.79it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 58 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 58 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "100% 2000/2000 [02:36<00:00, 12.81it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 75.38it/s]\n",
      "f1 0.8048287233326604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6429    0.7500    0.6923        12\n",
      "           1     0.8596    0.7778    0.8167        63\n",
      "           2     0.8915    0.9200    0.9055       125\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.7980    0.8159    0.8048       200\n",
      "weighted avg     0.8665    0.8650    0.8647       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d5256619d5ecb4f8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-468186b3490e9025.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8137, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:59, 15.07it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 107.17it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5545111298561096, 'eval_f1': 0.5419896640826875, 'eval_runtime': 0.2507, 'eval_samples_per_second': 797.712, 'eval_steps_per_second': 99.714, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:13<01:59, 15.07it/s]\n",
      "100% 25/25 [00:00<00:00, 110.57it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800] due to args.save_total_limit\n",
      "{'loss': 0.3707, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:40, 15.95it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 113.21it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4801065921783447, 'eval_f1': 0.6777438419229463, 'eval_runtime': 0.2472, 'eval_samples_per_second': 809.126, 'eval_steps_per_second': 101.141, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:40, 15.95it/s]\n",
      "100% 25/25 [00:00<00:00, 112.03it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1616, 'learning_rate': 7.384210526315791e-06, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:44<01:30, 15.42it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.31it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5584075450897217, 'eval_f1': 0.7170566608253445, 'eval_runtime': 0.2547, 'eval_samples_per_second': 785.132, 'eval_steps_per_second': 98.141, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:44<01:30, 15.42it/s]\n",
      "100% 25/25 [00:00<00:00, 104.72it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0839, 'learning_rate': 6.331578947368422e-06, 'epoch': 8.0}\n",
      " 40% 800/2000 [01:00<01:23, 14.41it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 95.17it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7087064981460571, 'eval_f1': 0.6975095785440613, 'eval_runtime': 0.2705, 'eval_samples_per_second': 739.406, 'eval_steps_per_second': 92.426, 'epoch': 8.0}\n",
      " 40% 800/2000 [01:00<01:23, 14.41it/s]\n",
      "100% 25/25 [00:00<00:00, 101.57it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0544, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:15<01:12, 13.85it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 73.63it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 69.52it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7922767400741577, 'eval_f1': 0.7060515543754841, 'eval_runtime': 0.3914, 'eval_samples_per_second': 511.002, 'eval_steps_per_second': 63.875, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:15<01:12, 13.85it/s]\n",
      "100% 25/25 [00:00<00:00, 68.94it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0406, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:31<00:56, 14.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 76.99it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 71.86it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8465726971626282, 'eval_f1': 0.6977099236641221, 'eval_runtime': 0.3797, 'eval_samples_per_second': 526.723, 'eval_steps_per_second': 65.84, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:31<00:56, 14.12it/s]\n",
      "100% 25/25 [00:00<00:00, 70.60it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0199, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:47<00:44, 13.38it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 78.53it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 71.90it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8928176760673523, 'eval_f1': 0.6997882402033951, 'eval_runtime': 0.3791, 'eval_samples_per_second': 527.559, 'eval_steps_per_second': 65.945, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:47<00:44, 13.38it/s]\n",
      "100% 25/25 [00:00<00:00, 70.15it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0129, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:03<00:27, 14.30it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 79.20it/s]\u001B[A\n",
      " 68% 17/25 [00:00<00:00, 74.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8886568546295166, 'eval_f1': 0.7039376741893903, 'eval_runtime': 0.3686, 'eval_samples_per_second': 542.631, 'eval_steps_per_second': 67.829, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:03<00:27, 14.30it/s]\n",
      "100% 25/25 [00:00<00:00, 72.65it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600 (score: 0.7170566608253445).\n",
      "{'train_runtime': 127.1685, 'train_samples_per_second': 125.817, 'train_steps_per_second': 15.727, 'train_loss': 0.19473589211702347, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:07<00:27, 14.30it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      " 80% 1600/2000 [02:07<00:31, 12.54it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 83.89it/s]\n",
      "f1 0.7170566608253445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.4286    0.5000        14\n",
      "           1     0.7455    0.7593    0.7523        54\n",
      "           2     0.8889    0.9091    0.8989       132\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.7448    0.6990    0.7171       200\n",
      "weighted avg     0.8299    0.8350    0.8314       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-88aa98cae448eb95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-80d401ff1a69a5b8.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.7928, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:37, 18.40it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.33it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.60787034034729, 'eval_f1': 0.48866638854123207, 'eval_runtime': 0.2652, 'eval_samples_per_second': 754.088, 'eval_steps_per_second': 94.261, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:37, 18.40it/s]\n",
      "100% 25/25 [00:00<00:00, 99.23it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.3842, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:33, 17.10it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5211350917816162, 'eval_f1': 0.6815981335666375, 'eval_runtime': 0.2719, 'eval_samples_per_second': 735.616, 'eval_steps_per_second': 91.952, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:33, 17.10it/s]\n",
      "100% 25/25 [00:00<00:00, 125.56it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1773, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:23, 16.78it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.97it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5778768062591553, 'eval_f1': 0.7402222222222221, 'eval_runtime': 0.2636, 'eval_samples_per_second': 758.678, 'eval_steps_per_second': 94.835, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:23, 16.78it/s]\n",
      "100% 25/25 [00:00<00:00, 99.94it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0914, 'learning_rate': 6.331578947368422e-06, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:59<01:07, 17.65it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.03it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6417542099952698, 'eval_f1': 0.7348873348873348, 'eval_runtime': 0.2657, 'eval_samples_per_second': 752.631, 'eval_steps_per_second': 94.079, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:59<01:07, 17.65it/s]\n",
      "100% 25/25 [00:00<00:00, 99.05it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0595, 'learning_rate': 5.278947368421054e-06, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:15<00:57, 17.32it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6689913868904114, 'eval_f1': 0.7447802446444859, 'eval_runtime': 0.2684, 'eval_samples_per_second': 745.208, 'eval_steps_per_second': 93.151, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:15<00:57, 17.32it/s]\n",
      "100% 25/25 [00:00<00:00, 120.75it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0411, 'learning_rate': 4.226315789473685e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:31<00:45, 17.41it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.76it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7560861110687256, 'eval_f1': 0.7511802134646963, 'eval_runtime': 0.269, 'eval_samples_per_second': 743.554, 'eval_steps_per_second': 92.944, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:31<00:45, 17.41it/s]\n",
      "100% 25/25 [00:00<00:00, 97.07it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0259, 'learning_rate': 3.173684210526316e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:46<00:35, 17.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.758423924446106, 'eval_f1': 0.7249303583502367, 'eval_runtime': 0.2585, 'eval_samples_per_second': 773.569, 'eval_steps_per_second': 96.696, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:47<00:35, 17.02it/s]\n",
      "100% 25/25 [00:00<00:00, 120.98it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/special_tokens_map.json\n",
      "{'loss': 0.0196, 'learning_rate': 2.1210526315789476e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:02<00:22, 17.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7733813524246216, 'eval_f1': 0.7362478701461752, 'eval_runtime': 0.2588, 'eval_samples_per_second': 772.896, 'eval_steps_per_second': 96.612, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:02<00:22, 17.57it/s]\n",
      "100% 25/25 [00:00<00:00, 121.03it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.017, 'learning_rate': 1.068421052631579e-06, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:18<00:12, 16.61it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.02it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8139893412590027, 'eval_f1': 0.7422524461540809, 'eval_runtime': 0.2633, 'eval_samples_per_second': 759.592, 'eval_steps_per_second': 94.949, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:18<00:12, 16.61it/s]\n",
      "100% 25/25 [00:00<00:00, 100.89it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.0189, 'learning_rate': 1.578947368421053e-08, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:33<00:00, 16.60it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8021436929702759, 'eval_f1': 0.7512095747389865, 'eval_runtime': 0.2492, 'eval_samples_per_second': 802.66, 'eval_steps_per_second': 100.333, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:34<00:00, 16.60it/s]\n",
      "100% 25/25 [00:00<00:00, 126.38it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000 (score: 0.7512095747389865).\n",
      "{'train_runtime': 157.3573, 'train_samples_per_second': 101.679, 'train_steps_per_second': 12.71, 'train_loss': 0.1627843371629715, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:37<00:00, 16.60it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "100% 2000/2000 [02:37<00:00, 12.69it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 83.77it/s]\n",
      "f1 0.7512095747389865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.5000    0.5714        16\n",
      "           1     0.7188    0.8364    0.7731        55\n",
      "           2     0.9274    0.8915    0.9091       129\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.7709    0.7426    0.7512       200\n",
      "weighted avg     0.8492    0.8450    0.8447       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-922ddddff4b607c8.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8105, 'learning_rate': 9.473684210526315e-06, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<02:08, 14.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 78.20it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 69.77it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5274975299835205, 'eval_f1': 0.5466784281603764, 'eval_runtime': 0.3936, 'eval_samples_per_second': 508.179, 'eval_steps_per_second': 63.522, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<02:08, 14.02it/s]\n",
      "100% 25/25 [00:00<00:00, 69.68it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000] due to args.save_total_limit\n",
      "{'loss': 0.4166, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:27<01:50, 14.47it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 84.64it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4650057256221771, 'eval_f1': 0.6931086803196985, 'eval_runtime': 0.3536, 'eval_samples_per_second': 565.628, 'eval_steps_per_second': 70.703, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:50, 14.47it/s]\n",
      "100% 25/25 [00:00<00:00, 75.75it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1947, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:36, 14.53it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 76.35it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 71.00it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.49097535014152527, 'eval_f1': 0.7501433045326175, 'eval_runtime': 0.3816, 'eval_samples_per_second': 524.052, 'eval_steps_per_second': 65.506, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:36, 14.53it/s]\n",
      "100% 25/25 [00:00<00:00, 71.94it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.08, 'learning_rate': 6.326315789473685e-06, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:20, 14.94it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 76.06it/s]\u001B[A\n",
      " 68% 17/25 [00:00<00:00, 70.30it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5642990469932556, 'eval_f1': 0.7453990990112663, 'eval_runtime': 0.382, 'eval_samples_per_second': 523.556, 'eval_steps_per_second': 65.445, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:20, 14.94it/s]\n",
      "100% 25/25 [00:00<00:00, 69.50it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.054, 'learning_rate': 5.278947368421054e-06, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:13<01:03, 15.66it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 96.13it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6716159582138062, 'eval_f1': 0.7386281796445137, 'eval_runtime': 0.33, 'eval_samples_per_second': 606.102, 'eval_steps_per_second': 75.763, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:13<01:03, 15.66it/s]\n",
      "100% 25/25 [00:00<00:00, 85.38it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0441, 'learning_rate': 4.226315789473685e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:28<00:50, 15.89it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 94.19it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.65952467918396, 'eval_f1': 0.7304107010822074, 'eval_runtime': 0.3218, 'eval_samples_per_second': 621.502, 'eval_steps_per_second': 77.688, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:28<00:50, 15.89it/s]\n",
      "100% 25/25 [00:00<00:00, 81.15it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0286, 'learning_rate': 3.173684210526316e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:43<00:35, 17.10it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6437852382659912, 'eval_f1': 0.7681689274609629, 'eval_runtime': 0.2486, 'eval_samples_per_second': 804.585, 'eval_steps_per_second': 100.573, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:44<00:35, 17.10it/s]\n",
      "100% 25/25 [00:00<00:00, 120.57it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0222, 'learning_rate': 2.1210526315789476e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [01:59<00:24, 16.47it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6538577079772949, 'eval_f1': 0.7832852738144278, 'eval_runtime': 0.2472, 'eval_samples_per_second': 809.14, 'eval_steps_per_second': 101.143, 'epoch': 16.0}\n",
      " 80% 1600/2000 [01:59<00:24, 16.47it/s]\n",
      "100% 25/25 [00:00<00:00, 120.82it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.0191, 'learning_rate': 1.068421052631579e-06, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:14<00:11, 17.01it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.30it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6658748388290405, 'eval_f1': 0.7832852738144278, 'eval_runtime': 0.2536, 'eval_samples_per_second': 788.505, 'eval_steps_per_second': 98.563, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:15<00:11, 17.01it/s]\n",
      "100% 25/25 [00:00<00:00, 104.61it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/special_tokens_map.json\n",
      "{'loss': 0.0171, 'learning_rate': 1.578947368421053e-08, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:30<00:00, 17.54it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.11it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6686989068984985, 'eval_f1': 0.7832852738144278, 'eval_runtime': 0.2572, 'eval_samples_per_second': 777.668, 'eval_steps_per_second': 97.208, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:30<00:00, 17.54it/s]\n",
      "100% 25/25 [00:00<00:00, 102.89it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600 (score: 0.7832852738144278).\n",
      "{'train_runtime': 153.7894, 'train_samples_per_second': 104.038, 'train_steps_per_second': 13.005, 'train_loss': 0.16867913842201232, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:33<00:00, 17.54it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "100% 2000/2000 [02:34<00:00, 12.97it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 70.38it/s]\n",
      "f1 0.7832852738144278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.6154    0.5926        13\n",
      "           1     0.8889    0.7869    0.8348        61\n",
      "           2     0.9015    0.9444    0.9225       126\n",
      "\n",
      "    accuracy                         0.8750       200\n",
      "   macro avg     0.7873    0.7822    0.7833       200\n",
      "weighted avg     0.8762    0.8750    0.8743       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ae00c3c78557fb9d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1075e5fb4da15629.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8014, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:41, 17.71it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 102.41it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6033025979995728, 'eval_f1': 0.5013627645613917, 'eval_runtime': 0.2662, 'eval_samples_per_second': 751.189, 'eval_steps_per_second': 93.899, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:41, 17.71it/s]\n",
      "100% 25/25 [00:00<00:00, 107.03it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.3796, 'learning_rate': 8.43684210526316e-06, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:31, 17.51it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.07it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5482571125030518, 'eval_f1': 0.6354475789216585, 'eval_runtime': 0.2483, 'eval_samples_per_second': 805.53, 'eval_steps_per_second': 100.691, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:31, 17.51it/s]\n",
      "100% 25/25 [00:00<00:00, 112.62it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.162, 'learning_rate': 7.3894736842105275e-06, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:21, 17.23it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.17it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.576296865940094, 'eval_f1': 0.7523570920303606, 'eval_runtime': 0.2717, 'eval_samples_per_second': 736.075, 'eval_steps_per_second': 92.009, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:44<01:21, 17.23it/s]\n",
      "100% 25/25 [00:00<00:00, 99.97it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0879, 'learning_rate': 6.336842105263158e-06, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:59<01:16, 15.77it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.17it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6951383948326111, 'eval_f1': 0.7577310183069158, 'eval_runtime': 0.2561, 'eval_samples_per_second': 780.989, 'eval_steps_per_second': 97.624, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:59<01:16, 15.77it/s]\n",
      "100% 25/25 [00:00<00:00, 108.39it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0699, 'learning_rate': 5.2842105263157896e-06, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:15<01:03, 15.76it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 105.45it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7701203227043152, 'eval_f1': 0.7620706506875418, 'eval_runtime': 0.2602, 'eval_samples_per_second': 768.673, 'eval_steps_per_second': 96.084, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:15<01:03, 15.76it/s]\n",
      "100% 25/25 [00:00<00:00, 106.10it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0449, 'learning_rate': 4.2315789473684215e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:30<00:59, 13.54it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 93.51it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8099259734153748, 'eval_f1': 0.7384707615865015, 'eval_runtime': 0.2844, 'eval_samples_per_second': 703.182, 'eval_steps_per_second': 87.898, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:31<00:59, 13.54it/s]\n",
      "100% 25/25 [00:00<00:00, 100.56it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200/special_tokens_map.json\n",
      "{'loss': 0.0255, 'learning_rate': 3.178947368421053e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:46<00:42, 14.15it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 82.80it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8088752031326294, 'eval_f1': 0.7741938616938616, 'eval_runtime': 0.3927, 'eval_samples_per_second': 509.354, 'eval_steps_per_second': 63.669, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:46<00:42, 14.15it/s]\n",
      "100% 25/25 [00:00<00:00, 73.88it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0172, 'learning_rate': 2.1263157894736844e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:02<00:29, 13.78it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.91it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 75.43it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8764904737472534, 'eval_f1': 0.7202927985106203, 'eval_runtime': 0.362, 'eval_samples_per_second': 552.413, 'eval_steps_per_second': 69.052, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:02<00:29, 13.78it/s]\n",
      "100% 25/25 [00:00<00:00, 74.24it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600/special_tokens_map.json\n",
      "{'loss': 0.0128, 'learning_rate': 1.0736842105263159e-06, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:18<00:13, 14.30it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 28% 7/25 [00:00<00:00, 65.92it/s]\u001B[A\n",
      " 56% 14/25 [00:00<00:00, 65.69it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9049617052078247, 'eval_f1': 0.7580714908013304, 'eval_runtime': 0.4047, 'eval_samples_per_second': 494.154, 'eval_steps_per_second': 61.769, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:18<00:13, 14.30it/s]\n",
      "100% 25/25 [00:00<00:00, 64.76it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.0112, 'learning_rate': 2.1052631578947368e-08, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:33<00:00, 14.09it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 79.74it/s]\u001B[A\n",
      " 68% 17/25 [00:00<00:00, 75.44it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8938653469085693, 'eval_f1': 0.7580714908013304, 'eval_runtime': 0.3643, 'eval_samples_per_second': 548.946, 'eval_steps_per_second': 68.618, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:34<00:00, 14.09it/s]\n",
      "100% 25/25 [00:00<00:00, 73.17it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-1400 (score: 0.7741938616938616).\n",
      "{'train_runtime': 157.394, 'train_samples_per_second': 101.656, 'train_steps_per_second': 12.707, 'train_loss': 0.1612264095544815, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:37<00:00, 14.09it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-b8/checkpoint-2000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "100% 2000/2000 [02:37<00:00, 12.68it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 93.75it/s] \n",
      "f1 0.7741938616938616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.5833    0.6364        24\n",
      "           1     0.7551    0.7872    0.7708        47\n",
      "           2     0.9084    0.9225    0.9154       129\n",
      "\n",
      "    accuracy                         0.8500       200\n",
      "   macro avg     0.7878    0.7643    0.7742       200\n",
      "weighted avg     0.8474    0.8500    0.8479       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7683137461074807\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-39\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=default-b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHMrlJ4x8tik",
    "outputId": "a0d689c6-1ccb-454a-83eb-f2093afd8c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-14 21:45:52.189487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 21:45:54.578619: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 21:45:54.578765: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-14 21:45:54.578785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 177.27it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-706cff7ce1e503f5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2dd56238ea05cc5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6305b363f5daf9a8.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/2000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.7159, 'learning_rate': 1.8957894736842106e-05, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:13<01:48, 16.52it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.68it/s]\u001B[A\n",
      " 96% 24/25 [00:00<00:00, 103.77it/s]\u001B[A\n",
      "{'eval_loss': 0.49963831901550293, 'eval_f1': 0.7320768662232077, 'eval_runtime': 0.2712, 'eval_samples_per_second': 737.358, 'eval_steps_per_second': 92.17, 'epoch': 2.0}\n",
      "\n",
      " 10% 200/2000 [00:13<01:48, 16.52it/s]\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.2449, 'learning_rate': 1.687368421052632e-05, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:27, 18.21it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 52% 13/25 [00:00<00:00, 122.34it/s]\u001B[A\n",
      "{'eval_loss': 0.6650695204734802, 'eval_f1': 0.7847257412474805, 'eval_runtime': 0.244, 'eval_samples_per_second': 819.72, 'eval_steps_per_second': 102.465, 'epoch': 4.0}\n",
      "\n",
      " 20% 400/2000 [00:28<01:27, 18.21it/s]\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1193, 'learning_rate': 1.4778947368421055e-05, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:44<01:20, 17.40it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.71it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7030768394470215, 'eval_f1': 0.7946988973706531, 'eval_runtime': 0.2499, 'eval_samples_per_second': 800.412, 'eval_steps_per_second': 100.052, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:44<01:20, 17.40it/s]\n",
      "100% 25/25 [00:00<00:00, 106.75it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0662, 'learning_rate': 1.2673684210526315e-05, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:59<01:11, 16.78it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7716001272201538, 'eval_f1': 0.780093664762493, 'eval_runtime': 0.2493, 'eval_samples_per_second': 802.097, 'eval_steps_per_second': 100.262, 'epoch': 8.0}\n",
      " 40% 800/2000 [01:00<01:11, 16.78it/s]\n",
      "100% 25/25 [00:00<00:00, 121.33it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0218, 'learning_rate': 1.0568421052631579e-05, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:15<00:56, 17.82it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.09it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.851157009601593, 'eval_f1': 0.7659412154666684, 'eval_runtime': 0.2577, 'eval_samples_per_second': 776.093, 'eval_steps_per_second': 97.012, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:15<00:56, 17.82it/s]\n",
      "100% 25/25 [00:00<00:00, 103.69it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0067, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:31<00:47, 16.86it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8898091912269592, 'eval_f1': 0.7695814338997099, 'eval_runtime': 0.2409, 'eval_samples_per_second': 830.079, 'eval_steps_per_second': 103.76, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:31<00:47, 16.86it/s]\n",
      "100% 25/25 [00:00<00:00, 126.23it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0019, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:46<00:34, 17.23it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9229980707168579, 'eval_f1': 0.7651481645289695, 'eval_runtime': 0.2484, 'eval_samples_per_second': 805.111, 'eval_steps_per_second': 100.639, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:47<00:34, 17.23it/s]\n",
      "100% 25/25 [00:00<00:00, 128.47it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0008, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:02<00:23, 16.88it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9535444378852844, 'eval_f1': 0.7651481645289695, 'eval_runtime': 0.2448, 'eval_samples_per_second': 817.013, 'eval_steps_per_second': 102.127, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:02<00:23, 16.88it/s]\n",
      "100% 25/25 [00:00<00:00, 124.73it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600 (score: 0.7946988973706531).\n",
      "{'train_runtime': 125.9859, 'train_samples_per_second': 126.998, 'train_steps_per_second': 15.875, 'train_loss': 0.14719995610415937, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:05<00:23, 16.88it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 72 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 72 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      " 80% 1600/2000 [02:06<00:31, 12.64it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 84.09it/s]\n",
      "f1 0.7946988973706531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.5000    0.6667        12\n",
      "           1     0.8596    0.7778    0.8167        63\n",
      "           2     0.8613    0.9440    0.9008       125\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.9070    0.7406    0.7947       200\n",
      "weighted avg     0.8691    0.8650    0.8602       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d5256619d5ecb4f8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-468186b3490e9025.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.7113, 'learning_rate': 1.8957894736842106e-05, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:11<01:51, 16.09it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 83.27it/s]\u001B[A\n",
      " 72% 18/25 [00:00<00:00, 79.55it/s]\u001B[A\n",
      "{'eval_loss': 0.500714898109436, 'eval_f1': 0.5592592592592593, 'eval_runtime': 0.3295, 'eval_samples_per_second': 606.987, 'eval_steps_per_second': 75.873, 'epoch': 2.0}\n",
      "\n",
      " 10% 200/2000 [00:12<01:51, 16.09it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.2267, 'learning_rate': 1.687368421052632e-05, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:27<01:34, 16.92it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 108.11it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6007480621337891, 'eval_f1': 0.6963281067945003, 'eval_runtime': 0.2594, 'eval_samples_per_second': 771.093, 'eval_steps_per_second': 96.387, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:27<01:34, 16.92it/s]\n",
      "100% 25/25 [00:00<00:00, 108.61it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0734, 'learning_rate': 1.4768421052631581e-05, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:42<01:19, 17.52it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 110.50it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6588139533996582, 'eval_f1': 0.7079097556625648, 'eval_runtime': 0.2577, 'eval_samples_per_second': 776.043, 'eval_steps_per_second': 97.005, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:42<01:19, 17.52it/s]\n",
      "100% 25/25 [00:00<00:00, 103.72it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0321, 'learning_rate': 1.2663157894736843e-05, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:57<01:06, 18.09it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 109.19it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.743510365486145, 'eval_f1': 0.7330371383002962, 'eval_runtime': 0.2551, 'eval_samples_per_second': 783.932, 'eval_steps_per_second': 97.992, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:06, 18.09it/s]\n",
      "100% 25/25 [00:00<00:00, 106.87it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0063, 'learning_rate': 1.0557894736842107e-05, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:13<00:58, 17.03it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.59it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8569335341453552, 'eval_f1': 0.7076500936734401, 'eval_runtime': 0.2452, 'eval_samples_per_second': 815.769, 'eval_steps_per_second': 101.971, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:13<00:58, 17.03it/s]\n",
      "100% 25/25 [00:00<00:00, 112.69it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.001, 'learning_rate': 8.45263157894737e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:29<00:44, 18.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.45it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8994061350822449, 'eval_f1': 0.7407321162444113, 'eval_runtime': 0.2422, 'eval_samples_per_second': 825.626, 'eval_steps_per_second': 103.203, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:29<00:44, 18.00it/s]\n",
      "100% 25/25 [00:00<00:00, 111.64it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0007, 'learning_rate': 6.347368421052632e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:44<00:35, 16.95it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 104.13it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8832808136940002, 'eval_f1': 0.7577370664823135, 'eval_runtime': 0.2549, 'eval_samples_per_second': 784.649, 'eval_steps_per_second': 98.081, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:44<00:35, 16.95it/s]\n",
      "100% 25/25 [00:00<00:00, 105.78it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0006, 'learning_rate': 4.242105263157895e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:00<00:22, 17.86it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.21it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8873419165611267, 'eval_f1': 0.7542231464756092, 'eval_runtime': 0.2435, 'eval_samples_per_second': 821.47, 'eval_steps_per_second': 102.684, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:00<00:22, 17.86it/s]\n",
      "100% 25/25 [00:00<00:00, 110.34it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/special_tokens_map.json\n",
      "{'loss': 0.0008, 'learning_rate': 2.136842105263158e-06, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:15<00:11, 17.75it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 104.19it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9157407879829407, 'eval_f1': 0.7540740386226039, 'eval_runtime': 0.2537, 'eval_samples_per_second': 788.33, 'eval_steps_per_second': 98.541, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:16<00:11, 17.75it/s]\n",
      "100% 25/25 [00:00<00:00, 107.59it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.0005, 'learning_rate': 3.157894736842106e-08, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:31<00:00, 17.91it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 106.58it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.910848081111908, 'eval_f1': 0.7494051107871574, 'eval_runtime': 0.2453, 'eval_samples_per_second': 815.429, 'eval_steps_per_second': 101.929, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:31<00:00, 17.91it/s]\n",
      "100% 25/25 [00:00<00:00, 111.51it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400 (score: 0.7577370664823135).\n",
      "{'train_runtime': 154.9363, 'train_samples_per_second': 103.268, 'train_steps_per_second': 12.909, 'train_loss': 0.10534306577593087, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:34<00:00, 17.91it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 64 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 64 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "100% 2000/2000 [02:35<00:00, 12.87it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 88.15it/s]\n",
      "f1 0.7577370664823135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.4286    0.5455        14\n",
      "           1     0.7541    0.8519    0.8000        54\n",
      "           2     0.9313    0.9242    0.9278       132\n",
      "\n",
      "    accuracy                         0.8700       200\n",
      "   macro avg     0.8118    0.7349    0.7577       200\n",
      "weighted avg     0.8708    0.8700    0.8665       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-88aa98cae448eb95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-80d401ff1a69a5b8.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.7234, 'learning_rate': 1.8957894736842106e-05, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<02:05, 14.39it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 86.23it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.47116583585739136, 'eval_f1': 0.5645884604867377, 'eval_runtime': 0.2895, 'eval_samples_per_second': 690.747, 'eval_steps_per_second': 86.343, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<02:05, 14.39it/s]\n",
      "100% 25/25 [00:00<00:00, 89.99it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.2418, 'learning_rate': 1.688421052631579e-05, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:27<01:50, 14.47it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 80.19it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6227686405181885, 'eval_f1': 0.6865618042088629, 'eval_runtime': 0.3739, 'eval_samples_per_second': 534.933, 'eval_steps_per_second': 66.867, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:50, 14.47it/s]\n",
      "100% 25/25 [00:00<00:00, 70.98it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1097, 'learning_rate': 1.4778947368421055e-05, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:38, 14.21it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 84.88it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5825380086898804, 'eval_f1': 0.7841170963560421, 'eval_runtime': 0.3763, 'eval_samples_per_second': 531.509, 'eval_steps_per_second': 66.439, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:38, 14.21it/s]\n",
      "100% 25/25 [00:00<00:00, 74.28it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0584, 'learning_rate': 1.2673684210526315e-05, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:22, 14.60it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.16it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 75.37it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6516977548599243, 'eval_f1': 0.8288302257069396, 'eval_runtime': 0.3722, 'eval_samples_per_second': 537.275, 'eval_steps_per_second': 67.159, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:22, 14.60it/s]\n",
      "100% 25/25 [00:00<00:00, 70.80it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0251, 'learning_rate': 1.0568421052631579e-05, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:13<01:09, 14.32it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 74.90it/s]\u001B[A\n",
      " 68% 17/25 [00:00<00:00, 70.27it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.705669105052948, 'eval_f1': 0.7904403658638971, 'eval_runtime': 0.3885, 'eval_samples_per_second': 514.815, 'eval_steps_per_second': 64.352, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:14<01:09, 14.32it/s]\n",
      "100% 25/25 [00:00<00:00, 68.09it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.014, 'learning_rate': 8.463157894736843e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:28<00:54, 14.78it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 92.03it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7676721215248108, 'eval_f1': 0.7848591296867159, 'eval_runtime': 0.3383, 'eval_samples_per_second': 591.269, 'eval_steps_per_second': 73.909, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:29<00:54, 14.78it/s]\n",
      "100% 25/25 [00:00<00:00, 79.07it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0011, 'learning_rate': 6.357894736842106e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:43<00:39, 15.37it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 92.45it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8197073340415955, 'eval_f1': 0.7992170348351498, 'eval_runtime': 0.3338, 'eval_samples_per_second': 599.168, 'eval_steps_per_second': 74.896, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:44<00:39, 15.37it/s]\n",
      "100% 25/25 [00:00<00:00, 82.24it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.001, 'learning_rate': 4.252631578947369e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:01<00:30, 13.32it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 75.11it/s]\u001B[A\n",
      " 68% 17/25 [00:00<00:00, 39.57it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8156269788742065, 'eval_f1': 0.7892561531734134, 'eval_runtime': 0.6212, 'eval_samples_per_second': 321.972, 'eval_steps_per_second': 40.247, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:02<00:30, 13.32it/s]\n",
      "100% 25/25 [00:00<00:00, 39.71it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.0007, 'learning_rate': 2.1473684210526317e-06, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:17<00:11, 16.92it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 106.98it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8216089010238647, 'eval_f1': 0.7930343995863355, 'eval_runtime': 0.2679, 'eval_samples_per_second': 746.439, 'eval_steps_per_second': 93.305, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:18<00:11, 16.92it/s]\n",
      "100% 25/25 [00:00<00:00, 96.65it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800 (score: 0.8288302257069396).\n",
      "{'train_runtime': 141.5322, 'train_samples_per_second': 113.048, 'train_steps_per_second': 14.131, 'train_loss': 0.13058339751429027, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:21<00:11, 16.92it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      " 90% 1800/2000 [02:21<00:15, 12.68it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 67.39it/s]\n",
      "f1 0.8288302257069396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7333    0.6875    0.7097        16\n",
      "           1     0.8824    0.8182    0.8491        55\n",
      "           2     0.9104    0.9457    0.9278       129\n",
      "\n",
      "    accuracy                         0.8900       200\n",
      "   macro avg     0.8420    0.8171    0.8288       200\n",
      "weighted avg     0.8886    0.8900    0.8887       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9ee582664d5bea23.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4ba16e0899882147.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.7507, 'learning_rate': 1.8957894736842106e-05, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:47, 16.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.46627724170684814, 'eval_f1': 0.5718034351145038, 'eval_runtime': 0.2494, 'eval_samples_per_second': 802.07, 'eval_steps_per_second': 100.259, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:47, 16.69it/s]\n",
      "100% 25/25 [00:00<00:00, 121.62it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.2959, 'learning_rate': 1.687368421052632e-05, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:27<01:31, 17.50it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.98it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5482611656188965, 'eval_f1': 0.7447786131996658, 'eval_runtime': 0.2547, 'eval_samples_per_second': 785.244, 'eval_steps_per_second': 98.156, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:28<01:31, 17.50it/s]\n",
      "100% 25/25 [00:00<00:00, 104.64it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0738, 'learning_rate': 1.4768421052631581e-05, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:19, 17.55it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6448157429695129, 'eval_f1': 0.7261241174284652, 'eval_runtime': 0.249, 'eval_samples_per_second': 803.194, 'eval_steps_per_second': 100.399, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:19, 17.55it/s]\n",
      "100% 25/25 [00:00<00:00, 122.85it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 0.0145, 'learning_rate': 1.2663157894736843e-05, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:07, 17.71it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7466418743133545, 'eval_f1': 0.7167750435632624, 'eval_runtime': 0.2357, 'eval_samples_per_second': 848.63, 'eval_steps_per_second': 106.079, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:59<01:07, 17.71it/s]\n",
      "100% 25/25 [00:00<00:00, 127.80it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0016, 'learning_rate': 1.0557894736842107e-05, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:14<00:59, 16.91it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.00it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7880897521972656, 'eval_f1': 0.707876212970339, 'eval_runtime': 0.259, 'eval_samples_per_second': 772.239, 'eval_steps_per_second': 96.53, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:14<00:59, 16.91it/s]\n",
      "100% 25/25 [00:00<00:00, 104.50it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0008, 'learning_rate': 8.45263157894737e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:30<00:48, 16.63it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8250024318695068, 'eval_f1': 0.702938652938653, 'eval_runtime': 0.2408, 'eval_samples_per_second': 830.429, 'eval_steps_per_second': 103.804, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:30<00:48, 16.63it/s]\n",
      "100% 25/25 [00:00<00:00, 125.08it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0006, 'learning_rate': 6.347368421052632e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:45<00:40, 14.67it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 105.06it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.842974841594696, 'eval_f1': 0.7141686101554021, 'eval_runtime': 0.2709, 'eval_samples_per_second': 738.156, 'eval_steps_per_second': 92.27, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:45<00:40, 14.67it/s]\n",
      "100% 25/25 [00:00<00:00, 97.59it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400 (score: 0.7447786131996658).\n",
      "{'train_runtime': 109.0734, 'train_samples_per_second': 146.69, 'train_steps_per_second': 18.336, 'train_loss': 0.16256222978234292, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:48<00:40, 14.67it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 70 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 70 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      " 70% 1400/2000 [01:49<00:46, 12.78it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 83.25it/s]\n",
      "f1 0.7447786131996658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.3846    0.5000        13\n",
      "           1     0.8868    0.7705    0.8246        61\n",
      "           2     0.8643    0.9603    0.9098       126\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.8218    0.7051    0.7448       200\n",
      "weighted avg     0.8614    0.8650    0.8571       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e02a86c513512614/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1075e5fb4da15629.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.7034, 'learning_rate': 1.8968421052631582e-05, 'epoch': 2.0}\n",
      " 10% 200/2000 [00:12<01:43, 17.46it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 52% 13/25 [00:00<00:00, 126.15it/s]\u001B[A\n",
      "{'eval_loss': 0.5440316796302795, 'eval_f1': 0.5208065208065209, 'eval_runtime': 0.2408, 'eval_samples_per_second': 830.613, 'eval_steps_per_second': 103.827, 'epoch': 2.0}\n",
      "\n",
      " 10% 200/2000 [00:12<01:43, 17.46it/s]\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.241, 'learning_rate': 1.687368421052632e-05, 'epoch': 4.0}\n",
      " 20% 400/2000 [00:27<01:30, 17.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.40it/s]\u001B[A\n",
      " 96% 24/25 [00:00<00:00, 113.04it/s]\u001B[A\n",
      "{'eval_loss': 0.5669270157814026, 'eval_f1': 0.7412698029431097, 'eval_runtime': 0.2479, 'eval_samples_per_second': 806.803, 'eval_steps_per_second': 100.85, 'epoch': 4.0}\n",
      "\n",
      " 20% 400/2000 [00:27<01:30, 17.73it/s]\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0784, 'learning_rate': 1.4768421052631581e-05, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:17, 18.14it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7007120251655579, 'eval_f1': 0.7533587942440155, 'eval_runtime': 0.2391, 'eval_samples_per_second': 836.452, 'eval_steps_per_second': 104.556, 'epoch': 6.0}\n",
      " 30% 600/2000 [00:43<01:17, 18.14it/s]\n",
      "100% 25/25 [00:00<00:00, 123.23it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0362, 'learning_rate': 1.2663157894736843e-05, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:12, 16.48it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.765394926071167, 'eval_f1': 0.7522239253756764, 'eval_runtime': 0.2457, 'eval_samples_per_second': 814.124, 'eval_steps_per_second': 101.766, 'epoch': 8.0}\n",
      " 40% 800/2000 [00:58<01:12, 16.48it/s]\n",
      "100% 25/25 [00:00<00:00, 125.01it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0051, 'learning_rate': 1.0557894736842107e-05, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:14<00:56, 17.66it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8759030103683472, 'eval_f1': 0.7753163975680094, 'eval_runtime': 0.2368, 'eval_samples_per_second': 844.659, 'eval_steps_per_second': 105.582, 'epoch': 10.0}\n",
      " 50% 1000/2000 [01:14<00:56, 17.66it/s]\n",
      "100% 25/25 [00:00<00:00, 124.62it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-600] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0035, 'learning_rate': 8.45263157894737e-06, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:29<00:49, 16.08it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9124976396560669, 'eval_f1': 0.7287307073115056, 'eval_runtime': 0.2412, 'eval_samples_per_second': 829.096, 'eval_steps_per_second': 103.637, 'epoch': 12.0}\n",
      " 60% 1200/2000 [01:30<00:49, 16.08it/s]\n",
      "100% 25/25 [00:00<00:00, 124.20it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200/special_tokens_map.json\n",
      "{'loss': 0.0007, 'learning_rate': 6.347368421052632e-06, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:45<00:35, 17.11it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9715124368667603, 'eval_f1': 0.7508335417187629, 'eval_runtime': 0.2339, 'eval_samples_per_second': 855.126, 'eval_steps_per_second': 106.891, 'epoch': 14.0}\n",
      " 70% 1400/2000 [01:45<00:35, 17.11it/s]\n",
      "100% 25/25 [00:00<00:00, 127.53it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0006, 'learning_rate': 4.242105263157895e-06, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:00<00:24, 16.61it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9477347731590271, 'eval_f1': 0.7504326723443514, 'eval_runtime': 0.2667, 'eval_samples_per_second': 750.01, 'eval_steps_per_second': 93.751, 'epoch': 16.0}\n",
      " 80% 1600/2000 [02:01<00:24, 16.61it/s]\n",
      "100% 25/25 [00:00<00:00, 127.02it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.0005, 'learning_rate': 2.136842105263158e-06, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:16<00:11, 17.80it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.024510383605957, 'eval_f1': 0.7544793206387851, 'eval_runtime': 0.2388, 'eval_samples_per_second': 837.562, 'eval_steps_per_second': 104.695, 'epoch': 18.0}\n",
      " 90% 1800/2000 [02:16<00:11, 17.80it/s]\n",
      "100% 25/25 [00:00<00:00, 123.17it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.0005, 'learning_rate': 3.157894736842106e-08, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:32<00:00, 17.75it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.60it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0237219333648682, 'eval_f1': 0.7601317122593718, 'eval_runtime': 0.248, 'eval_samples_per_second': 806.531, 'eval_steps_per_second': 100.816, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:32<00:00, 17.75it/s]\n",
      "100% 25/25 [00:00<00:00, 113.08it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-1000 (score: 0.7753163975680094).\n",
      "{'train_runtime': 155.585, 'train_samples_per_second': 102.838, 'train_steps_per_second': 12.855, 'train_loss': 0.10699425459653139, 'epoch': 20.0}\n",
      "100% 2000/2000 [02:35<00:00, 17.75it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-default-lr2e-5-b8/checkpoint-2000] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 69 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 69 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "100% 2000/2000 [02:36<00:00, 12.82it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:00<00:00, 86.38it/s]\n",
      "f1 0.7753163975680094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.6250    0.6667        24\n",
      "           1     0.7556    0.7234    0.7391        47\n",
      "           2     0.9030    0.9380    0.9202       129\n",
      "\n",
      "    accuracy                         0.8500       200\n",
      "   macro avg     0.7909    0.7621    0.7753       200\n",
      "weighted avg     0.8457    0.8500    0.8472       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7858776532035666\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-41\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=default-lr2e-5-b8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJPKFhVYjG5a"
   },
   "source": [
    "## Data Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCfkCNoFBiQS",
    "outputId": "68dec163-7aec-431a-9233-a2bdf70501dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-15 12:42:14.644537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 12:42:15.539262: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 12:42:15.539383: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 12:42:15.539403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 4.22kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 74.4kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 348kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 526kB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 23 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 23 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100% 1/1 [00:00<00:00, 7738.57it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 1375.18it/s]\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100% 1/1 [00:00<00:00, 693.50it/s]\n",
      "Map:   0% 0/4000 [00:00<?, ? examples/s][nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      "Downloading pytorch_model.bin: 100% 268M/268M [00:03<00:00, 67.8MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10000\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/10000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.9234, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.4}\n",
      "  2% 200/10000 [00:15<11:45, 13.89it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 75.99it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 71.53it/s]\u001B[A\n",
      " 96% 24/25 [00:00<00:00, 68.71it/s]\u001B[A\n",
      "\n",
      "Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 4.54MB/s]\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7500885128974915, 'eval_f1': 0.25641025641025644, 'eval_runtime': 3.1527, 'eval_samples_per_second': 63.437, 'eval_steps_per_second': 7.93, 'epoch': 0.4}\n",
      "  2% 200/10000 [00:18<11:45, 13.89it/s]\n",
      "100% 25/25 [00:03<00:00, 68.71it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.6123, 'learning_rate': 1.588e-05, 'epoch': 0.8}\n",
      "  4% 400/10000 [00:33<09:06, 17.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.69it/s]\u001B[A\n",
      " 96% 24/25 [00:00<00:00, 104.11it/s]\u001B[A\n",
      "{'eval_loss': 0.616344153881073, 'eval_f1': 0.4835828912912106, 'eval_runtime': 2.0796, 'eval_samples_per_second': 96.172, 'eval_steps_per_second': 12.022, 'epoch': 0.8}\n",
      "\n",
      "  4% 400/10000 [00:36<09:06, 17.57it/s]\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.431, 'learning_rate': 1.9795789473684213e-05, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:51<08:39, 18.11it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.74it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6051453351974487, 'eval_f1': 0.7230721348368406, 'eval_runtime': 2.1272, 'eval_samples_per_second': 94.022, 'eval_steps_per_second': 11.753, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:53<08:39, 18.11it/s]\n",
      "100% 25/25 [00:02<00:00, 99.79it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.3364, 'learning_rate': 1.9374736842105263e-05, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:09<08:29, 18.05it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 113.75it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0943013429641724, 'eval_f1': 0.6114745730398579, 'eval_runtime': 2.0901, 'eval_samples_per_second': 95.688, 'eval_steps_per_second': 11.961, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:11<08:29, 18.05it/s]\n",
      "100% 25/25 [00:02<00:00, 105.87it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.2844, 'learning_rate': 1.8953684210526316e-05, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:26<09:01, 16.61it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 113.13it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.9828889966011047, 'eval_f1': 0.6284889962426194, 'eval_runtime': 2.1112, 'eval_samples_per_second': 94.735, 'eval_steps_per_second': 11.842, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:28<09:01, 16.61it/s]\n",
      "100% 25/25 [00:02<00:00, 101.92it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.1507, 'learning_rate': 1.853263157894737e-05, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:43<08:20, 17.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 109.98it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.0497746467590332, 'eval_f1': 0.66986762475951, 'eval_runtime': 2.1056, 'eval_samples_per_second': 94.985, 'eval_steps_per_second': 11.873, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:45<08:20, 17.57it/s]\n",
      "100% 25/25 [00:02<00:00, 97.46it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.1208, 'learning_rate': 1.8113684210526317e-05, 'epoch': 2.8}\n",
      " 14% 1400/10000 [02:00<08:05, 17.71it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 109.91it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.2308056354522705, 'eval_f1': 0.6759321965877567, 'eval_runtime': 2.1074, 'eval_samples_per_second': 94.903, 'eval_steps_per_second': 11.863, 'epoch': 2.8}\n",
      " 14% 1400/10000 [02:03<08:05, 17.71it/s]\n",
      "100% 25/25 [00:02<00:00, 109.78it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.1476, 'learning_rate': 1.769263157894737e-05, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:17<07:49, 17.90it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 91.75it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.1992994546890259, 'eval_f1': 0.7092097445038622, 'eval_runtime': 2.1967, 'eval_samples_per_second': 91.045, 'eval_steps_per_second': 11.381, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:20<07:49, 17.90it/s]\n",
      "100% 25/25 [00:02<00:00, 78.46it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600 (score: 0.7230721348368406).\n",
      "{'train_runtime': 143.5307, 'train_samples_per_second': 557.372, 'train_steps_per_second': 69.672, 'train_loss': 0.37582347869873045, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:23<07:49, 17.90it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      " 16% 1600/10000 [02:24<12:36, 11.11it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.80it/s]\n",
      "f1 0.7230721348368406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.6667    0.5714        12\n",
      "           1     0.7963    0.6825    0.7350        63\n",
      "           2     0.8462    0.8800    0.8627       125\n",
      "\n",
      "    accuracy                         0.8050       200\n",
      "   macro avg     0.7142    0.7431    0.7231       200\n",
      "weighted avg     0.8097    0.8050    0.8050       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9393, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.4}\n",
      "  2% 200/10000 [00:12<09:04, 18.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 108.10it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7073742747306824, 'eval_f1': 0.33429171920975204, 'eval_runtime': 2.0999, 'eval_samples_per_second': 95.243, 'eval_steps_per_second': 11.905, 'epoch': 0.4}\n",
      "  2% 200/10000 [00:14<09:04, 18.00it/s]\n",
      "100% 25/25 [00:02<00:00, 106.28it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.5914, 'learning_rate': 1.5960000000000003e-05, 'epoch': 0.8}\n",
      "  4% 400/10000 [00:29<09:35, 16.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.07it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.46478912234306335, 'eval_f1': 0.6030260589084119, 'eval_runtime': 2.0958, 'eval_samples_per_second': 95.43, 'eval_steps_per_second': 11.929, 'epoch': 0.8}\n",
      "  4% 400/10000 [00:31<09:35, 16.69it/s]\n",
      "100% 25/25 [00:02<00:00, 106.63it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.3876, 'learning_rate': 1.979368421052632e-05, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:46<08:42, 18.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 108.94it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4741194546222687, 'eval_f1': 0.7262905691902711, 'eval_runtime': 2.1034, 'eval_samples_per_second': 95.084, 'eval_steps_per_second': 11.886, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:48<08:42, 18.00it/s]\n",
      "100% 25/25 [00:02<00:00, 108.32it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.2652, 'learning_rate': 1.9374736842105263e-05, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:03<10:21, 14.81it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 85.50it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5514532923698425, 'eval_f1': 0.7610364285121568, 'eval_runtime': 2.2302, 'eval_samples_per_second': 89.679, 'eval_steps_per_second': 11.21, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:06<10:21, 14.81it/s]\n",
      "100% 25/25 [00:02<00:00, 76.77it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.2786, 'learning_rate': 1.8953684210526316e-05, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:21<11:03, 13.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 74.36it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 69.74it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.5962615013122559, 'eval_f1': 0.6874003189792663, 'eval_runtime': 2.2543, 'eval_samples_per_second': 88.721, 'eval_steps_per_second': 11.09, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:23<11:03, 13.57it/s]\n",
      "100% 25/25 [00:02<00:00, 69.52it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.09, 'learning_rate': 1.8534736842105264e-05, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:38<10:26, 14.04it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 77.75it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 71.40it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.7399651408195496, 'eval_f1': 0.6704898933342381, 'eval_runtime': 2.2144, 'eval_samples_per_second': 90.318, 'eval_steps_per_second': 11.29, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:40<10:26, 14.04it/s]\n",
      "100% 25/25 [00:02<00:00, 69.09it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.1073, 'learning_rate': 1.8113684210526317e-05, 'epoch': 2.8}\n",
      " 14% 1400/10000 [01:55<07:56, 18.05it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.46it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.7904284596443176, 'eval_f1': 0.7144699399415404, 'eval_runtime': 2.0745, 'eval_samples_per_second': 96.408, 'eval_steps_per_second': 12.051, 'epoch': 2.8}\n",
      " 14% 1400/10000 [01:58<07:56, 18.05it/s]\n",
      "100% 25/25 [00:02<00:00, 108.71it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0837, 'learning_rate': 1.769263157894737e-05, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:13<07:58, 17.55it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.07it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.9813349843025208, 'eval_f1': 0.6596833596403119, 'eval_runtime': 2.0617, 'eval_samples_per_second': 97.008, 'eval_steps_per_second': 12.126, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:15<07:58, 17.55it/s]\n",
      "100% 25/25 [00:02<00:00, 111.45it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.0255, 'learning_rate': 1.727157894736842e-05, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:30<07:49, 17.47it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.79it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.9657669067382812, 'eval_f1': 0.6574240028114567, 'eval_runtime': 2.1079, 'eval_samples_per_second': 94.881, 'eval_steps_per_second': 11.86, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:32<07:49, 17.47it/s]\n",
      "100% 25/25 [00:02<00:00, 108.81it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800 (score: 0.7610364285121568).\n",
      "{'train_runtime': 156.3043, 'train_samples_per_second': 511.822, 'train_steps_per_second': 63.978, 'train_loss': 0.307630828221639, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:35<07:49, 17.47it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      " 18% 1800/10000 [02:36<11:54, 11.47it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.57it/s]\n",
      "f1 0.7610364285121568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.5000    0.5833        14\n",
      "           1     0.8163    0.7407    0.7767        54\n",
      "           2     0.8936    0.9545    0.9231       132\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.8033    0.7318    0.7610       200\n",
      "weighted avg     0.8592    0.8650    0.8598       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9331, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.4}\n",
      "  2% 200/10000 [00:12<09:08, 17.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 52% 13/25 [00:00<00:00, 119.37it/s]\u001B[A\n",
      "100% 25/25 [00:00<00:00, 97.49it/s] \u001B[A\n",
      "{'eval_loss': 0.7745739817619324, 'eval_f1': 0.32725114968105623, 'eval_runtime': 2.128, 'eval_samples_per_second': 93.987, 'eval_steps_per_second': 11.748, 'epoch': 0.4}\n",
      "\n",
      "  2% 200/10000 [00:14<09:08, 17.87it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.6253, 'learning_rate': 1.5960000000000003e-05, 'epoch': 0.8}\n",
      "  4% 400/10000 [00:30<09:14, 17.31it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.66it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5235674977302551, 'eval_f1': 0.5092839389543057, 'eval_runtime': 2.1408, 'eval_samples_per_second': 93.424, 'eval_steps_per_second': 11.678, 'epoch': 0.8}\n",
      "  4% 400/10000 [00:32<09:14, 17.31it/s]\n",
      "100% 25/25 [00:02<00:00, 98.09it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.43, 'learning_rate': 1.979368421052632e-05, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:47<08:50, 17.72it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 107.92it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6233932971954346, 'eval_f1': 0.6601230205470489, 'eval_runtime': 2.138, 'eval_samples_per_second': 93.544, 'eval_steps_per_second': 11.693, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:49<08:50, 17.72it/s]\n",
      "100% 25/25 [00:02<00:00, 94.97it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.3423, 'learning_rate': 1.9372631578947368e-05, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:05<08:26, 18.17it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 118.37it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6750436425209045, 'eval_f1': 0.6748466736981914, 'eval_runtime': 2.1305, 'eval_samples_per_second': 93.875, 'eval_steps_per_second': 11.734, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:07<08:26, 18.17it/s]\n",
      "100% 25/25 [00:02<00:00, 93.14it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.2604, 'learning_rate': 1.8953684210526316e-05, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:22<08:38, 17.36it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 108.00it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.7474894523620605, 'eval_f1': 0.7087981077126603, 'eval_runtime': 2.1018, 'eval_samples_per_second': 95.159, 'eval_steps_per_second': 11.895, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:24<08:38, 17.36it/s]\n",
      "100% 25/25 [00:02<00:00, 94.04it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.1107, 'learning_rate': 1.8534736842105264e-05, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:39<08:22, 17.50it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.57it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.9302862286567688, 'eval_f1': 0.7125884426207949, 'eval_runtime': 2.1255, 'eval_samples_per_second': 94.096, 'eval_steps_per_second': 11.762, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:41<08:22, 17.50it/s]\n",
      "100% 25/25 [00:02<00:00, 97.07it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.1783, 'learning_rate': 1.8113684210526317e-05, 'epoch': 2.8}\n",
      " 14% 1400/10000 [02:04<17:04,  8.39it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 20% 5/25 [00:00<00:00, 44.62it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 50.30it/s]\u001B[A\n",
      " 68% 17/25 [00:00<00:00, 47.55it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.888373613357544, 'eval_f1': 0.6737373737373739, 'eval_runtime': 2.4222, 'eval_samples_per_second': 82.569, 'eval_steps_per_second': 10.321, 'epoch': 2.8}\n",
      " 14% 1400/10000 [02:07<17:04,  8.39it/s]\n",
      "100% 25/25 [00:02<00:00, 51.12it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/special_tokens_map.json\n",
      "{'loss': 0.0813, 'learning_rate': 1.769263157894737e-05, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:27<07:56, 17.64it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 113.76it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.2376701831817627, 'eval_f1': 0.6530353704266748, 'eval_runtime': 2.142, 'eval_samples_per_second': 93.37, 'eval_steps_per_second': 11.671, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:29<07:56, 17.64it/s]\n",
      "100% 25/25 [00:02<00:00, 97.00it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.0525, 'learning_rate': 1.727157894736842e-05, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:45<07:53, 17.31it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 109.17it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.1649408340454102, 'eval_f1': 0.6855563199062397, 'eval_runtime': 2.1361, 'eval_samples_per_second': 93.63, 'eval_steps_per_second': 11.704, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:47<07:53, 17.31it/s]\n",
      "100% 25/25 [00:02<00:00, 93.90it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.036, 'learning_rate': 1.6850526315789473e-05, 'epoch': 4.0}\n",
      " 20% 2000/10000 [03:02<08:19, 16.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.70it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.203830361366272, 'eval_f1': 0.684876047797396, 'eval_runtime': 2.1177, 'eval_samples_per_second': 94.442, 'eval_steps_per_second': 11.805, 'epoch': 4.0}\n",
      " 20% 2000/10000 [03:05<08:19, 16.00it/s]\n",
      "100% 25/25 [00:02<00:00, 96.71it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800] due to args.save_total_limit\n",
      "{'loss': 0.0117, 'learning_rate': 1.642947368421053e-05, 'epoch': 4.4}\n",
      " 22% 2200/10000 [03:19<07:15, 17.90it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 105.54it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.5090420246124268, 'eval_f1': 0.6874312217324922, 'eval_runtime': 2.1368, 'eval_samples_per_second': 93.596, 'eval_steps_per_second': 11.7, 'epoch': 4.4}\n",
      " 22% 2200/10000 [03:21<07:15, 17.90it/s]\n",
      "100% 25/25 [00:02<00:00, 94.85it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200 (score: 0.7125884426207949).\n",
      "{'train_runtime': 205.5122, 'train_samples_per_second': 389.271, 'train_steps_per_second': 48.659, 'train_loss': 0.2783410599014976, 'epoch': 4.4}\n",
      " 22% 2200/10000 [03:25<07:15, 17.90it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      " 22% 2200/10000 [03:26<12:10, 10.68it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.57it/s]\n",
      "f1 0.7125884426207949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6154    0.5000    0.5517        16\n",
      "           1     0.8250    0.6000    0.6947        55\n",
      "           2     0.8367    0.9535    0.8913       129\n",
      "\n",
      "    accuracy                         0.8200       200\n",
      "   macro avg     0.7590    0.6845    0.7126       200\n",
      "weighted avg     0.8158    0.8200    0.8101       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9024, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.4}\n",
      "  2% 200/10000 [00:12<14:26, 11.31it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 4/25 [00:00<00:00, 35.64it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 34.51it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 36.39it/s]\u001B[A\n",
      " 84% 21/25 [00:00<00:00, 56.26it/s]\u001B[A\n",
      "{'eval_loss': 0.7717053294181824, 'eval_f1': 0.25766871165644173, 'eval_runtime': 2.3564, 'eval_samples_per_second': 84.875, 'eval_steps_per_second': 10.609, 'epoch': 0.4}\n",
      "\n",
      "  2% 200/10000 [00:15<14:26, 11.31it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.6344, 'learning_rate': 1.5920000000000003e-05, 'epoch': 0.8}\n",
      "  4% 400/10000 [00:31<11:05, 14.43it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.15it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 75.85it/s]\u001B[A\n",
      " 96% 24/25 [00:00<00:00, 75.32it/s]\u001B[A\n",
      "{'eval_loss': 0.5344470739364624, 'eval_f1': 0.5314256185000968, 'eval_runtime': 2.2045, 'eval_samples_per_second': 90.722, 'eval_steps_per_second': 11.34, 'epoch': 0.8}\n",
      "\n",
      "  4% 400/10000 [00:33<11:05, 14.43it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.4118, 'learning_rate': 1.9795789473684213e-05, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:48<10:59, 14.25it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.06it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 68.71it/s]\u001B[A\n",
      " 92% 23/25 [00:00<00:00, 67.91it/s]\u001B[A\n",
      "{'eval_loss': 0.6609434485435486, 'eval_f1': 0.6461904761904762, 'eval_runtime': 2.2455, 'eval_samples_per_second': 89.066, 'eval_steps_per_second': 11.133, 'epoch': 1.2}\n",
      "\n",
      "  6% 600/10000 [00:50<10:59, 14.25it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.2875, 'learning_rate': 1.9374736842105263e-05, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:05<10:52, 14.10it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 76.54it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 65.76it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.569118857383728, 'eval_f1': 0.6833147779210126, 'eval_runtime': 2.2721, 'eval_samples_per_second': 88.024, 'eval_steps_per_second': 11.003, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:08<10:52, 14.10it/s]\n",
      "100% 25/25 [00:02<00:00, 65.43it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.2401, 'learning_rate': 1.8953684210526316e-05, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:23<08:40, 17.28it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.25it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.8030915856361389, 'eval_f1': 0.6625590947841588, 'eval_runtime': 2.0891, 'eval_samples_per_second': 95.736, 'eval_steps_per_second': 11.967, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:25<08:40, 17.28it/s]\n",
      "100% 25/25 [00:02<00:00, 102.17it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.0996, 'learning_rate': 1.8534736842105264e-05, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:41<08:28, 17.32it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 105.96it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.9234074354171753, 'eval_f1': 0.7106687861404843, 'eval_runtime': 2.0941, 'eval_samples_per_second': 95.504, 'eval_steps_per_second': 11.938, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:43<08:28, 17.32it/s]\n",
      "100% 25/25 [00:02<00:00, 98.98it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.1436, 'learning_rate': 1.8113684210526317e-05, 'epoch': 2.8}\n",
      " 14% 1400/10000 [01:58<08:53, 16.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 110.60it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.9435880780220032, 'eval_f1': 0.6529914820184199, 'eval_runtime': 2.1189, 'eval_samples_per_second': 94.388, 'eval_steps_per_second': 11.798, 'epoch': 2.8}\n",
      " 14% 1400/10000 [02:00<08:53, 16.12it/s]\n",
      "100% 25/25 [00:02<00:00, 94.72it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/special_tokens_map.json\n",
      "{'loss': 0.0796, 'learning_rate': 1.769263157894737e-05, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:16<08:17, 16.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 110.25it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.131137490272522, 'eval_f1': 0.6891152113019121, 'eval_runtime': 2.091, 'eval_samples_per_second': 95.65, 'eval_steps_per_second': 11.956, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:18<08:17, 16.87it/s]\n",
      "100% 25/25 [00:02<00:00, 97.01it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.051, 'learning_rate': 1.727157894736842e-05, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:33<08:00, 17.07it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.54it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.2581549882888794, 'eval_f1': 0.6738540520718739, 'eval_runtime': 2.1002, 'eval_samples_per_second': 95.227, 'eval_steps_per_second': 11.903, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:35<08:00, 17.07it/s]\n",
      "100% 25/25 [00:02<00:00, 98.93it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.0621, 'learning_rate': 1.6850526315789473e-05, 'epoch': 4.0}\n",
      " 20% 2000/10000 [02:51<07:47, 17.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.63it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.398945927619934, 'eval_f1': 0.6614743846611538, 'eval_runtime': 2.1156, 'eval_samples_per_second': 94.535, 'eval_steps_per_second': 11.817, 'epoch': 4.0}\n",
      " 20% 2000/10000 [02:53<07:47, 17.12it/s]\n",
      "100% 25/25 [00:02<00:00, 98.75it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800] due to args.save_total_limit\n",
      "{'loss': 0.033, 'learning_rate': 1.642947368421053e-05, 'epoch': 4.4}\n",
      " 22% 2200/10000 [03:08<07:24, 17.56it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 108.35it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.323411464691162, 'eval_f1': 0.6582589697017558, 'eval_runtime': 2.1172, 'eval_samples_per_second': 94.464, 'eval_steps_per_second': 11.808, 'epoch': 4.4}\n",
      " 22% 2200/10000 [03:10<07:24, 17.56it/s]\n",
      "100% 25/25 [00:02<00:00, 98.17it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200 (score: 0.7106687861404843).\n",
      "{'train_runtime': 195.3859, 'train_samples_per_second': 409.446, 'train_steps_per_second': 51.181, 'train_loss': 0.2677326302094893, 'epoch': 4.4}\n",
      " 22% 2200/10000 [03:15<07:24, 17.56it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 38 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 38 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      " 22% 2200/10000 [03:15<11:34, 11.23it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.59it/s]\n",
      "f1 0.7106687861404843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.3846    0.4762        13\n",
      "           1     0.8889    0.6557    0.7547        61\n",
      "           2     0.8367    0.9762    0.9011       126\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7835    0.6722    0.7107       200\n",
      "weighted avg     0.8389    0.8400    0.8288       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10000\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.907, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.4}\n",
      "  2% 200/10000 [00:12<10:12, 16.01it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.57it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7965973019599915, 'eval_f1': 0.26016260162601623, 'eval_runtime': 2.1794, 'eval_samples_per_second': 91.767, 'eval_steps_per_second': 11.471, 'epoch': 0.4}\n",
      "  2% 200/10000 [00:14<10:12, 16.01it/s]\n",
      "100% 25/25 [00:02<00:00, 83.92it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.6117, 'learning_rate': 1.5920000000000003e-05, 'epoch': 0.8}\n",
      "  4% 400/10000 [00:29<10:48, 14.80it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.82it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 75.25it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7313401103019714, 'eval_f1': 0.3930553069625917, 'eval_runtime': 2.2221, 'eval_samples_per_second': 90.003, 'eval_steps_per_second': 11.25, 'epoch': 0.8}\n",
      "  4% 400/10000 [00:31<10:48, 14.80it/s]\n",
      "100% 25/25 [00:02<00:00, 72.18it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.3985, 'learning_rate': 1.9795789473684213e-05, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:47<11:19, 13.83it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 77.22it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 69.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6814702749252319, 'eval_f1': 0.7082562883390566, 'eval_runtime': 2.2398, 'eval_samples_per_second': 89.294, 'eval_steps_per_second': 11.162, 'epoch': 1.2}\n",
      "  6% 600/10000 [00:49<11:19, 13.83it/s]\n",
      "100% 25/25 [00:02<00:00, 67.07it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.3338, 'learning_rate': 1.9374736842105263e-05, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:05<11:10, 13.72it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 89.07it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.755340576171875, 'eval_f1': 0.6952380952380951, 'eval_runtime': 2.1301, 'eval_samples_per_second': 93.894, 'eval_steps_per_second': 11.737, 'epoch': 1.6}\n",
      "  8% 800/10000 [01:07<11:10, 13.72it/s]\n",
      "100% 25/25 [00:02<00:00, 99.44it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.2531, 'learning_rate': 1.8953684210526316e-05, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:23<08:48, 17.03it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 110.37it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 0.8448979258537292, 'eval_f1': 0.6569905779494841, 'eval_runtime': 2.1076, 'eval_samples_per_second': 94.895, 'eval_steps_per_second': 11.862, 'epoch': 2.0}\n",
      " 10% 1000/10000 [01:25<08:48, 17.03it/s]\n",
      "100% 25/25 [00:02<00:00, 101.44it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.1253, 'learning_rate': 1.8534736842105264e-05, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:41<08:55, 16.44it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 102.86it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.072624921798706, 'eval_f1': 0.6763038548752834, 'eval_runtime': 2.1105, 'eval_samples_per_second': 94.762, 'eval_steps_per_second': 11.845, 'epoch': 2.4}\n",
      " 12% 1200/10000 [01:43<08:55, 16.44it/s]\n",
      "100% 25/25 [00:02<00:00, 100.83it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.1122, 'learning_rate': 1.8113684210526317e-05, 'epoch': 2.8}\n",
      " 14% 1400/10000 [01:59<08:12, 17.48it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 84.95it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.0918376445770264, 'eval_f1': 0.67232960837612, 'eval_runtime': 2.1411, 'eval_samples_per_second': 93.409, 'eval_steps_per_second': 11.676, 'epoch': 2.8}\n",
      " 14% 1400/10000 [02:01<08:12, 17.48it/s]\n",
      "100% 25/25 [00:02<00:00, 87.57it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0776, 'learning_rate': 1.769263157894737e-05, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:17<08:01, 17.45it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 103.24it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.099181056022644, 'eval_f1': 0.7096558019550617, 'eval_runtime': 2.111, 'eval_samples_per_second': 94.742, 'eval_steps_per_second': 11.843, 'epoch': 3.2}\n",
      " 16% 1600/10000 [02:19<08:01, 17.45it/s]\n",
      "100% 25/25 [00:02<00:00, 100.30it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-600] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.0555, 'learning_rate': 1.727157894736842e-05, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:35<08:05, 16.88it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 106.97it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.2992963790893555, 'eval_f1': 0.6896214896214895, 'eval_runtime': 2.122, 'eval_samples_per_second': 94.252, 'eval_steps_per_second': 11.782, 'epoch': 3.6}\n",
      " 18% 1800/10000 [02:37<08:05, 16.88it/s]\n",
      "100% 25/25 [00:02<00:00, 102.43it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800/special_tokens_map.json\n",
      "{'loss': 0.0382, 'learning_rate': 1.6850526315789473e-05, 'epoch': 4.0}\n",
      " 20% 2000/10000 [02:52<07:53, 16.91it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.47it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.4051769971847534, 'eval_f1': 0.6682573307214555, 'eval_runtime': 2.1519, 'eval_samples_per_second': 92.941, 'eval_steps_per_second': 11.618, 'epoch': 4.0}\n",
      " 20% 2000/10000 [02:54<07:53, 16.91it/s]\n",
      "100% 25/25 [00:02<00:00, 102.58it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1800] due to args.save_total_limit\n",
      "{'loss': 0.0104, 'learning_rate': 1.642947368421053e-05, 'epoch': 4.4}\n",
      " 22% 2200/10000 [03:09<09:16, 14.02it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.72it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 65.88it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.3898180723190308, 'eval_f1': 0.6651991429661589, 'eval_runtime': 2.2574, 'eval_samples_per_second': 88.596, 'eval_steps_per_second': 11.075, 'epoch': 4.4}\n",
      " 22% 2200/10000 [03:12<09:16, 14.02it/s]\n",
      "100% 25/25 [00:02<00:00, 67.87it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2000] due to args.save_total_limit\n",
      "{'loss': 0.0174, 'learning_rate': 1.600842105263158e-05, 'epoch': 4.8}\n",
      " 24% 2400/10000 [03:27<09:02, 14.00it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 73.37it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 65.01it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.5540870428085327, 'eval_f1': 0.6889691103353432, 'eval_runtime': 2.2468, 'eval_samples_per_second': 89.016, 'eval_steps_per_second': 11.127, 'epoch': 4.8}\n",
      " 24% 2400/10000 [03:29<09:02, 14.00it/s]\n",
      "100% 25/25 [00:02<00:00, 65.83it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2200] due to args.save_total_limit\n",
      "{'loss': 0.0152, 'learning_rate': 1.5587368421052633e-05, 'epoch': 5.2}\n",
      " 26% 2600/10000 [03:45<09:04, 13.58it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 72.35it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 67.16it/s]\u001B[A\n",
      "                                        \n",
      "\u001B[A{'eval_loss': 1.5061721801757812, 'eval_f1': 0.6829089565932298, 'eval_runtime': 2.2391, 'eval_samples_per_second': 89.321, 'eval_steps_per_second': 11.165, 'epoch': 5.2}\n",
      " 26% 2600/10000 [03:47<09:04, 13.58it/s]\n",
      "100% 25/25 [00:02<00:00, 66.49it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-1600 (score: 0.7096558019550617).\n",
      "{'train_runtime': 231.0025, 'train_samples_per_second': 346.317, 'train_steps_per_second': 43.29, 'train_loss': 0.22735863300470205, 'epoch': 5.2}\n",
      " 26% 2600/10000 [03:50<09:04, 13.58it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation/checkpoint-2600] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 42 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 42 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      " 26% 2600/10000 [03:51<10:59, 11.22it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.74it/s]\n",
      "f1 0.7096558019550617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6842    0.5417    0.6047        24\n",
      "           1     0.7222    0.5532    0.6265        47\n",
      "           2     0.8483    0.9535    0.8978       129\n",
      "\n",
      "    accuracy                         0.8100       200\n",
      "   macro avg     0.7516    0.6828    0.7097       200\n",
      "weighted avg     0.7990    0.8100    0.7989       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7277907760487715\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-64\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUkAYKOIMQXa",
    "outputId": "77db6819-fa5b-491e-f7ee-9247beb45963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-15 13:00:40.223961: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 13:00:42.272743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 13:00:42.272926: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 13:00:42.272947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 77.73it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-135f35376c3d460f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c1dca4a919ebe264.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1419\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1780\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/1780 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.7879, 'learning_rate': 1.8722649319929037e-05, 'epoch': 1.12}\n",
      " 11% 200/1780 [00:12<01:48, 14.51it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.90it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 79.01it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6183158755302429, 'eval_f1': 0.6659213250517598, 'eval_runtime': 2.2236, 'eval_samples_per_second': 89.944, 'eval_steps_per_second': 11.243, 'epoch': 1.12}\n",
      " 11% 200/1780 [00:15<01:48, 14.51it/s]\n",
      "100% 25/25 [00:02<00:00, 75.39it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.2748, 'learning_rate': 1.6357185097575403e-05, 'epoch': 2.25}\n",
      " 22% 400/1780 [00:30<01:38, 14.06it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 76.65it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 68.71it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.8159704804420471, 'eval_f1': 0.7121623468486936, 'eval_runtime': 2.2417, 'eval_samples_per_second': 89.218, 'eval_steps_per_second': 11.152, 'epoch': 2.25}\n",
      " 22% 400/1780 [00:32<01:38, 14.06it/s]\n",
      "100% 25/25 [00:02<00:00, 66.12it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1412, 'learning_rate': 1.4003548196333532e-05, 'epoch': 3.37}\n",
      " 34% 600/1780 [00:47<01:21, 14.43it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 73.45it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 60.69it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.9536481499671936, 'eval_f1': 0.7086402313927286, 'eval_runtime': 2.2454, 'eval_samples_per_second': 89.07, 'eval_steps_per_second': 11.134, 'epoch': 3.37}\n",
      " 34% 600/1780 [00:49<01:21, 14.43it/s]\n",
      "100% 25/25 [00:02<00:00, 61.49it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 0.052, 'learning_rate': 1.1649911295091663e-05, 'epoch': 4.49}\n",
      " 45% 800/1780 [01:07<00:55, 17.52it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.52it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.1007715463638306, 'eval_f1': 0.7263592289847135, 'eval_runtime': 2.0942, 'eval_samples_per_second': 95.502, 'eval_steps_per_second': 11.938, 'epoch': 4.49}\n",
      " 45% 800/1780 [01:09<00:55, 17.52it/s]\n",
      "100% 25/25 [00:02<00:00, 102.11it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0172, 'learning_rate': 9.284447072738025e-06, 'epoch': 5.62}\n",
      " 56% 1000/1780 [01:25<00:50, 15.35it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 107.06it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2147505283355713, 'eval_f1': 0.7035954066053925, 'eval_runtime': 2.1084, 'eval_samples_per_second': 94.856, 'eval_steps_per_second': 11.857, 'epoch': 5.62}\n",
      " 56% 1000/1780 [01:27<00:50, 15.35it/s]\n",
      "100% 25/25 [00:02<00:00, 102.58it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.0106, 'learning_rate': 6.918982850384388e-06, 'epoch': 6.74}\n",
      " 67% 1200/1780 [01:43<00:33, 17.20it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.85it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.242990255355835, 'eval_f1': 0.7026756238196145, 'eval_runtime': 2.1004, 'eval_samples_per_second': 95.219, 'eval_steps_per_second': 11.902, 'epoch': 6.74}\n",
      " 67% 1200/1780 [01:45<00:33, 17.20it/s]\n",
      "100% 25/25 [00:02<00:00, 105.86it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0132, 'learning_rate': 4.5535186280307515e-06, 'epoch': 7.87}\n",
      " 79% 1400/1780 [02:00<00:21, 17.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 99.28it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2359139919281006, 'eval_f1': 0.7194060290648331, 'eval_runtime': 2.1306, 'eval_samples_per_second': 93.869, 'eval_steps_per_second': 11.734, 'epoch': 7.87}\n",
      " 79% 1400/1780 [02:02<00:21, 17.87it/s]\n",
      "100% 25/25 [00:02<00:00, 92.09it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0066, 'learning_rate': 2.1880544056771143e-06, 'epoch': 8.99}\n",
      " 90% 1600/1780 [02:18<00:10, 17.58it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.33it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.3380727767944336, 'eval_f1': 0.697587419531971, 'eval_runtime': 2.1364, 'eval_samples_per_second': 93.616, 'eval_steps_per_second': 11.702, 'epoch': 8.99}\n",
      " 90% 1600/1780 [02:20<00:10, 17.58it/s]\n",
      "100% 25/25 [00:02<00:00, 99.82it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400] due to args.save_total_limit\n",
      "100% 1780/1780 [02:34<00:00, 16.91it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800 (score: 0.7263592289847135).\n",
      "{'train_runtime': 155.2518, 'train_samples_per_second': 91.4, 'train_steps_per_second': 11.465, 'train_loss': 0.14681368155425853, 'epoch': 10.0}\n",
      "100% 1780/1780 [02:34<00:00, 16.91it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 51 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 51 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "100% 1780/1780 [02:35<00:00, 11.43it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.30it/s]\n",
      "f1 0.7263592289847135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6364    0.5833    0.6087        12\n",
      "           1     0.8261    0.6032    0.6972        63\n",
      "           2     0.8182    0.9360    0.8731       125\n",
      "\n",
      "    accuracy                         0.8100       200\n",
      "   macro avg     0.7602    0.7075    0.7264       200\n",
      "weighted avg     0.8098    0.8100    0.8019       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d7e937b243529bcb.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1416\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1770\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.7868, 'learning_rate': 1.870315288518739e-05, 'epoch': 1.13}\n",
      " 11% 200/1770 [00:12<01:34, 16.56it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 108.65it/s]\u001B[A\n",
      " 88% 22/25 [00:00<00:00, 100.56it/s]\u001B[A\n",
      "{'eval_loss': 0.6861096024513245, 'eval_f1': 0.5634438721725249, 'eval_runtime': 2.1385, 'eval_samples_per_second': 93.521, 'eval_steps_per_second': 11.69, 'epoch': 1.13}\n",
      "\n",
      " 11% 200/1770 [00:15<01:34, 16.56it/s]\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.2596, 'learning_rate': 1.6335514574657944e-05, 'epoch': 2.26}\n",
      " 23% 400/1770 [00:30<01:23, 16.39it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.83it/s]\u001B[A\n",
      " 96% 24/25 [00:00<00:00, 105.50it/s]\u001B[A\n",
      "{'eval_loss': 0.8920378088951111, 'eval_f1': 0.45320396199717, 'eval_runtime': 2.0877, 'eval_samples_per_second': 95.8, 'eval_steps_per_second': 11.975, 'epoch': 2.26}\n",
      "\n",
      " 23% 400/1770 [00:32<01:23, 16.39it/s]\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0816, 'learning_rate': 1.3955978584176087e-05, 'epoch': 3.39}\n",
      " 34% 600/1770 [00:48<01:06, 17.49it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.20it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.3879224061965942, 'eval_f1': 0.45463549366380124, 'eval_runtime': 2.1012, 'eval_samples_per_second': 95.183, 'eval_steps_per_second': 11.898, 'epoch': 3.39}\n",
      " 34% 600/1770 [00:50<01:06, 17.49it/s]\n",
      "100% 25/25 [00:02<00:00, 100.40it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0314, 'learning_rate': 1.157644259369423e-05, 'epoch': 4.52}\n",
      " 45% 800/1770 [01:06<01:01, 15.69it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 107.24it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.4598170518875122, 'eval_f1': 0.4785043842098243, 'eval_runtime': 2.1213, 'eval_samples_per_second': 94.281, 'eval_steps_per_second': 11.785, 'epoch': 4.52}\n",
      " 45% 800/1770 [01:08<01:01, 15.69it/s]\n",
      "100% 25/25 [00:02<00:00, 99.23it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0076, 'learning_rate': 9.196906603212375e-06, 'epoch': 5.65}\n",
      " 56% 1000/1770 [01:23<00:51, 15.05it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 87.56it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.7266626358032227, 'eval_f1': 0.4886281465150734, 'eval_runtime': 2.2136, 'eval_samples_per_second': 90.35, 'eval_steps_per_second': 11.294, 'epoch': 5.65}\n",
      " 56% 1000/1770 [01:25<00:51, 15.05it/s]\n",
      "100% 25/25 [00:02<00:00, 84.38it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0015, 'learning_rate': 6.817370612730518e-06, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:41<00:40, 14.03it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.66it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 69.51it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.7442865371704102, 'eval_f1': 0.47192857619670514, 'eval_runtime': 2.2479, 'eval_samples_per_second': 88.973, 'eval_steps_per_second': 11.122, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:43<00:40, 14.03it/s]\n",
      "100% 25/25 [00:02<00:00, 66.76it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200 (score: 0.5634438721725249).\n",
      "{'train_runtime': 106.7743, 'train_samples_per_second': 132.616, 'train_steps_per_second': 16.577, 'train_loss': 0.194758177647988, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:46<00:40, 14.03it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 41 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 41 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      " 68% 1200/1770 [01:47<00:50, 11.18it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.92it/s]\n",
      "f1 0.5634438721725249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4286    0.2143    0.2857        14\n",
      "           1     0.5286    0.6852    0.5968        54\n",
      "           2     0.8374    0.7803    0.8078       132\n",
      "\n",
      "    accuracy                         0.7150       200\n",
      "   macro avg     0.5982    0.5599    0.5634       200\n",
      "weighted avg     0.7254    0.7150    0.7143       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47109cdfc04ca0c7.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1466\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1840\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8202, 'learning_rate': 1.8787185354691077e-05, 'epoch': 1.09}\n",
      " 11% 200/1840 [00:12<01:57, 13.98it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 71.75it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 68.48it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6177281141281128, 'eval_f1': 0.6117777021147808, 'eval_runtime': 2.2305, 'eval_samples_per_second': 89.664, 'eval_steps_per_second': 11.208, 'epoch': 1.09}\n",
      " 11% 200/1840 [00:14<01:57, 13.98it/s]\n",
      "100% 25/25 [00:02<00:00, 66.78it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.2488, 'learning_rate': 1.651029748283753e-05, 'epoch': 2.17}\n",
      " 22% 400/1840 [00:30<01:30, 15.94it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 104.17it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.9549069404602051, 'eval_f1': 0.523086408667804, 'eval_runtime': 2.1716, 'eval_samples_per_second': 92.097, 'eval_steps_per_second': 11.512, 'epoch': 2.17}\n",
      " 22% 400/1840 [00:32<01:30, 15.94it/s]\n",
      "100% 25/25 [00:02<00:00, 90.37it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0906, 'learning_rate': 1.422196796338673e-05, 'epoch': 3.26}\n",
      " 33% 600/1840 [00:48<01:11, 17.37it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.38it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.1456111669540405, 'eval_f1': 0.5637259958395654, 'eval_runtime': 2.1309, 'eval_samples_per_second': 93.857, 'eval_steps_per_second': 11.732, 'epoch': 3.26}\n",
      " 33% 600/1840 [00:50<01:11, 17.37it/s]\n",
      "100% 25/25 [00:02<00:00, 92.94it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0234, 'learning_rate': 1.1933638443935928e-05, 'epoch': 4.35}\n",
      " 43% 800/1840 [01:06<00:59, 17.48it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.17it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.3268541097640991, 'eval_f1': 0.5606756480914472, 'eval_runtime': 2.1134, 'eval_samples_per_second': 94.636, 'eval_steps_per_second': 11.83, 'epoch': 4.35}\n",
      " 43% 800/1840 [01:08<00:59, 17.48it/s]\n",
      "100% 25/25 [00:02<00:00, 97.55it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0196, 'learning_rate': 9.645308924485126e-06, 'epoch': 5.43}\n",
      " 54% 1000/1840 [01:25<00:51, 16.24it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 103.71it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.5065412521362305, 'eval_f1': 0.6064341266657752, 'eval_runtime': 2.1253, 'eval_samples_per_second': 94.104, 'eval_steps_per_second': 11.763, 'epoch': 5.43}\n",
      " 54% 1000/1840 [01:27<00:51, 16.24it/s]\n",
      "100% 25/25 [00:02<00:00, 92.76it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0044, 'learning_rate': 7.356979405034326e-06, 'epoch': 6.52}\n",
      " 65% 1200/1840 [01:43<00:38, 16.54it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 118.77it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.518320918083191, 'eval_f1': 0.564590135388098, 'eval_runtime': 2.1168, 'eval_samples_per_second': 94.481, 'eval_steps_per_second': 11.81, 'epoch': 6.52}\n",
      " 65% 1200/1840 [01:45<00:38, 16.54it/s]\n",
      "100% 25/25 [00:02<00:00, 99.81it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200 (score: 0.6117777021147808).\n",
      "{'train_runtime': 108.8578, 'train_samples_per_second': 134.671, 'train_steps_per_second': 16.903, 'train_loss': 0.2011357063551744, 'epoch': 6.52}\n",
      " 65% 1200/1840 [01:48<00:38, 16.54it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      " 65% 1200/1840 [01:49<00:58, 10.96it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.72it/s]\n",
      "f1 0.6117777021147808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4000    0.5000    0.4444        16\n",
      "           1     0.7353    0.4545    0.5618        55\n",
      "           2     0.7808    0.8837    0.8291       129\n",
      "\n",
      "    accuracy                         0.7350       200\n",
      "   macro avg     0.6387    0.6128    0.6118       200\n",
      "weighted avg     0.7378    0.7350    0.7248       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6fd7d7933a2794be.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1415\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1770\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8091, 'learning_rate': 1.870315288518739e-05, 'epoch': 1.13}\n",
      " 11% 200/1770 [00:12<01:33, 16.75it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 86.36it/s]\u001B[A\n",
      " 72% 18/25 [00:00<00:00, 76.91it/s]\u001B[A\n",
      "{'eval_loss': 0.5977456569671631, 'eval_f1': 0.5968383054809717, 'eval_runtime': 2.2297, 'eval_samples_per_second': 89.699, 'eval_steps_per_second': 11.212, 'epoch': 1.13}\n",
      "\n",
      " 11% 200/1770 [00:14<01:33, 16.75it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.261, 'learning_rate': 1.6335514574657944e-05, 'epoch': 2.26}\n",
      " 23% 400/1770 [00:30<01:40, 13.59it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 28% 7/25 [00:00<00:00, 69.20it/s]\u001B[A\n",
      " 56% 14/25 [00:00<00:00, 64.90it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.8981781601905823, 'eval_f1': 0.543736025878883, 'eval_runtime': 2.2505, 'eval_samples_per_second': 88.87, 'eval_steps_per_second': 11.109, 'epoch': 2.26}\n",
      " 23% 400/1770 [00:32<01:40, 13.59it/s]\n",
      "100% 25/25 [00:02<00:00, 66.33it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.0945, 'learning_rate': 1.3967876264128497e-05, 'epoch': 3.39}\n",
      " 34% 600/1770 [00:47<01:22, 14.26it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 76.02it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 67.92it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.1538240909576416, 'eval_f1': 0.6503053227521076, 'eval_runtime': 2.2771, 'eval_samples_per_second': 87.831, 'eval_steps_per_second': 10.979, 'epoch': 3.39}\n",
      " 34% 600/1770 [00:50<01:22, 14.26it/s]\n",
      "100% 25/25 [00:02<00:00, 63.80it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0312, 'learning_rate': 1.1588340273646638e-05, 'epoch': 4.52}\n",
      " 45% 800/1770 [01:06<01:07, 14.32it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.46it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.0660126209259033, 'eval_f1': 0.6518724985706118, 'eval_runtime': 2.1157, 'eval_samples_per_second': 94.531, 'eval_steps_per_second': 11.816, 'epoch': 4.52}\n",
      " 45% 800/1770 [01:08<01:07, 14.32it/s]\n",
      "100% 25/25 [00:02<00:00, 103.40it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0047, 'learning_rate': 9.208804283164785e-06, 'epoch': 5.65}\n",
      " 56% 1000/1770 [01:23<00:44, 17.45it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.10it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.5087512731552124, 'eval_f1': 0.5568110135425822, 'eval_runtime': 2.0994, 'eval_samples_per_second': 95.266, 'eval_steps_per_second': 11.908, 'epoch': 5.65}\n",
      " 56% 1000/1770 [01:26<00:44, 17.45it/s]\n",
      "100% 25/25 [00:02<00:00, 102.19it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.0036, 'learning_rate': 6.829268292682928e-06, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:42<00:37, 15.40it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.33it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.3224825859069824, 'eval_f1': 0.5949830313120317, 'eval_runtime': 2.1371, 'eval_samples_per_second': 93.585, 'eval_steps_per_second': 11.698, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:44<00:37, 15.40it/s]\n",
      "100% 25/25 [00:02<00:00, 102.43it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0007, 'learning_rate': 4.449732302201072e-06, 'epoch': 7.91}\n",
      " 79% 1400/1770 [02:01<00:23, 15.53it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 109.40it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.311029076576233, 'eval_f1': 0.6089931573802542, 'eval_runtime': 2.1168, 'eval_samples_per_second': 94.483, 'eval_steps_per_second': 11.81, 'epoch': 7.91}\n",
      " 79% 1400/1770 [02:04<00:23, 15.53it/s]\n",
      "100% 25/25 [00:02<00:00, 99.46it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0012, 'learning_rate': 2.070196311719215e-06, 'epoch': 9.04}\n",
      " 90% 1600/1770 [02:19<00:09, 17.62it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 73.91it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 71.71it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.351765751838684, 'eval_f1': 0.6055555555555555, 'eval_runtime': 2.2433, 'eval_samples_per_second': 89.153, 'eval_steps_per_second': 11.144, 'epoch': 9.04}\n",
      " 90% 1600/1770 [02:21<00:09, 17.62it/s]\n",
      "100% 25/25 [00:02<00:00, 73.20it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400] due to args.save_total_limit\n",
      "100% 1770/1770 [02:34<00:00, 15.60it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800 (score: 0.6518724985706118).\n",
      "{'train_runtime': 155.3698, 'train_samples_per_second': 91.073, 'train_steps_per_second': 11.392, 'train_loss': 0.13630267699475343, 'epoch': 10.0}\n",
      "100% 1770/1770 [02:35<00:00, 15.60it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 52 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 52 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "100% 1770/1770 [02:35<00:00, 11.35it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.42it/s]\n",
      "f1 0.6518724985706118\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4444    0.3077    0.3636        13\n",
      "           1     0.8444    0.6230    0.7170        61\n",
      "           2     0.8151    0.9444    0.8750       126\n",
      "\n",
      "    accuracy                         0.8050       200\n",
      "   macro avg     0.7013    0.6250    0.6519       200\n",
      "weighted avg     0.7999    0.8050    0.7936       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b6a2e16bf7233b30.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1473\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1850\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8084, 'learning_rate': 1.881616391576551e-05, 'epoch': 1.08}\n",
      " 11% 200/1850 [00:12<01:41, 16.24it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 93.91it/s]\u001B[A\n",
      " 80% 20/25 [00:00<00:00, 85.17it/s]\u001B[A\n",
      "{'eval_loss': 0.7420451641082764, 'eval_f1': 0.4994736842105263, 'eval_runtime': 2.1565, 'eval_samples_per_second': 92.745, 'eval_steps_per_second': 11.593, 'epoch': 1.08}\n",
      "\n",
      " 11% 200/1850 [00:15<01:41, 16.24it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.2864, 'learning_rate': 1.6550939100739896e-05, 'epoch': 2.16}\n",
      " 22% 400/1850 [00:30<01:26, 16.83it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 107.82it/s]\u001B[A\n",
      " 88% 22/25 [00:00<00:00, 105.19it/s]\u001B[A\n",
      "{'eval_loss': 1.0339455604553223, 'eval_f1': 0.5424969552486331, 'eval_runtime': 2.1238, 'eval_samples_per_second': 94.171, 'eval_steps_per_second': 11.771, 'epoch': 2.16}\n",
      "\n",
      " 22% 400/1850 [00:32<01:26, 16.83it/s]\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.0689, 'learning_rate': 1.42743312464428e-05, 'epoch': 3.24}\n",
      " 32% 600/1850 [00:48<01:27, 14.32it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 85.21it/s]\u001B[A\n",
      " 72% 18/25 [00:00<00:00, 78.28it/s]\u001B[A\n",
      "{'eval_loss': 1.298902988433838, 'eval_f1': 0.5215686274509804, 'eval_runtime': 2.2149, 'eval_samples_per_second': 90.297, 'eval_steps_per_second': 11.287, 'epoch': 3.24}\n",
      "\n",
      " 32% 600/1850 [00:50<01:27, 14.32it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 0.0224, 'learning_rate': 1.1997723392145704e-05, 'epoch': 4.32}\n",
      " 43% 800/1850 [01:06<01:16, 13.70it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 71.25it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 69.34it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.2037557363510132, 'eval_f1': 0.6067640692640692, 'eval_runtime': 2.246, 'eval_samples_per_second': 89.048, 'eval_steps_per_second': 11.131, 'epoch': 4.32}\n",
      " 43% 800/1850 [01:08<01:16, 13.70it/s]\n",
      "100% 25/25 [00:02<00:00, 66.54it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-400] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0047, 'learning_rate': 9.721115537848607e-06, 'epoch': 5.41}\n",
      " 54% 1000/1850 [01:25<00:52, 16.31it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 110.77it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.452090859413147, 'eval_f1': 0.5609601559644206, 'eval_runtime': 2.1267, 'eval_samples_per_second': 94.042, 'eval_steps_per_second': 11.755, 'epoch': 5.41}\n",
      " 54% 1000/1850 [01:27<00:52, 16.31it/s]\n",
      "100% 25/25 [00:02<00:00, 102.97it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.0026, 'learning_rate': 7.444507683551509e-06, 'epoch': 6.49}\n",
      " 65% 1200/1850 [01:42<00:46, 13.84it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 108.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.4697375297546387, 'eval_f1': 0.6164324391034047, 'eval_runtime': 2.0839, 'eval_samples_per_second': 95.975, 'eval_steps_per_second': 11.997, 'epoch': 6.49}\n",
      " 65% 1200/1850 [01:44<00:46, 13.84it/s]\n",
      "100% 25/25 [00:02<00:00, 105.92it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-800] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0006, 'learning_rate': 5.167899829254412e-06, 'epoch': 7.57}\n",
      " 76% 1400/1850 [02:00<00:27, 16.54it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 106.97it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.3610092401504517, 'eval_f1': 0.6601187048245872, 'eval_runtime': 2.0903, 'eval_samples_per_second': 95.681, 'eval_steps_per_second': 11.96, 'epoch': 7.57}\n",
      " 76% 1400/1850 [02:02<00:27, 16.54it/s]\n",
      "100% 25/25 [00:02<00:00, 101.81it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0004, 'learning_rate': 2.8912919749573137e-06, 'epoch': 8.65}\n",
      " 86% 1600/1850 [02:19<00:15, 16.21it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.11it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.5588736534118652, 'eval_f1': 0.5732227994346045, 'eval_runtime': 2.127, 'eval_samples_per_second': 94.031, 'eval_steps_per_second': 11.754, 'epoch': 8.65}\n",
      " 86% 1600/1850 [02:21<00:15, 16.21it/s]\n",
      "100% 25/25 [00:02<00:00, 111.05it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600/special_tokens_map.json\n",
      "{'loss': 0.0004, 'learning_rate': 6.146841206602163e-07, 'epoch': 9.73}\n",
      " 97% 1800/1850 [02:36<00:02, 17.18it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 106.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.5592520236968994, 'eval_f1': 0.5732227994346045, 'eval_runtime': 2.1793, 'eval_samples_per_second': 91.773, 'eval_steps_per_second': 11.472, 'epoch': 9.73}\n",
      " 97% 1800/1850 [02:38<00:02, 17.18it/s]\n",
      "100% 25/25 [00:02<00:00, 103.25it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1600] due to args.save_total_limit\n",
      "100% 1850/1850 [02:45<00:00, 16.17it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1400 (score: 0.6601187048245872).\n",
      "{'train_runtime': 166.1033, 'train_samples_per_second': 88.68, 'train_steps_per_second': 11.138, 'train_loss': 0.1291791299952043, 'epoch': 10.0}\n",
      "100% 1850/1850 [02:45<00:00, 16.17it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample/checkpoint-1800] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 35 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 35 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "100% 1850/1850 [02:46<00:00, 11.10it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.98it/s] \n",
      "f1 0.6601187048245872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.2917    0.4118        24\n",
      "           1     0.7045    0.6596    0.6813        47\n",
      "           2     0.8356    0.9457    0.8873       129\n",
      "\n",
      "    accuracy                         0.8000       200\n",
      "   macro avg     0.7467    0.6323    0.6601       200\n",
      "weighted avg     0.7885    0.8000    0.7818       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.6442471946912436\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-65\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=augmentation-upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1ZrABpKR2Sx",
    "outputId": "b31944ad-17b0-4c59-e733-a574ee4ffaa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-15 13:31:09.089970: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 13:31:10.609193: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 13:31:10.609333: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 13:31:10.609355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-67\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-67\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 317.85it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-135f35376c3d460f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c1dca4a919ebe264.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-67\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5000\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/5000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.9225, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.4}\n",
      "  4% 200/5000 [00:13<04:45, 16.83it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.21it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7570574879646301, 'eval_f1': 0.25641025641025644, 'eval_runtime': 2.1394, 'eval_samples_per_second': 93.486, 'eval_steps_per_second': 11.686, 'epoch': 0.4}\n",
      "  4% 200/5000 [00:15<04:45, 16.83it/s]\n",
      "100% 25/25 [00:02<00:00, 97.10it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.6248, 'learning_rate': 9.68842105263158e-06, 'epoch': 0.8}\n",
      "  8% 400/5000 [00:30<04:26, 17.24it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 116.65it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6233218908309937, 'eval_f1': 0.4870370370370371, 'eval_runtime': 2.0933, 'eval_samples_per_second': 95.541, 'eval_steps_per_second': 11.943, 'epoch': 0.8}\n",
      "  8% 400/5000 [00:32<04:26, 17.24it/s]\n",
      "100% 25/25 [00:02<00:00, 102.08it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.4317, 'learning_rate': 9.27157894736842e-06, 'epoch': 1.2}\n",
      " 12% 600/5000 [00:48<04:25, 16.60it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 108.26it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6316681504249573, 'eval_f1': 0.6712352837814092, 'eval_runtime': 2.1405, 'eval_samples_per_second': 93.438, 'eval_steps_per_second': 11.68, 'epoch': 1.2}\n",
      " 12% 600/5000 [00:50<04:25, 16.60it/s]\n",
      "100% 25/25 [00:02<00:00, 102.74it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.3453, 'learning_rate': 8.850526315789474e-06, 'epoch': 1.6}\n",
      " 16% 800/5000 [01:05<04:24, 15.87it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 89.04it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.8740056753158569, 'eval_f1': 0.6520486688459813, 'eval_runtime': 2.1893, 'eval_samples_per_second': 91.353, 'eval_steps_per_second': 11.419, 'epoch': 1.6}\n",
      " 16% 800/5000 [01:07<04:24, 15.87it/s]\n",
      "100% 25/25 [00:02<00:00, 80.40it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.3081, 'learning_rate': 8.429473684210527e-06, 'epoch': 2.0}\n",
      " 20% 1000/5000 [01:22<04:40, 14.26it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 73.69it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 70.26it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8766067624092102, 'eval_f1': 0.6739950525664811, 'eval_runtime': 2.2539, 'eval_samples_per_second': 88.734, 'eval_steps_per_second': 11.092, 'epoch': 2.0}\n",
      " 20% 1000/5000 [01:24<04:40, 14.26it/s]\n",
      "100% 25/25 [00:02<00:00, 63.53it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-600] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.1747, 'learning_rate': 8.00842105263158e-06, 'epoch': 2.4}\n",
      " 24% 1200/5000 [01:39<04:23, 14.40it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 36% 9/25 [00:00<00:00, 80.37it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9674209356307983, 'eval_f1': 0.6528298282810918, 'eval_runtime': 2.2351, 'eval_samples_per_second': 89.48, 'eval_steps_per_second': 11.185, 'epoch': 2.4}\n",
      " 24% 1200/5000 [01:42<04:23, 14.40it/s]\n",
      "100% 25/25 [00:02<00:00, 69.36it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1200/special_tokens_map.json\n",
      "{'loss': 0.1841, 'learning_rate': 7.587368421052632e-06, 'epoch': 2.8}\n",
      " 28% 1400/5000 [01:57<04:02, 14.82it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 99.67it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8555294871330261, 'eval_f1': 0.7274034738356013, 'eval_runtime': 2.1279, 'eval_samples_per_second': 93.99, 'eval_steps_per_second': 11.749, 'epoch': 2.8}\n",
      " 28% 1400/5000 [01:59<04:02, 14.82it/s]\n",
      "100% 25/25 [00:02<00:00, 98.62it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1000] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.1801, 'learning_rate': 7.1663157894736844e-06, 'epoch': 3.2}\n",
      " 32% 1600/5000 [02:15<03:09, 17.94it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 108.95it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.913229763507843, 'eval_f1': 0.7171355907378064, 'eval_runtime': 2.106, 'eval_samples_per_second': 94.966, 'eval_steps_per_second': 11.871, 'epoch': 3.2}\n",
      " 32% 1600/5000 [02:17<03:09, 17.94it/s]\n",
      "100% 25/25 [00:02<00:00, 103.54it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1600/special_tokens_map.json\n",
      "{'loss': 0.1321, 'learning_rate': 6.745263157894737e-06, 'epoch': 3.6}\n",
      " 36% 1800/5000 [02:32<03:05, 17.29it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.00it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1289845705032349, 'eval_f1': 0.7009411008584676, 'eval_runtime': 2.1209, 'eval_samples_per_second': 94.3, 'eval_steps_per_second': 11.788, 'epoch': 3.6}\n",
      " 36% 1800/5000 [02:35<03:05, 17.29it/s]\n",
      "100% 25/25 [00:02<00:00, 98.50it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1600] due to args.save_total_limit\n",
      "{'loss': 0.0859, 'learning_rate': 6.324210526315791e-06, 'epoch': 4.0}\n",
      " 40% 2000/5000 [02:50<02:47, 17.91it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.35it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1338658332824707, 'eval_f1': 0.7080063644673144, 'eval_runtime': 2.1351, 'eval_samples_per_second': 93.671, 'eval_steps_per_second': 11.709, 'epoch': 4.0}\n",
      " 40% 2000/5000 [02:52<02:47, 17.91it/s]\n",
      "100% 25/25 [00:02<00:00, 101.05it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-1800] due to args.save_total_limit\n",
      "{'loss': 0.0707, 'learning_rate': 5.903157894736843e-06, 'epoch': 4.4}\n",
      " 44% 2200/5000 [03:08<02:36, 17.94it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, id. If text, id are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.65it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1512999534606934, 'eval_f1': 0.7028140885984023, 'eval_runtime': 2.0948, 'eval_samples_per_second': 95.472, 'eval_steps_per_second': 11.934, 'epoch': 4.4}\n",
      " 44% 2200/5000 [03:10<02:36, 17.94it/s]\n",
      "100% 25/25 [00:02<00:00, 101.32it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-extend-lr1e-5/checkpoint-2000] due to args.save_total_limit\n",
      " 47% 2355/5000 [03:23<02:32, 17.35it/s]\n",
      "Aborted!\n",
      " 47% 2355/5000 [03:23<03:48, 11.59it/s]\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-67\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=augmentation-extend-lr1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoEprOLjZRB0",
    "outputId": "6349cfcc-3cb7-49fb-8f3c-f0c4a6c26712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-15 13:16:08.778253: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 13:16:10.089820: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 13:16:10.090001: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 13:16:10.090030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 297.09it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-135f35376c3d460f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-812bea9444c151bb.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8f5e911b7229bdf8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d34077e3c43a0f38.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c1dca4a919ebe264.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1419\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1780\n",
      "  Number of trainable parameters = 66955779\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1221: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/1780 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.9171, 'learning_rate': 9.349497338852752e-06, 'epoch': 1.12}\n",
      " 11% 200/1780 [00:13<01:27, 18.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 113.44it/s]\u001B[A\n",
      " 96% 24/25 [00:00<00:00, 98.14it/s] \u001B[A\n",
      "{'eval_loss': 0.6365002393722534, 'eval_f1': 0.641103341103341, 'eval_runtime': 2.1234, 'eval_samples_per_second': 94.187, 'eval_steps_per_second': 11.773, 'epoch': 1.12}\n",
      "\n",
      " 11% 200/1780 [00:15<01:27, 18.12it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.4149, 'learning_rate': 8.172678888231817e-06, 'epoch': 2.25}\n",
      " 22% 400/1780 [00:30<01:17, 17.88it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 109.65it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6224255561828613, 'eval_f1': 0.6924742130589191, 'eval_runtime': 2.1073, 'eval_samples_per_second': 94.908, 'eval_steps_per_second': 11.864, 'epoch': 2.25}\n",
      " 22% 400/1780 [00:32<01:17, 17.88it/s]\n",
      "100% 25/25 [00:02<00:00, 104.48it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.2143, 'learning_rate': 6.995860437610882e-06, 'epoch': 3.37}\n",
      " 34% 600/1780 [00:47<01:07, 17.56it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.58it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6641584038734436, 'eval_f1': 0.720879120879121, 'eval_runtime': 2.101, 'eval_samples_per_second': 95.193, 'eval_steps_per_second': 11.899, 'epoch': 3.37}\n",
      " 34% 600/1780 [00:49<01:07, 17.56it/s]\n",
      "100% 25/25 [00:02<00:00, 96.32it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.107, 'learning_rate': 5.819041986989947e-06, 'epoch': 4.49}\n",
      " 45% 800/1780 [01:05<00:56, 17.37it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.01it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.8499879240989685, 'eval_f1': 0.6898348072353016, 'eval_runtime': 2.1058, 'eval_samples_per_second': 94.974, 'eval_steps_per_second': 11.872, 'epoch': 4.49}\n",
      " 45% 800/1780 [01:07<00:56, 17.37it/s]\n",
      "100% 25/25 [00:02<00:00, 99.97it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/special_tokens_map.json\n",
      "{'loss': 0.0624, 'learning_rate': 4.636309875813128e-06, 'epoch': 5.62}\n",
      " 56% 1000/1780 [01:22<00:50, 15.41it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.33it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8754405379295349, 'eval_f1': 0.7061688311688311, 'eval_runtime': 2.1466, 'eval_samples_per_second': 93.172, 'eval_steps_per_second': 11.646, 'epoch': 5.62}\n",
      " 56% 1000/1780 [01:24<00:50, 15.41it/s]\n",
      "100% 25/25 [00:02<00:00, 96.43it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0322, 'learning_rate': 3.45357776463631e-06, 'epoch': 6.74}\n",
      " 67% 1200/1780 [01:39<00:38, 15.18it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 92.09it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9484775066375732, 'eval_f1': 0.711255494537526, 'eval_runtime': 2.1855, 'eval_samples_per_second': 91.513, 'eval_steps_per_second': 11.439, 'epoch': 6.74}\n",
      " 67% 1200/1780 [01:41<00:38, 15.18it/s]\n",
      "100% 25/25 [00:02<00:00, 85.42it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0383, 'learning_rate': 2.2708456534594915e-06, 'epoch': 7.87}\n",
      " 79% 1400/1780 [01:56<00:26, 14.43it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 74.05it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 70.32it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0580846071243286, 'eval_f1': 0.7142140468227424, 'eval_runtime': 2.2672, 'eval_samples_per_second': 88.215, 'eval_steps_per_second': 11.027, 'epoch': 7.87}\n",
      " 79% 1400/1780 [01:58<00:26, 14.43it/s]\n",
      "100% 25/25 [00:02<00:00, 65.59it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.018, 'learning_rate': 1.0881135422826731e-06, 'epoch': 8.99}\n",
      " 90% 1600/1780 [02:14<00:12, 14.22it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 73.81it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 70.13it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0398141145706177, 'eval_f1': 0.7162208860322069, 'eval_runtime': 2.2604, 'eval_samples_per_second': 88.48, 'eval_steps_per_second': 11.06, 'epoch': 8.99}\n",
      " 90% 1600/1780 [02:16<00:12, 14.22it/s]\n",
      "100% 25/25 [00:02<00:00, 65.76it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600 (score: 0.720879120879121).\n",
      "{'train_runtime': 139.7807, 'train_samples_per_second': 101.516, 'train_steps_per_second': 12.734, 'train_loss': 0.22553105890750885, 'epoch': 8.99}\n",
      " 90% 1600/1780 [02:19<00:12, 14.22it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 38 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 38 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      " 90% 1600/1780 [02:20<00:15, 11.40it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.69it/s]\n",
      "f1 0.720879120879121\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.6667    0.5714        12\n",
      "           1     0.8163    0.6349    0.7143        63\n",
      "           2     0.8444    0.9120    0.8769       125\n",
      "\n",
      "    accuracy                         0.8100       200\n",
      "   macro avg     0.7203    0.7379    0.7209       200\n",
      "weighted avg     0.8149    0.8100    0.8074       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9f438207c4d6ef9b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6f13f78688aa2720.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2b3135066cb61a0c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d7e937b243529bcb.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1416\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1770\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8871, 'learning_rate': 9.34562760261749e-06, 'epoch': 1.13}\n",
      " 11% 200/1770 [00:12<01:55, 13.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 72.33it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 70.46it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.685457170009613, 'eval_f1': 0.5415749338976404, 'eval_runtime': 2.2237, 'eval_samples_per_second': 89.94, 'eval_steps_per_second': 11.243, 'epoch': 1.13}\n",
      " 11% 200/1770 [00:14<01:55, 13.57it/s]\n",
      "100% 25/25 [00:02<00:00, 69.70it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.3725, 'learning_rate': 8.161808447352767e-06, 'epoch': 2.26}\n",
      " 23% 400/1770 [00:30<01:24, 16.19it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 103.57it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.700895369052887, 'eval_f1': 0.5281067577887365, 'eval_runtime': 2.1079, 'eval_samples_per_second': 94.881, 'eval_steps_per_second': 11.86, 'epoch': 2.26}\n",
      " 23% 400/1770 [00:32<01:24, 16.19it/s]\n",
      "100% 25/25 [00:02<00:00, 100.19it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/special_tokens_map.json\n",
      "{'loss': 0.1671, 'learning_rate': 6.977989292088044e-06, 'epoch': 3.39}\n",
      " 34% 600/1770 [00:48<01:11, 16.42it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 113.74it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.9742214679718018, 'eval_f1': 0.5331151118385161, 'eval_runtime': 2.0906, 'eval_samples_per_second': 95.668, 'eval_steps_per_second': 11.959, 'epoch': 3.39}\n",
      " 34% 600/1770 [00:50<01:11, 16.42it/s]\n",
      "100% 25/25 [00:02<00:00, 105.24it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0934, 'learning_rate': 5.794170136823319e-06, 'epoch': 4.52}\n",
      " 45% 800/1770 [01:06<00:56, 17.14it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.66it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.0988938808441162, 'eval_f1': 0.5189125295508275, 'eval_runtime': 2.0876, 'eval_samples_per_second': 95.804, 'eval_steps_per_second': 11.976, 'epoch': 4.52}\n",
      " 45% 800/1770 [01:08<00:56, 17.14it/s]\n",
      "100% 25/25 [00:02<00:00, 102.42it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.073, 'learning_rate': 4.604402141582392e-06, 'epoch': 5.65}\n",
      " 56% 1000/1770 [01:24<00:45, 16.76it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 106.15it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1979424953460693, 'eval_f1': 0.4636031380217427, 'eval_runtime': 2.0868, 'eval_samples_per_second': 95.84, 'eval_steps_per_second': 11.98, 'epoch': 5.65}\n",
      " 56% 1000/1770 [01:26<00:45, 16.76it/s]\n",
      "100% 25/25 [00:02<00:00, 105.48it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0293, 'learning_rate': 3.414634146341464e-06, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:42<00:33, 17.22it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 108.58it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2363406419754028, 'eval_f1': 0.46709479268462933, 'eval_runtime': 2.0984, 'eval_samples_per_second': 95.311, 'eval_steps_per_second': 11.914, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:44<00:33, 17.22it/s]\n",
      "100% 25/25 [00:02<00:00, 106.48it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200 (score: 0.5415749338976404).\n",
      "{'train_runtime': 107.9286, 'train_samples_per_second': 131.198, 'train_steps_per_second': 16.4, 'train_loss': 0.2703971064090729, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:47<00:33, 17.22it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      " 68% 1200/1770 [01:48<00:51, 11.06it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.20it/s]\n",
      "f1 0.5415749338976404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2632    0.3571    0.3030        14\n",
      "           1     0.5192    0.5000    0.5094        54\n",
      "           2     0.8217    0.8030    0.8123       132\n",
      "\n",
      "    accuracy                         0.6900       200\n",
      "   macro avg     0.5347    0.5534    0.5416       200\n",
      "weighted avg     0.7009    0.6900    0.6949       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b9cef8bda46790e3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-86a534e58039eacb.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f29183361658e0ee.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-47109cdfc04ca0c7.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1466\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1840\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9332, 'learning_rate': 9.393592677345538e-06, 'epoch': 1.09}\n",
      " 11% 200/1840 [00:12<01:38, 16.73it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.10it/s]\u001B[A\n",
      " 96% 24/25 [00:00<00:00, 95.20it/s] \u001B[A\n",
      "{'eval_loss': 0.7318554520606995, 'eval_f1': 0.4992618670421874, 'eval_runtime': 2.1339, 'eval_samples_per_second': 93.724, 'eval_steps_per_second': 11.716, 'epoch': 1.09}\n",
      "\n",
      " 11% 200/1840 [00:14<01:38, 16.73it/s]\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/special_tokens_map.json\n",
      "{'loss': 0.4217, 'learning_rate': 8.255148741418765e-06, 'epoch': 2.17}\n",
      " 22% 400/1840 [00:30<01:31, 15.79it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.68it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6849534511566162, 'eval_f1': 0.5873519778281683, 'eval_runtime': 2.1481, 'eval_samples_per_second': 93.106, 'eval_steps_per_second': 11.638, 'epoch': 2.17}\n",
      " 22% 400/1840 [00:32<01:31, 15.79it/s]\n",
      "100% 25/25 [00:02<00:00, 96.38it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1878, 'learning_rate': 7.116704805491992e-06, 'epoch': 3.26}\n",
      " 33% 600/1840 [00:47<01:27, 14.25it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.00it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 73.77it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7475094795227051, 'eval_f1': 0.6222222222222222, 'eval_runtime': 2.246, 'eval_samples_per_second': 89.047, 'eval_steps_per_second': 11.131, 'epoch': 3.26}\n",
      " 33% 600/1840 [00:50<01:27, 14.25it/s]\n",
      "100% 25/25 [00:02<00:00, 67.46it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0695, 'learning_rate': 5.9725400457665905e-06, 'epoch': 4.35}\n",
      " 43% 800/1840 [01:05<01:12, 14.34it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 79.29it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 70.97it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.043481707572937, 'eval_f1': 0.6353881278538812, 'eval_runtime': 2.2556, 'eval_samples_per_second': 88.669, 'eval_steps_per_second': 11.084, 'epoch': 4.35}\n",
      " 43% 800/1840 [01:07<01:12, 14.34it/s]\n",
      "100% 25/25 [00:02<00:00, 64.04it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0425, 'learning_rate': 4.82837528604119e-06, 'epoch': 5.43}\n",
      " 54% 1000/1840 [01:23<00:56, 14.84it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.69it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1135845184326172, 'eval_f1': 0.5747820672478207, 'eval_runtime': 2.0998, 'eval_samples_per_second': 95.247, 'eval_steps_per_second': 11.906, 'epoch': 5.43}\n",
      " 54% 1000/1840 [01:25<00:56, 14.84it/s]\n",
      "100% 25/25 [00:02<00:00, 98.08it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.0237, 'learning_rate': 3.6842105263157896e-06, 'epoch': 6.52}\n",
      " 65% 1200/1840 [01:41<00:38, 16.57it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 117.28it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2760120630264282, 'eval_f1': 0.5784447112178205, 'eval_runtime': 2.109, 'eval_samples_per_second': 94.831, 'eval_steps_per_second': 11.854, 'epoch': 6.52}\n",
      " 65% 1200/1840 [01:43<00:38, 16.57it/s]\n",
      "100% 25/25 [00:02<00:00, 95.69it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0034, 'learning_rate': 2.540045766590389e-06, 'epoch': 7.61}\n",
      " 76% 1400/1840 [01:59<00:26, 16.62it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 107.44it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.3095682859420776, 'eval_f1': 0.5668872073716363, 'eval_runtime': 2.1258, 'eval_samples_per_second': 94.083, 'eval_steps_per_second': 11.76, 'epoch': 7.61}\n",
      " 76% 1400/1840 [02:01<00:26, 16.62it/s]\n",
      "100% 25/25 [00:02<00:00, 95.08it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0097, 'learning_rate': 1.3958810068649885e-06, 'epoch': 8.7}\n",
      " 87% 1600/1840 [02:17<00:13, 17.30it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 110.28it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.3091970682144165, 'eval_f1': 0.589212072244946, 'eval_runtime': 2.1236, 'eval_samples_per_second': 94.179, 'eval_steps_per_second': 11.772, 'epoch': 8.7}\n",
      " 87% 1600/1840 [02:19<00:13, 17.30it/s]\n",
      "100% 25/25 [00:02<00:00, 91.73it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.0093, 'learning_rate': 2.517162471395881e-07, 'epoch': 9.78}\n",
      " 98% 1800/1840 [02:35<00:02, 17.05it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 107.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.3853747844696045, 'eval_f1': 0.549454200284765, 'eval_runtime': 2.1289, 'eval_samples_per_second': 93.943, 'eval_steps_per_second': 11.743, 'epoch': 9.78}\n",
      " 98% 1800/1840 [02:37<00:02, 17.05it/s]\n",
      "100% 25/25 [00:02<00:00, 88.44it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800 (score: 0.6353881278538812).\n",
      "{'train_runtime': 161.1102, 'train_samples_per_second': 90.994, 'train_steps_per_second': 11.421, 'train_loss': 0.18897002670500013, 'epoch': 9.78}\n",
      " 98% 1800/1840 [02:40<00:02, 17.05it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 44 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 44 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      " 98% 1800/1840 [02:41<00:03, 11.14it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.07it/s]\n",
      "f1 0.6353881278538812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5833    0.4375    0.5000        16\n",
      "           1     0.8800    0.4000    0.5500        55\n",
      "           2     0.7669    0.9690    0.8562       129\n",
      "\n",
      "    accuracy                         0.7700       200\n",
      "   macro avg     0.7434    0.6022    0.6354       200\n",
      "weighted avg     0.7833    0.7700    0.7435       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-555d45043097e134.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a419fc982a2995a4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4bf01af4826335d3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6fd7d7933a2794be.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1415\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1770\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.8839, 'learning_rate': 9.34562760261749e-06, 'epoch': 1.13}\n",
      " 11% 200/1770 [00:12<01:29, 17.50it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 101.98it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6436288356781006, 'eval_f1': 0.5487427601778226, 'eval_runtime': 2.1391, 'eval_samples_per_second': 93.499, 'eval_steps_per_second': 11.687, 'epoch': 1.13}\n",
      " 11% 200/1770 [00:15<01:29, 17.50it/s]\n",
      "100% 25/25 [00:02<00:00, 95.97it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.3827, 'learning_rate': 8.167757287328972e-06, 'epoch': 2.26}\n",
      " 23% 400/1770 [00:30<01:33, 14.66it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 40% 10/25 [00:00<00:00, 90.12it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.720090925693512, 'eval_f1': 0.6028598730462084, 'eval_runtime': 2.1887, 'eval_samples_per_second': 91.38, 'eval_steps_per_second': 11.422, 'epoch': 2.26}\n",
      " 23% 400/1770 [00:32<01:33, 14.66it/s]\n",
      "100% 25/25 [00:02<00:00, 79.11it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.2013, 'learning_rate': 6.977989292088044e-06, 'epoch': 3.39}\n",
      " 34% 600/1770 [00:48<01:23, 14.01it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 78.63it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 66.30it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.9647522568702698, 'eval_f1': 0.5543758967001434, 'eval_runtime': 2.2438, 'eval_samples_per_second': 89.134, 'eval_steps_per_second': 11.142, 'epoch': 3.39}\n",
      " 34% 600/1770 [00:50<01:23, 14.01it/s]\n",
      "100% 25/25 [00:02<00:00, 65.81it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/special_tokens_map.json\n",
      "{'loss': 0.095, 'learning_rate': 5.794170136823319e-06, 'epoch': 4.52}\n",
      " 45% 800/1770 [01:06<01:18, 12.38it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.53it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.9921578764915466, 'eval_f1': 0.6120951971192277, 'eval_runtime': 2.0925, 'eval_samples_per_second': 95.581, 'eval_steps_per_second': 11.948, 'epoch': 4.52}\n",
      " 45% 800/1770 [01:08<01:18, 12.38it/s]\n",
      "100% 25/25 [00:02<00:00, 105.43it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.0434, 'learning_rate': 4.604402141582392e-06, 'epoch': 5.65}\n",
      " 56% 1000/1770 [01:24<00:43, 17.63it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 119.44it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0905036926269531, 'eval_f1': 0.6410751456478095, 'eval_runtime': 2.1085, 'eval_samples_per_second': 94.856, 'eval_steps_per_second': 11.857, 'epoch': 5.65}\n",
      " 56% 1000/1770 [01:26<00:43, 17.63it/s]\n",
      "100% 25/25 [00:02<00:00, 106.51it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800] due to args.save_total_limit\n",
      "{'loss': 0.0237, 'learning_rate': 3.414634146341464e-06, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:42<00:37, 15.30it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 110.59it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1642385721206665, 'eval_f1': 0.6157853099790525, 'eval_runtime': 2.1293, 'eval_samples_per_second': 93.929, 'eval_steps_per_second': 11.741, 'epoch': 6.78}\n",
      " 68% 1200/1770 [01:44<00:37, 15.30it/s]\n",
      "100% 25/25 [00:02<00:00, 97.51it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/special_tokens_map.json\n",
      "{'loss': 0.0172, 'learning_rate': 2.224866151100536e-06, 'epoch': 7.91}\n",
      " 79% 1400/1770 [02:00<00:22, 16.72it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 44% 11/25 [00:00<00:00, 109.10it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.230597734451294, 'eval_f1': 0.6070182129919756, 'eval_runtime': 2.1167, 'eval_samples_per_second': 94.485, 'eval_steps_per_second': 11.811, 'epoch': 7.91}\n",
      " 79% 1400/1770 [02:02<00:22, 16.72it/s]\n",
      "100% 25/25 [00:02<00:00, 94.89it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200] due to args.save_total_limit\n",
      "{'loss': 0.0118, 'learning_rate': 1.0350981558596076e-06, 'epoch': 9.04}\n",
      " 90% 1600/1770 [02:18<00:09, 17.14it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 112.31it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2091624736785889, 'eval_f1': 0.6324789505778751, 'eval_runtime': 2.1044, 'eval_samples_per_second': 95.037, 'eval_steps_per_second': 11.88, 'epoch': 9.04}\n",
      " 90% 1600/1770 [02:20<00:09, 17.14it/s]\n",
      "100% 25/25 [00:02<00:00, 101.36it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400] due to args.save_total_limit\n",
      "100% 1770/1770 [02:34<00:00, 17.54it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000 (score: 0.6410751456478095).\n",
      "{'train_runtime': 154.704, 'train_samples_per_second': 91.465, 'train_steps_per_second': 11.441, 'train_loss': 0.1886665683681682, 'epoch': 10.0}\n",
      "100% 1770/1770 [02:34<00:00, 17.54it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 39 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 39 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "100% 1770/1770 [02:35<00:00, 11.40it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.79it/s]\n",
      "f1 0.6410751456478095\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4545    0.3846    0.4167        13\n",
      "           1     0.8611    0.5082    0.6392        61\n",
      "           2     0.7908    0.9603    0.8674       126\n",
      "\n",
      "    accuracy                         0.7850       200\n",
      "   macro avg     0.7022    0.6177    0.6411       200\n",
      "weighted avg     0.7904    0.7850    0.7685       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ee8a03802eae7bec.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-daca641bac7e7610.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fda024722c5b90e4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-c7a5f207955e5608/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b6a2e16bf7233b30.arrow\n",
      "\n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1473\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1850\n",
      "  Number of trainable parameters = 66955779\n",
      "{'loss': 0.9488, 'learning_rate': 9.408081957882755e-06, 'epoch': 1.08}\n",
      " 11% 200/1850 [00:13<01:42, 16.05it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 115.73it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.7934655547142029, 'eval_f1': 0.4087626139981652, 'eval_runtime': 2.1145, 'eval_samples_per_second': 94.583, 'eval_steps_per_second': 11.823, 'epoch': 1.08}\n",
      " 11% 200/1850 [00:15<01:42, 16.05it/s]\n",
      "100% 25/25 [00:02<00:00, 104.90it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.4606, 'learning_rate': 8.269778030734207e-06, 'epoch': 2.16}\n",
      " 22% 400/1850 [00:31<01:27, 16.61it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 114.13it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.761710524559021, 'eval_f1': 0.49166040441208225, 'eval_runtime': 2.1023, 'eval_samples_per_second': 95.136, 'eval_steps_per_second': 11.892, 'epoch': 2.16}\n",
      " 22% 400/1850 [00:33<01:27, 16.61it/s]\n",
      "100% 25/25 [00:02<00:00, 103.31it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-200] due to args.save_total_limit\n",
      "{'loss': 0.1907, 'learning_rate': 7.131474103585658e-06, 'epoch': 3.24}\n",
      " 32% 600/1850 [00:48<01:17, 16.10it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 111.89it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.8355931043624878, 'eval_f1': 0.5740137998474176, 'eval_runtime': 2.1258, 'eval_samples_per_second': 94.083, 'eval_steps_per_second': 11.76, 'epoch': 3.24}\n",
      " 32% 600/1850 [00:51<01:17, 16.10it/s]\n",
      "100% 25/25 [00:02<00:00, 103.39it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-400] due to args.save_total_limit\n",
      "{'loss': 0.0833, 'learning_rate': 5.998861696072852e-06, 'epoch': 4.32}\n",
      " 43% 800/1850 [01:06<01:05, 16.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 67.88it/s]\u001B[A\n",
      " 60% 15/25 [00:00<00:00, 68.93it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 1.025363802909851, 'eval_f1': 0.593184267597625, 'eval_runtime': 2.2366, 'eval_samples_per_second': 89.422, 'eval_steps_per_second': 11.178, 'epoch': 4.32}\n",
      " 43% 800/1850 [01:08<01:05, 16.12it/s]\n",
      "100% 25/25 [00:02<00:00, 70.40it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-600] due to args.save_total_limit\n",
      "{'loss': 0.023, 'learning_rate': 4.860557768924303e-06, 'epoch': 5.41}\n",
      " 54% 1000/1850 [01:24<01:00, 14.12it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 73.29it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 70.02it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2217307090759277, 'eval_f1': 0.5648611799593007, 'eval_runtime': 2.2463, 'eval_samples_per_second': 89.035, 'eval_steps_per_second': 11.129, 'epoch': 5.41}\n",
      " 54% 1000/1850 [01:26<01:00, 14.12it/s]\n",
      "100% 25/25 [00:02<00:00, 68.48it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.0185, 'learning_rate': 3.7222538417757544e-06, 'epoch': 6.49}\n",
      " 65% 1200/1850 [01:42<00:53, 12.06it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 32% 8/25 [00:00<00:00, 72.96it/s]\u001B[A\n",
      " 64% 16/25 [00:00<00:00, 68.16it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2106866836547852, 'eval_f1': 0.6235680665077328, 'eval_runtime': 2.262, 'eval_samples_per_second': 88.418, 'eval_steps_per_second': 11.052, 'epoch': 6.49}\n",
      " 65% 1200/1850 [01:44<00:53, 12.06it/s]\n",
      "100% 25/25 [00:02<00:00, 63.65it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-800] due to args.save_total_limit\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1000] due to args.save_total_limit\n",
      "{'loss': 0.0066, 'learning_rate': 2.583949914627206e-06, 'epoch': 7.57}\n",
      " 76% 1400/1850 [02:00<00:27, 16.39it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 110.99it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2258610725402832, 'eval_f1': 0.6124159139844438, 'eval_runtime': 2.103, 'eval_samples_per_second': 95.102, 'eval_steps_per_second': 11.888, 'epoch': 7.57}\n",
      " 76% 1400/1850 [02:02<00:27, 16.39it/s]\n",
      "100% 25/25 [00:02<00:00, 100.65it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400/special_tokens_map.json\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2675824165344238, 'eval_f1': 0.6061365412266714, 'eval_runtime': 2.1751, 'eval_samples_per_second': 91.951, 'eval_steps_per_second': 11.494, 'epoch': 8.65}\n",
      " 86% 1600/1850 [02:21<00:15, 16.23it/s]\n",
      "100% 25/25 [00:02<00:00, 101.52it/s]\u001B[A\n",
      "                                    \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1400] due to args.save_total_limit\n",
      "{'loss': 0.0042, 'learning_rate': 3.073420603301082e-07, 'epoch': 9.73}\n",
      " 97% 1800/1850 [02:37<00:02, 16.77it/s]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "\n",
      "  0% 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 48% 12/25 [00:00<00:00, 107.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.3614212274551392, 'eval_f1': 0.5559815608096202, 'eval_runtime': 2.7404, 'eval_samples_per_second': 72.981, 'eval_steps_per_second': 9.123, 'epoch': 9.73}\n",
      " 97% 1800/1850 [02:39<00:02, 16.77it/s]\n",
      "100% 25/25 [00:02<00:00, 99.62it/s]\u001B[A\n",
      "                                   \u001B[ASaving model checkpoint to /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800\n",
      "Configuration saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800/config.json\n",
      "Model weights saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1600] due to args.save_total_limit\n",
      "100% 1850/1850 [02:46<00:00, 13.57it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1200 (score: 0.6235680665077328).\n",
      "{'train_runtime': 167.1504, 'train_samples_per_second': 88.124, 'train_steps_per_second': 11.068, 'train_loss': 0.18795762820823772, 'epoch': 10.0}\n",
      "100% 1850/1850 [02:46<00:00, 13.57it/s]Deleting older checkpoint [/content/results/finetuning-distilbert-base-uncased-config-augmentation-upsample-lr1e-5/checkpoint-1800] due to args.save_total_limit\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 39 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 39 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "100% 1850/1850 [02:47<00:00, 11.03it/s]\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, text. If id, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 200\n",
      "  Batch size = 8\n",
      "100% 25/25 [00:02<00:00, 11.30it/s]\n",
      "f1 0.6235680665077328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.3333    0.4211        24\n",
      "           1     0.6857    0.5106    0.5854        47\n",
      "           2     0.8013    0.9380    0.8643       129\n",
      "\n",
      "    accuracy                         0.7650       200\n",
      "   macro avg     0.6862    0.5940    0.6236       200\n",
      "weighted avg     0.7466    0.7650    0.7456       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.6321816029387425\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 21 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 21 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-66\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=augmentation-upsample-lr1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcvibmC5Lay_"
   },
   "source": [
    "## EFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O77bUFs7Lf3K",
    "outputId": "7d2eb682-ba9b-466b-c0e3-b7ec0df21e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-15 22:17:54.221243: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 22:17:55.635130: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 22:17:55.635342: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 22:17:55.635361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 551.45it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a7d6048ffa37a8c2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-031c3931682d9ad4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a9a96ec33cd20cff.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-575c16c13f11d43a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1c3c0bfde0e49168.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-21141121ca7040a9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4f34d35c3993c230.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eb5397f716e093bd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-403ec24ed809ad67.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1c89d0bc3b5dda29.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7c72d992e9575749.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e181d17c1770f046.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2cfa008ef11960f7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-95a47a36b33af419.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6c44bfbcd81baca4.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fc1a03893284d29d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-21300b452955922c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c2d18e12a79bf379.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-07fe49a0fa4e2cae.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9549c37aaac0b483.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d9f06bd20c5c1eec.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d1fcc1039f1e48f1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1e936f820960d8b5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6f29fa23078781f3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f50ac7845eec1a7a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-57959795035ba72b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-09a8b720263d1b15.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-680fdf56336dc414.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eb51fa6cb4cb4c56.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-97069ca6371e9593.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fc5dec89386fee7d.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-852893d8f4a71e99.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-be1d33029587f931.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b435490f951e8c42.arrow\n",
      "\n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1276: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/6000 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.6008, 'learning_rate': 9.866666666666668e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:35<14:13,  6.68it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 71.26it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 66.66it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 63.85it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 62.37it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 63.41it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 53.76it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 49.98it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 47.52it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 46.71it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 46.84it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4989676773548126, 'eval_f1': 0.7368388991093946, 'eval_runtime': 2.2151, 'eval_samples_per_second': 270.865, 'eval_steps_per_second': 33.858, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:37<14:13,  6.68it/s]\n",
      "100% 75/75 [00:02<00:00, 41.43it/s]\u001B[A\n",
      "{'loss': 0.4563, 'learning_rate': 9.482456140350878e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:17<10:39,  8.44it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 69.81it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 66.18it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.83it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 63.18it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.77it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 52.54it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 56.10it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 56.46it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 58.28it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 53.22it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.39380204677581787, 'eval_f1': 0.7909519725557461, 'eval_runtime': 1.9401, 'eval_samples_per_second': 309.268, 'eval_steps_per_second': 38.659, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:19<10:39,  8.44it/s]\n",
      "100% 75/75 [00:01<00:00, 49.99it/s]\u001B[A\n",
      "{'loss': 0.3549, 'learning_rate': 8.956140350877193e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:00<12:10,  6.98it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 44.55it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 38.76it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 38.44it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 38.51it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 38.89it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 39.21it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 38.98it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 37.81it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 38.34it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 34.69it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 35.93it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 38.53it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 44.18it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 47.57it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 44.87it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3630593419075012, 'eval_f1': 0.8690629011553275, 'eval_runtime': 2.4432, 'eval_samples_per_second': 245.577, 'eval_steps_per_second': 30.697, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:03<12:10,  6.98it/s]\n",
      "100% 75/75 [00:02<00:00, 42.31it/s]\u001B[A\n",
      "{'loss': 0.3002, 'learning_rate': 8.42982456140351e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:43<08:22,  9.56it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 70.67it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 64.05it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 63.00it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 62.59it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 62.26it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 54.59it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 55.86it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 51.38it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 48.24it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 47.00it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5044209957122803, 'eval_f1': 0.881280637254902, 'eval_runtime': 2.1207, 'eval_samples_per_second': 282.92, 'eval_steps_per_second': 35.365, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:45<08:22,  9.56it/s]\n",
      "100% 75/75 [00:02<00:00, 39.01it/s]\u001B[A\n",
      "{'loss': 0.2206, 'learning_rate': 7.903508771929826e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:26<07:53,  9.51it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 71.37it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 65.66it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 63.84it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 62.28it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 63.41it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 55.11it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 56.84it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 58.14it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 57.83it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5234606266021729, 'eval_f1': 0.8992008976825716, 'eval_runtime': 1.9577, 'eval_samples_per_second': 306.485, 'eval_steps_per_second': 38.311, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:27<07:53,  9.51it/s]\n",
      "100% 75/75 [00:01<00:00, 51.89it/s]\u001B[A\n",
      "{'loss': 0.1695, 'learning_rate': 7.37719298245614e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:09<08:56,  7.83it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 42.05it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 39.31it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 39.02it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 38.45it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 38.02it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 38.62it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 41.26it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 48.89it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 44.73it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 49.77it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 52.19it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 53.30it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 48.98it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.513999342918396, 'eval_f1': 0.8816969696969698, 'eval_runtime': 2.2678, 'eval_samples_per_second': 264.572, 'eval_steps_per_second': 33.072, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:11<08:56,  7.83it/s]\n",
      "100% 75/75 [00:02<00:00, 45.96it/s]\u001B[A\n",
      "{'loss': 0.0833, 'learning_rate': 6.850877192982457e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:51<06:40,  9.74it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 70.73it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 67.95it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 61.13it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.49it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 55.37it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 46.05it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 45.28it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 45.55it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 45.38it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 46.06it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 45.84it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6031810641288757, 'eval_f1': 0.8947355230015414, 'eval_runtime': 2.2484, 'eval_samples_per_second': 266.86, 'eval_steps_per_second': 33.357, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:54<06:40,  9.74it/s]\n",
      "100% 75/75 [00:02<00:00, 37.79it/s]\u001B[A\n",
      "{'loss': 0.0608, 'learning_rate': 6.326315789473685e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:34<06:06,  9.82it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 71.48it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 65.53it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 62.94it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.63it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 61.95it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 53.45it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 56.37it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 58.25it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 58.54it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6619390845298767, 'eval_f1': 0.8845975576302769, 'eval_runtime': 1.9383, 'eval_samples_per_second': 309.553, 'eval_steps_per_second': 38.694, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:36<06:06,  9.82it/s]\n",
      "100% 75/75 [00:01<00:00, 52.21it/s]\u001B[A\n",
      "{'loss': 0.0412, 'learning_rate': 5.801754385964913e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:17<07:06,  7.74it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 49.40it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.76it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 40.85it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 39.48it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 38.19it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 37.54it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 37.80it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 37.86it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 38.39it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 34.57it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 36.11it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 36.65it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 36.67it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 37.39it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 44.74it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7101404070854187, 'eval_f1': 0.8848966113688181, 'eval_runtime': 2.5271, 'eval_samples_per_second': 237.426, 'eval_steps_per_second': 29.678, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:19<07:06,  7.74it/s]\n",
      "100% 75/75 [00:02<00:00, 38.27it/s]\u001B[A\n",
      "{'loss': 0.0315, 'learning_rate': 5.275438596491228e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:59<05:12,  9.59it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.24it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.00it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.14it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.01it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 57.99it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 51.91it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 54.07it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 57.46it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 58.90it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 54.75it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8127301931381226, 'eval_f1': 0.8788612961841309, 'eval_runtime': 2.1503, 'eval_samples_per_second': 279.035, 'eval_steps_per_second': 34.879, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:02<05:12,  9.59it/s]\n",
      "100% 75/75 [00:02<00:00, 41.78it/s]\u001B[A\n",
      "{'train_runtime': 430.7142, 'train_samples_per_second': 111.443, 'train_steps_per_second': 13.93, 'train_loss': 0.23191711870829265, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:10<05:12,  9.59it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      " 50% 3000/6000 [07:11<07:11,  6.96it/s]\n",
      "100% 75/75 [00:01<00:00, 37.87it/s]\n",
      "f1 0.7723902471622061\n",
      "efl_f1 0.8992008976825716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.5000    0.5714        12\n",
      "           1     0.8475    0.7937    0.8197        63\n",
      "           2     0.9015    0.9520    0.9261       125\n",
      "\n",
      "    accuracy                         0.8750       200\n",
      "   macro avg     0.8052    0.7486    0.7724       200\n",
      "weighted avg     0.8704    0.8750    0.8713       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-66e12c250a32db91.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ddf3052e61361864.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7013e08c7a517995.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-811f2d5250f2dccf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ebc1890012864818.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bd1bae00f3f83c9e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-589590c69bf9bbd9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fedf89db858f2461.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-49242d902b59ca53.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e302f3721e77c7e2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5b4c59ace3ca429f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f1dd50fe9affdc27.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f118c1f7973fc628.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a61c5df896c1c4df.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c937a37ae4f5a751.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3e9a9c0443a30b22.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-81c276ae543abd22.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c980cbe4f5b1edfc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ee7f40c1f6e31cac.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-84039628c04288a4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-41a467ecc8901634.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2d162ef3e11c6d44.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2bd310141fcbe19.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ae32580a2cab2b8f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-86cb8c45beaecffa.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-95c7661cdf76de72.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63ff2bd7cd0f56e5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f07a0082966c0acd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-199cd9544b8bd57f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cd338f747d6f3e95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-912ae878320c1650.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-742e569161d13bef.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0cca48cf2d47e797.arrow\n",
      "\n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.6254, 'learning_rate': 9.9e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:34<09:38,  9.85it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 66.38it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.57it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 59.85it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.65it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 58.36it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 59.54it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 60.20it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 60.18it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 59.73it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 58.31it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4627319276332855, 'eval_f1': 0.745, 'eval_runtime': 1.8564, 'eval_samples_per_second': 323.211, 'eval_steps_per_second': 40.401, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:36<09:38,  9.85it/s]\n",
      "100% 75/75 [00:01<00:00, 59.73it/s]\u001B[A\n",
      "{'loss': 0.4432, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:18<12:34,  7.16it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 47.19it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 41.14it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 39.50it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 39.39it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 39.98it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:01, 39.88it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 40.29it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 40.55it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 44.89it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 50.11it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 50.72it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 54.79it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3681478500366211, 'eval_f1': 0.8320160230870287, 'eval_runtime': 2.1751, 'eval_samples_per_second': 275.851, 'eval_steps_per_second': 34.481, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:20<12:34,  7.16it/s]\n",
      "100% 75/75 [00:02<00:00, 55.08it/s]\u001B[A\n",
      "{'loss': 0.3465, 'learning_rate': 8.956140350877193e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:01<08:52,  9.58it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.81it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.89it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 54.20it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 49.14it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 47.76it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 47.10it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 47.01it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 46.63it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 46.99it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 46.65it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 46.21it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 46.02it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4087100923061371, 'eval_f1': 0.8554216867469879, 'eval_runtime': 2.1907, 'eval_samples_per_second': 273.884, 'eval_steps_per_second': 34.236, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:03<08:52,  9.58it/s]\n",
      "100% 75/75 [00:02<00:00, 44.30it/s]\u001B[A\n",
      "{'loss': 0.2611, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:44<08:01,  9.96it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.39it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.06it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.16it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.36it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.87it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.70it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.12it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 61.91it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 59.77it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5276432037353516, 'eval_f1': 0.883457186180278, 'eval_runtime': 1.8393, 'eval_samples_per_second': 326.212, 'eval_steps_per_second': 40.776, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:46<08:01,  9.96it/s]\n",
      "100% 75/75 [00:01<00:00, 57.45it/s]\u001B[A\n",
      "{'loss': 0.2024, 'learning_rate': 7.905263157894737e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:27<09:54,  7.57it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 49.86it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 54.48it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 55.38it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 57.40it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 56.33it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 59.26it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 59.48it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 60.98it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 59.56it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 60.88it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.603251039981842, 'eval_f1': 0.8843226031145828, 'eval_runtime': 1.8767, 'eval_samples_per_second': 319.702, 'eval_steps_per_second': 39.963, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:29<09:54,  7.57it/s]\n",
      "100% 75/75 [00:01<00:00, 59.94it/s]\u001B[A\n",
      "{'loss': 0.1576, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:10<08:24,  8.32it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 53.14it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 47.09it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 45.88it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 44.75it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 45.09it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 43.79it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 45.09it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 43.97it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 45.30it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 45.21it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 44.63it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 45.06it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 44.30it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7407573461532593, 'eval_f1': 0.8765570643025474, 'eval_runtime': 2.2931, 'eval_samples_per_second': 261.65, 'eval_steps_per_second': 32.706, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:12<08:24,  8.32it/s]\n",
      "100% 75/75 [00:02<00:00, 43.84it/s]\u001B[A\n",
      "{'loss': 0.1029, 'learning_rate': 6.8526315789473685e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:53<07:56,  8.18it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.90it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.15it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.22it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.13it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.29it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 59.12it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 59.01it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 58.94it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 57.38it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 57.41it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8291894793510437, 'eval_f1': 0.8748408267977621, 'eval_runtime': 1.8897, 'eval_samples_per_second': 317.518, 'eval_steps_per_second': 39.69, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:55<07:56,  8.18it/s]\n",
      "100% 75/75 [00:01<00:00, 57.09it/s]\u001B[A\n",
      "{'loss': 0.0629, 'learning_rate': 6.326315789473685e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:36<07:05,  8.45it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 67.78it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 59.90it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.35it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.37it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.04it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.91it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.66it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.06it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 59.24it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8661456108093262, 'eval_f1': 0.8711860982016568, 'eval_runtime': 1.8444, 'eval_samples_per_second': 325.305, 'eval_steps_per_second': 40.663, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:38<07:05,  8.45it/s]\n",
      "100% 75/75 [00:01<00:00, 58.17it/s]\u001B[A\n",
      "{'loss': 0.0302, 'learning_rate': 5.8e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:19<06:34,  8.37it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 55.49it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 47.02it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 47.32it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 46.33it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 44.34it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 45.44it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 46.16it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 45.78it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 46.64it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 47.17it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 46.06it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 45.81it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 44.97it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9211671948432922, 'eval_f1': 0.8787633865427359, 'eval_runtime': 2.2662, 'eval_samples_per_second': 264.764, 'eval_steps_per_second': 33.096, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:21<06:34,  8.37it/s]\n",
      "100% 75/75 [00:02<00:00, 43.81it/s]\u001B[A\n",
      "{'loss': 0.0421, 'learning_rate': 5.273684210526317e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:02<05:08,  9.71it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.97it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 57.29it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 58.33it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.74it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 58.33it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 59.15it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 60.18it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 60.12it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 58.62it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.41it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9415091872215271, 'eval_f1': 0.8765570643025474, 'eval_runtime': 1.8879, 'eval_samples_per_second': 317.813, 'eval_steps_per_second': 39.727, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:04<05:08,  9.71it/s]\n",
      "100% 75/75 [00:01<00:00, 59.50it/s]\u001B[A\n",
      "{'train_runtime': 432.9191, 'train_samples_per_second': 110.875, 'train_steps_per_second': 13.859, 'train_loss': 0.22742520968119304, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:12<05:08,  9.71it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      " 50% 3000/6000 [07:13<07:13,  6.92it/s]\n",
      "100% 75/75 [00:01<00:00, 39.53it/s]\n",
      "f1 0.7172258580098921\n",
      "efl_f1 0.8843226031145828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5455    0.4286    0.4800        14\n",
      "           1     0.6912    0.8704    0.7705        54\n",
      "           2     0.9421    0.8636    0.9012       132\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.7263    0.7209    0.7172       200\n",
      "weighted avg     0.8466    0.8350    0.8364       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-af05cab7cac3ff26.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a65cf7b20c5fb4b7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-621d03c153854877.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-87ba67d32abdc71d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6623d15157329121.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a34ade94fbe2f1cd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-361929a39f50e928.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e78c033794c2779d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-61181f4db0061302.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-36e43347bb361904.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cb74597f2671c3ae.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fbfc6dc770e03175.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c82e93da6a3f6072.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0498c174f0272723.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0f65e730fb292f43.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f289dacb219b9ab4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4f7d488f4d405a88.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cd2c0abd2ae01a9e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a9766adadcabe42d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-07c4cfb93fb176fc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ff9ffa1318471eca.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-53599013f134f7ca.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3e50770607b3d2a8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1a5050c6ae9821b2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d26d5296010cce57.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cd9764ae45dfbd01.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-98faaa7b71dfa92d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-99a474b1c130db00.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4f5c13f39bc40c12.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22c2f3df7345b755.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8ca246364be9df3c.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0fe93c0abe733005.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-918dabde6a291e71.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-18626053e63f60a8.arrow\n",
      "\n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.6189, 'learning_rate': 9.9e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:35<12:13,  7.77it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.45it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 41.58it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 38.91it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 37.93it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 37.72it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 37.66it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 37.95it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 37.65it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 37.94it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 37.35it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 37.77it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 37.73it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 37.75it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 44.14it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5133423805236816, 'eval_f1': 0.73375, 'eval_runtime': 2.4035, 'eval_samples_per_second': 249.641, 'eval_steps_per_second': 31.205, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:37<12:13,  7.77it/s]\n",
      "100% 75/75 [00:02<00:00, 50.55it/s]\u001B[A\n",
      "{'loss': 0.4473, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:18<09:34,  9.40it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 66.58it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 54.95it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 53.80it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 56.00it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.15it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 57.94it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 49.23it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 47.46it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 45.78it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 45.77it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 46.39it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3541445732116699, 'eval_f1': 0.7986449348317053, 'eval_runtime': 2.1076, 'eval_samples_per_second': 284.685, 'eval_steps_per_second': 35.586, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:20<09:34,  9.40it/s]\n",
      "100% 75/75 [00:02<00:00, 45.64it/s]\u001B[A\n",
      "{'loss': 0.354, 'learning_rate': 8.95438596491228e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:01<09:26,  9.00it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 69.10it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 55.14it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 52.32it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 54.91it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.55it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 55.25it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 55.12it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 57.50it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 55.54it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 56.57it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.379428505897522, 'eval_f1': 0.8422630001577371, 'eval_runtime': 1.9381, 'eval_samples_per_second': 309.579, 'eval_steps_per_second': 38.697, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:03<09:26,  9.00it/s]\n",
      "100% 75/75 [00:01<00:00, 56.83it/s]\u001B[A\n",
      "{'loss': 0.2962, 'learning_rate': 8.42982456140351e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:45<10:33,  7.58it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 41.32it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 38.79it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 38.65it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 39.02it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 38.43it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 35.79it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 39.90it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 47.26it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 47.96it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 51.29it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 52.90it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 55.91it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5952461361885071, 'eval_f1': 0.8450705999420589, 'eval_runtime': 2.1549, 'eval_samples_per_second': 278.433, 'eval_steps_per_second': 34.804, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:47<10:33,  7.58it/s]\n",
      "100% 75/75 [00:02<00:00, 58.56it/s]\u001B[A\n",
      "{'loss': 0.2495, 'learning_rate': 7.903508771929826e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:28<07:49,  9.58it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 65.10it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 45.93it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 43.49it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 44.05it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 43.50it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 43.33it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 44.29it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 42.45it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 43.48it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 41.66it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 41.50it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 42.56it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 43.66it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5836120843887329, 'eval_f1': 0.8704992128383526, 'eval_runtime': 2.3129, 'eval_samples_per_second': 259.414, 'eval_steps_per_second': 32.427, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:30<07:49,  9.58it/s]\n",
      "100% 75/75 [00:02<00:00, 44.68it/s]\u001B[A\n",
      "{'loss': 0.1576, 'learning_rate': 7.37719298245614e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:11<07:26,  9.40it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.64it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 54.08it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 52.26it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 54.64it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 57.09it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 57.77it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 57.25it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 59.05it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 58.03it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 56.86it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7307658791542053, 'eval_f1': 0.8774509803921569, 'eval_runtime': 1.9198, 'eval_samples_per_second': 312.539, 'eval_steps_per_second': 39.067, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:13<07:26,  9.40it/s]\n",
      "100% 75/75 [00:01<00:00, 57.88it/s]\u001B[A\n",
      "{'loss': 0.1175, 'learning_rate': 6.854385964912281e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:55<08:29,  7.66it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 46.85it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 41.38it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 40.59it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 38.99it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 38.55it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:01, 37.86it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:01, 37.35it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 43.04it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 46.32it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 50.11it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 52.67it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 55.18it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7452208399772644, 'eval_f1': 0.8825520307132754, 'eval_runtime': 2.1993, 'eval_samples_per_second': 272.82, 'eval_steps_per_second': 34.103, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:57<08:29,  7.66it/s]\n",
      "100% 75/75 [00:02<00:00, 56.57it/s]\u001B[A\n",
      "{'loss': 0.0781, 'learning_rate': 6.3280701754385966e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:38<07:28,  8.03it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 68.34it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 47.21it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 44.62it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 44.17it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 43.16it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 44.23it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 43.12it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 42.96it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 43.49it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 43.75it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 43.16it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 43.65it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8420412540435791, 'eval_f1': 0.8743285200335125, 'eval_runtime': 2.3591, 'eval_samples_per_second': 254.339, 'eval_steps_per_second': 31.792, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:40<07:28,  8.03it/s]\n",
      "100% 75/75 [00:02<00:00, 43.74it/s]\u001B[A\n",
      "{'loss': 0.064, 'learning_rate': 5.801754385964913e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:21<05:45,  9.54it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.19it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 54.47it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 53.46it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 57.11it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 57.85it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.23it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 56.97it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 59.29it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 58.95it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 58.99it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8678957223892212, 'eval_f1': 0.8690629011553275, 'eval_runtime': 1.8814, 'eval_samples_per_second': 318.909, 'eval_steps_per_second': 39.864, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:23<05:45,  9.54it/s]\n",
      "100% 75/75 [00:01<00:00, 58.87it/s]\u001B[A\n",
      "{'loss': 0.059, 'learning_rate': 5.275438596491228e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:04<06:59,  7.16it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 44.66it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 37.90it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 35.30it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 36.39it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 36.66it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 37.42it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 37.99it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 38.03it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 44.18it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 47.63it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 51.92it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 53.09it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 53.69it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8930953145027161, 'eval_f1': 0.87625, 'eval_runtime': 2.2244, 'eval_samples_per_second': 269.731, 'eval_steps_per_second': 33.716, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:06<06:59,  7.16it/s]\n",
      "100% 75/75 [00:02<00:00, 54.37it/s]\u001B[A\n",
      "{'loss': 0.0479, 'learning_rate': 4.749122807017544e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:47<04:49,  9.33it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 68.64it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 54.59it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 53.84it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 55.82it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 47.78it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 47.18it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 44.97it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 45.72it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 47.08it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 46.79it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 47.10it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 47.58it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8805569410324097, 'eval_f1': 0.8854814939971651, 'eval_runtime': 2.1607, 'eval_samples_per_second': 277.694, 'eval_steps_per_second': 34.712, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:49<04:49,  9.33it/s]\n",
      "100% 75/75 [00:02<00:00, 47.36it/s]\u001B[A\n",
      "{'loss': 0.0326, 'learning_rate': 4.22280701754386e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:30<04:27,  8.97it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 68.06it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 55.07it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 54.74it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.51it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.27it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 58.04it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 56.53it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 56.44it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 56.37it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 56.79it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.855318009853363, 'eval_f1': 0.8869289914066034, 'eval_runtime': 1.9317, 'eval_samples_per_second': 310.609, 'eval_steps_per_second': 38.826, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:32<04:27,  8.97it/s]\n",
      "100% 75/75 [00:01<00:00, 57.48it/s]\u001B[A\n",
      "{'loss': 0.0261, 'learning_rate': 3.696491228070176e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [09:13<04:54,  7.13it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 49.45it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 39.48it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 39.92it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 39.06it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 39.20it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 38.96it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 39.03it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 37.80it/s]\u001B[A\n",
      " 55% 41/75 [00:01<00:00, 38.54it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 38.58it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 38.86it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 43.26it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 47.65it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 52.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.015966534614563, 'eval_f1': 0.8738985730462905, 'eval_runtime': 2.2982, 'eval_samples_per_second': 261.073, 'eval_steps_per_second': 32.634, 'epoch': 13.0}\n",
      " 65% 3900/6000 [09:16<04:54,  7.13it/s]\n",
      "100% 75/75 [00:02<00:00, 53.61it/s]\u001B[A\n",
      "{'loss': 0.0204, 'learning_rate': 3.170175438596492e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:56<03:12,  9.36it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 65.88it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 56.38it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 55.03it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.42it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.41it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 54.32it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 47.01it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 46.28it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 45.70it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 46.25it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 46.91it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0141757726669312, 'eval_f1': 0.8735749665854233, 'eval_runtime': 2.1416, 'eval_samples_per_second': 280.166, 'eval_steps_per_second': 35.021, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:59<03:12,  9.36it/s]\n",
      "100% 75/75 [00:02<00:00, 46.90it/s]\u001B[A\n",
      "{'loss': 0.0193, 'learning_rate': 2.6438596491228073e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:39<02:45,  9.05it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 68.57it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 57.43it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 55.63it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.51it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.84it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.02it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.93it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.47it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 60.76it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.090645670890808, 'eval_f1': 0.8708452041785375, 'eval_runtime': 1.8622, 'eval_samples_per_second': 322.205, 'eval_steps_per_second': 40.276, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:41<02:45,  9.05it/s]\n",
      "100% 75/75 [00:01<00:00, 59.00it/s]\u001B[A\n",
      "{'loss': 0.0082, 'learning_rate': 2.117543859649123e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [11:22<02:37,  7.62it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.99it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 41.21it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 37.49it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 38.03it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 37.61it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 36.28it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 36.92it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 36.09it/s]\u001B[A\n",
      " 52% 39/75 [00:01<00:00, 37.06it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 37.10it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 36.07it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 37.12it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 38.32it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 39.20it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 39.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.078620433807373, 'eval_f1': 0.8776627947464012, 'eval_runtime': 2.5799, 'eval_samples_per_second': 232.572, 'eval_steps_per_second': 29.071, 'epoch': 16.0}\n",
      " 80% 4800/6000 [11:25<02:37,  7.62it/s]\n",
      "100% 75/75 [00:02<00:00, 40.22it/s]\u001B[A\n",
      "{'loss': 0.0205, 'learning_rate': 1.592982456140351e-06, 'epoch': 17.0}\n",
      " 85% 5100/6000 [12:06<01:37,  9.22it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.88it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 54.48it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 51.59it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 54.43it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 56.49it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 56.24it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 55.41it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 57.29it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 55.79it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 56.26it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1323333978652954, 'eval_f1': 0.871521960927561, 'eval_runtime': 1.9543, 'eval_samples_per_second': 307.017, 'eval_steps_per_second': 38.377, 'epoch': 17.0}\n",
      " 85% 5100/6000 [12:08<01:37,  9.22it/s]\n",
      "100% 75/75 [00:01<00:00, 56.50it/s]\u001B[A\n",
      "{'train_runtime': 736.7772, 'train_samples_per_second': 65.149, 'train_steps_per_second': 8.144, 'train_loss': 0.1539373669904821, 'epoch': 17.0}\n",
      " 85% 5100/6000 [12:16<01:37,  9.22it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      " 85% 5100/6000 [12:17<02:10,  6.92it/s]\n",
      "100% 75/75 [00:01<00:00, 38.94it/s]\n",
      "f1 0.7666815648333158\n",
      "efl_f1 0.8869289914066034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.6250    0.6250        16\n",
      "           1     0.7500    0.7636    0.7568        55\n",
      "           2     0.9219    0.9147    0.9183       129\n",
      "\n",
      "    accuracy                         0.8500       200\n",
      "   macro avg     0.7656    0.7678    0.7667       200\n",
      "weighted avg     0.8509    0.8500    0.8504       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2c1446007ecba933.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0d4b73a5d91a5e38.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c92f47ec9e90095a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9951d9b051fc6832.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ebbb6d566a846a95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1c09cb4c8f539a2d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7981181b1ba2e5ce.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6f1d3b2a7622a4fc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5cbd4872d09a8006.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5dc41b1288f5f353.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-030073b44817630a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a4a1e5f9c20d9374.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-862f6cf180952435.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a06db00e57b04ff0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d009774e0b8dd278.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-15474a7780e3ff1c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ec90f0a2d07b0689.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-72cfe16916a12da3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-92e64144f6715077.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dd776867830dce30.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2531db1a03cc7757.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d1ceba43d6d10c61.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ce968c4d6eca175f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e63977f918673a95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6ad58e456ffce1fd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22309baa5758f0e8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b80a8bdf50df3bc3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-067920841d5e0b9d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-10b8be12b8491bae.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-12200c1316ae5bfc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b71949f3ef0762a5.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d8c75b1a79e0ddb8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fd1c68811a5d2b5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-01901a2d4880adea.arrow\n",
      "\n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.6178, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:35<10:06,  9.40it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.84it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.21it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.37it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 55.88it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 56.23it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 56.93it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 58.50it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 59.09it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 54.44it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 55.76it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.47718384861946106, 'eval_f1': 0.7455470737913485, 'eval_runtime': 1.9374, 'eval_samples_per_second': 309.691, 'eval_steps_per_second': 38.711, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:37<10:06,  9.40it/s]\n",
      "100% 75/75 [00:01<00:00, 52.07it/s]\u001B[A\n",
      "{'loss': 0.4551, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:17<11:30,  7.82it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 50.50it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 46.84it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 44.22it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 41.78it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 37.91it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 38.14it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 38.56it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 38.57it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 38.75it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 38.71it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 38.58it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 38.32it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 38.23it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 35.00it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 36.04it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 34.78it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.36700496077537537, 'eval_f1': 0.804803083491608, 'eval_runtime': 2.5985, 'eval_samples_per_second': 230.898, 'eval_steps_per_second': 28.862, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:20<11:30,  7.82it/s]\n",
      "100% 75/75 [00:02<00:00, 34.04it/s]\u001B[A\n",
      "{'loss': 0.3528, 'learning_rate': 8.95438596491228e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:01<08:45,  9.70it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.44it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.00it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.71it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 54.90it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 58.51it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 58.02it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 58.54it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 60.48it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 54.68it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.49it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.38663315773010254, 'eval_f1': 0.8828818500949649, 'eval_runtime': 1.9027, 'eval_samples_per_second': 315.338, 'eval_steps_per_second': 39.417, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:03<08:45,  9.70it/s]\n",
      "100% 75/75 [00:01<00:00, 54.40it/s]\u001B[A\n",
      "{'loss': 0.312, 'learning_rate': 8.42982456140351e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:44<08:14,  9.70it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.50it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.05it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.77it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 55.73it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 57.59it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 55.99it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 56.47it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 57.95it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 53.32it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 56.36it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.3947306275367737, 'eval_f1': 0.8787754331459763, 'eval_runtime': 1.9307, 'eval_samples_per_second': 310.772, 'eval_steps_per_second': 38.847, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:46<08:14,  9.70it/s]\n",
      "100% 75/75 [00:01<00:00, 52.65it/s]\u001B[A\n",
      "{'loss': 0.2425, 'learning_rate': 7.903508771929826e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:26<09:32,  7.86it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 52.00it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 47.89it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 46.65it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 46.31it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 42.12it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 42.31it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 42.74it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 41.80it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 41.64it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 41.01it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 40.58it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 38.43it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 38.24it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 38.37it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5412220358848572, 'eval_f1': 0.88375, 'eval_runtime': 2.4825, 'eval_samples_per_second': 241.694, 'eval_steps_per_second': 30.212, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:29<09:32,  7.86it/s]\n",
      "100% 75/75 [00:02<00:00, 35.05it/s]\u001B[A\n",
      "{'loss': 0.1725, 'learning_rate': 7.37719298245614e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:10<07:23,  9.47it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 60.79it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.35it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.85it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 56.25it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 58.71it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 57.88it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 57.50it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 57.73it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 57.90it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 53.73it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6104927659034729, 'eval_f1': 0.8655045557035299, 'eval_runtime': 1.9642, 'eval_samples_per_second': 305.462, 'eval_steps_per_second': 38.183, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:12<07:23,  9.47it/s]\n",
      "100% 75/75 [00:01<00:00, 51.97it/s]\u001B[A\n",
      "{'loss': 0.1248, 'learning_rate': 6.8526315789473685e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:53<08:29,  7.66it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.64it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.28it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 44.72it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 49.92it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 48.31it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 51.78it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 53.50it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 53.03it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 56.22it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 56.10it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 53.71it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6513077020645142, 'eval_f1': 0.88375, 'eval_runtime': 2.0618, 'eval_samples_per_second': 291.008, 'eval_steps_per_second': 36.376, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:55<08:29,  7.66it/s]\n",
      "100% 75/75 [00:02<00:00, 52.50it/s]\u001B[A\n",
      "{'loss': 0.1054, 'learning_rate': 6.326315789473685e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:36<06:20,  9.46it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 51.52it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 44.13it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 42.67it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 43.42it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 41.44it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 43.09it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 44.48it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 43.13it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 44.62it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 44.21it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 43.35it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 41.45it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 43.03it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6865454316139221, 'eval_f1': 0.8885918003565062, 'eval_runtime': 2.3863, 'eval_samples_per_second': 251.437, 'eval_steps_per_second': 31.43, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:38<06:20,  9.46it/s]\n",
      "100% 75/75 [00:02<00:00, 40.09it/s]\u001B[A\n",
      "{'loss': 0.0617, 'learning_rate': 5.8e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:19<06:00,  9.16it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 59.57it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 59.97it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.93it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 55.83it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.11it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 57.39it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 56.82it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 58.02it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 58.01it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 52.86it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6886258125305176, 'eval_f1': 0.8833559078957853, 'eval_runtime': 1.9476, 'eval_samples_per_second': 308.074, 'eval_steps_per_second': 38.509, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:21<06:00,  9.16it/s]\n",
      "100% 75/75 [00:01<00:00, 50.96it/s]\u001B[A\n",
      "{'loss': 0.0669, 'learning_rate': 5.273684210526317e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:02<06:31,  7.66it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 46.67it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.22it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 39.74it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 40.47it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 38.29it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 41.13it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 47.94it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 50.66it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 54.84it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 56.13it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 52.23it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 54.78it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6586235761642456, 'eval_f1': 0.8910047008206518, 'eval_runtime': 2.144, 'eval_samples_per_second': 279.851, 'eval_steps_per_second': 34.981, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:04<06:31,  7.66it/s]\n",
      "100% 75/75 [00:02<00:00, 51.29it/s]\u001B[A\n",
      "{'loss': 0.029, 'learning_rate': 4.747368421052632e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:45<04:42,  9.56it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.03it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.46it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 51.36it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 43.41it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 44.01it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 44.61it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 43.70it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 42.46it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 42.34it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 41.39it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 40.19it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 40.93it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7851353883743286, 'eval_f1': 0.8848966113688181, 'eval_runtime': 2.3689, 'eval_samples_per_second': 253.281, 'eval_steps_per_second': 31.66, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:47<04:42,  9.56it/s]\n",
      "100% 75/75 [00:02<00:00, 37.56it/s]\u001B[A\n",
      "{'loss': 0.0262, 'learning_rate': 4.221052631578948e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:28<04:06,  9.72it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.81it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.86it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.18it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 55.83it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.13it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 57.34it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 57.71it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 58.28it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 58.48it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 54.20it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8428463935852051, 'eval_f1': 0.8787633865427359, 'eval_runtime': 1.9612, 'eval_samples_per_second': 305.939, 'eval_steps_per_second': 38.242, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:30<04:06,  9.72it/s]\n",
      "100% 75/75 [00:01<00:00, 51.97it/s]\u001B[A\n",
      "{'loss': 0.0209, 'learning_rate': 3.696491228070176e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [09:11<04:34,  7.64it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 49.35it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 43.57it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 41.12it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 40.18it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 37.68it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 38.26it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 38.63it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 38.84it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 38.79it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 38.73it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 39.22it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 38.87it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 38.66it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 36.92it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 37.51it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 35.93it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8988454341888428, 'eval_f1': 0.8840384543448174, 'eval_runtime': 2.5538, 'eval_samples_per_second': 234.945, 'eval_steps_per_second': 29.368, 'epoch': 13.0}\n",
      " 65% 3900/6000 [09:14<04:34,  7.64it/s]\n",
      "100% 75/75 [00:02<00:00, 36.91it/s]\u001B[A\n",
      "{'loss': 0.0123, 'learning_rate': 3.170175438596492e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:55<03:20,  9.00it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.26it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.55it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.49it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.08it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.61it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 58.17it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.14it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.03it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 53.47it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 47.93it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9090480804443359, 'eval_f1': 0.8825520307132754, 'eval_runtime': 2.0402, 'eval_samples_per_second': 294.086, 'eval_steps_per_second': 36.761, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:57<03:20,  9.00it/s]\n",
      "100% 75/75 [00:02<00:00, 44.15it/s]\u001B[A\n",
      "{'loss': 0.0097, 'learning_rate': 2.6438596491228073e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:37<02:50,  8.80it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.07it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.90it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.08it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 56.95it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.44it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 58.11it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 58.43it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 60.68it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 56.01it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 57.82it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9963246583938599, 'eval_f1': 0.8787633865427359, 'eval_runtime': 1.8867, 'eval_samples_per_second': 318.022, 'eval_steps_per_second': 39.753, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:39<02:50,  8.80it/s]\n",
      "100% 75/75 [00:01<00:00, 54.45it/s]\u001B[A\n",
      "{'train_runtime': 648.3196, 'train_samples_per_second': 74.038, 'train_steps_per_second': 9.255, 'train_loss': 0.17397839334275988, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:48<02:50,  8.80it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      " 75% 4500/6000 [10:48<03:36,  6.93it/s]\n",
      "100% 75/75 [00:02<00:00, 34.00it/s]\n",
      "f1 0.6945970695970695\n",
      "efl_f1 0.8910047008206518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3333    0.3846    0.3571        13\n",
      "           1     0.8824    0.7377    0.8036        61\n",
      "           2     0.8955    0.9524    0.9231       126\n",
      "\n",
      "    accuracy                         0.8500       200\n",
      "   macro avg     0.7037    0.6916    0.6946       200\n",
      "weighted avg     0.8550    0.8500    0.8498       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.6134, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:35<12:09,  7.82it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.67it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 42.84it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 41.18it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 40.34it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 39.80it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 39.42it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 39.50it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 39.04it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 39.05it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 39.27it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 39.17it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 39.09it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 39.25it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 39.37it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 38.99it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 38.80it/s]\u001B[A\n",
      " 99% 74/75 [00:01<00:00, 38.74it/s]\u001B[A\n",
      "{'eval_loss': 0.5149555206298828, 'eval_f1': 0.7523532600660391, 'eval_runtime': 2.5126, 'eval_samples_per_second': 238.801, 'eval_steps_per_second': 29.85, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:37<12:09,  7.82it/s]\n",
      "{'loss': 0.4714, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:18<10:26,  8.61it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 69.43it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 59.15it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 58.34it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.60it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 59.34it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 54.53it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 54.37it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 54.32it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 54.96it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 57.57it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4382794201374054, 'eval_f1': 0.7725932788680199, 'eval_runtime': 1.9405, 'eval_samples_per_second': 309.2, 'eval_steps_per_second': 38.65, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:20<10:26,  8.61it/s]\n",
      "100% 75/75 [00:01<00:00, 58.33it/s]\u001B[A\n",
      "{'loss': 0.3647, 'learning_rate': 8.956140350877193e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:01<09:07,  9.31it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 74.48it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 60.08it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 56.81it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 58.91it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 56.75it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 54.03it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 52.83it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 54.76it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 55.69it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 57.69it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.536707878112793, 'eval_f1': 0.8106003967423269, 'eval_runtime': 1.9057, 'eval_samples_per_second': 314.847, 'eval_steps_per_second': 39.356, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:03<09:07,  9.31it/s]\n",
      "100% 75/75 [00:01<00:00, 58.07it/s]\u001B[A\n",
      "{'loss': 0.3181, 'learning_rate': 8.42982456140351e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:43<09:41,  8.26it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 56.26it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 48.07it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 45.30it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 44.89it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 43.02it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 42.79it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 43.24it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 40.90it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 41.26it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 40.61it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 40.28it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 38.80it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 38.84it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 38.56it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5937262177467346, 'eval_f1': 0.855650522317189, 'eval_runtime': 2.4709, 'eval_samples_per_second': 242.83, 'eval_steps_per_second': 30.354, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:46<09:41,  8.26it/s]\n",
      "100% 75/75 [00:02<00:00, 38.63it/s]\u001B[A\n",
      "{'loss': 0.2779, 'learning_rate': 7.903508771929826e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:27<08:03,  9.31it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 71.56it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 59.92it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 56.89it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 58.47it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 57.18it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 54.15it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 53.30it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 54.48it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 55.68it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 57.14it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5161466598510742, 'eval_f1': 0.8596329304990722, 'eval_runtime': 1.9072, 'eval_samples_per_second': 314.591, 'eval_steps_per_second': 39.324, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:28<08:03,  9.31it/s]\n",
      "100% 75/75 [00:01<00:00, 57.22it/s]\u001B[A\n",
      "{'loss': 0.1844, 'learning_rate': 7.380701754385966e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:10<08:17,  8.44it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 70.71it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 58.88it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 56.32it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 59.96it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 58.19it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 55.46it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 53.35it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 55.91it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.46it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.33it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.48323994874954224, 'eval_f1': 0.881280637254902, 'eval_runtime': 1.9403, 'eval_samples_per_second': 309.228, 'eval_steps_per_second': 38.653, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:12<08:17,  8.44it/s]\n",
      "100% 75/75 [00:01<00:00, 57.76it/s]\u001B[A\n",
      "{'loss': 0.1331, 'learning_rate': 6.854385964912281e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:52<07:47,  8.35it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 57.46it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 50.88it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 46.47it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 45.32it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 46.50it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 47.08it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 45.32it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 44.53it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 44.59it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 43.92it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 44.45it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 43.98it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 44.14it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5854148268699646, 'eval_f1': 0.8828818500949649, 'eval_runtime': 2.2739, 'eval_samples_per_second': 263.868, 'eval_steps_per_second': 32.984, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:54<07:47,  8.35it/s]\n",
      "100% 75/75 [00:02<00:00, 44.56it/s]\u001B[A\n",
      "{'loss': 0.0982, 'learning_rate': 6.3280701754385966e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:35<06:26,  9.31it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 69.54it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.42it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 58.75it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 58.82it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 61.70it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 55.20it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 55.08it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 56.11it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 57.07it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 59.21it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.562671422958374, 'eval_f1': 0.895, 'eval_runtime': 1.8804, 'eval_samples_per_second': 319.082, 'eval_steps_per_second': 39.885, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:37<06:26,  9.31it/s]\n",
      "100% 75/75 [00:01<00:00, 59.36it/s]\u001B[A\n",
      "{'loss': 0.0746, 'learning_rate': 5.801754385964913e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:18<07:18,  7.53it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.74it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 43.82it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 41.11it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 39.58it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 38.45it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:01, 37.82it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:01, 37.99it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 38.01it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 41.10it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 44.59it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 47.94it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 50.79it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 54.81it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7290579080581665, 'eval_f1': 0.8731280163192199, 'eval_runtime': 2.2445, 'eval_samples_per_second': 267.321, 'eval_steps_per_second': 33.415, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:20<07:18,  7.53it/s]\n",
      "100% 75/75 [00:02<00:00, 53.98it/s]\u001B[A\n",
      "{'loss': 0.0599, 'learning_rate': 5.275438596491228e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:01<06:21,  7.86it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 73.45it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 61.23it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 57.06it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 59.58it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 52.90it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 48.81it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 47.41it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 47.37it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 47.54it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 48.45it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 48.73it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7069509625434875, 'eval_f1': 0.8872166317873658, 'eval_runtime': 2.1273, 'eval_samples_per_second': 282.041, 'eval_steps_per_second': 35.255, 'epoch': 10.0}\n",
      " 50% 3000/6000 [07:03<06:21,  7.86it/s]\n",
      "100% 75/75 [00:02<00:00, 46.04it/s]\u001B[A\n",
      "{'loss': 0.0453, 'learning_rate': 4.749122807017544e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:44<04:41,  9.61it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.05it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.49it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 56.22it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 56.51it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 60.09it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 54.03it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 54.42it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 56.51it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 55.52it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 56.69it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.803726077079773, 'eval_f1': 0.8831599577868234, 'eval_runtime': 1.9574, 'eval_samples_per_second': 306.537, 'eval_steps_per_second': 38.317, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:46<04:41,  9.61it/s]\n",
      "100% 75/75 [00:01<00:00, 58.07it/s]\u001B[A\n",
      "{'loss': 0.0462, 'learning_rate': 4.22280701754386e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:27<05:20,  7.49it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.29it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.15it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 39.70it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 39.02it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 37.85it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:01, 38.51it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:01, 38.42it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 38.62it/s]\u001B[A\n",
      " 55% 41/75 [00:01<00:00, 38.25it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 38.46it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 38.34it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 37.06it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 36.52it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 36.95it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 36.19it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 36.89it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7003374099731445, 'eval_f1': 0.902254414215717, 'eval_runtime': 2.6397, 'eval_samples_per_second': 227.3, 'eval_steps_per_second': 28.412, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:29<05:20,  7.49it/s]\n",
      "100% 75/75 [00:02<00:00, 37.07it/s]\u001B[A\n",
      "{'loss': 0.0336, 'learning_rate': 3.696491228070176e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [09:10<03:38,  9.59it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 75.15it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 60.39it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 56.70it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 60.58it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 59.34it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 57.11it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 54.47it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 56.40it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 58.03it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7773895263671875, 'eval_f1': 0.885110294117647, 'eval_runtime': 1.9373, 'eval_samples_per_second': 309.706, 'eval_steps_per_second': 38.713, 'epoch': 13.0}\n",
      " 65% 3900/6000 [09:12<03:38,  9.59it/s]\n",
      "100% 75/75 [00:01<00:00, 57.51it/s]\u001B[A\n",
      "{'loss': 0.0299, 'learning_rate': 3.170175438596492e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:52<03:14,  9.23it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 74.93it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 59.84it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 57.41it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 60.31it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 58.84it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 56.01it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 53.57it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 54.97it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 55.72it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 56.65it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7693397998809814, 'eval_f1': 0.8929909042268592, 'eval_runtime': 1.9551, 'eval_samples_per_second': 306.896, 'eval_steps_per_second': 38.362, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:54<03:14,  9.23it/s]\n",
      "100% 75/75 [00:01<00:00, 57.69it/s]\u001B[A\n",
      "{'loss': 0.0243, 'learning_rate': 2.6438596491228073e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:35<03:07,  7.99it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 52.51it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 46.01it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 43.09it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 41.86it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 41.09it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 39.56it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 39.79it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 39.52it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 38.38it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 38.51it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 39.39it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 38.36it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 37.97it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 38.98it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8056389093399048, 'eval_f1': 0.8929909042268592, 'eval_runtime': 2.4869, 'eval_samples_per_second': 241.267, 'eval_steps_per_second': 30.158, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:38<03:07,  7.99it/s]\n",
      "100% 75/75 [00:02<00:00, 39.85it/s]\u001B[A\n",
      "{'loss': 0.0196, 'learning_rate': 2.117543859649123e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [11:18<02:04,  9.62it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 75.91it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 59.90it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 56.02it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 59.32it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 58.03it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 54.30it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 52.83it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 55.13it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 55.90it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 56.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8622369170188904, 'eval_f1': 0.8901293190543544, 'eval_runtime': 1.9458, 'eval_samples_per_second': 308.35, 'eval_steps_per_second': 38.544, 'epoch': 16.0}\n",
      " 80% 4800/6000 [11:20<02:04,  9.62it/s]\n",
      "100% 75/75 [00:01<00:00, 56.87it/s]\u001B[A\n",
      "{'loss': 0.0086, 'learning_rate': 1.5912280701754388e-06, 'epoch': 17.0}\n",
      " 85% 5100/6000 [12:01<01:46,  8.46it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.28it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.56it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 57.63it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 57.40it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 59.07it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 54.34it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 54.66it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 54.54it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 55.50it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 58.36it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9286378026008606, 'eval_f1': 0.8819257528820272, 'eval_runtime': 1.9069, 'eval_samples_per_second': 314.652, 'eval_steps_per_second': 39.331, 'epoch': 17.0}\n",
      " 85% 5100/6000 [12:03<01:46,  8.46it/s]\n",
      "100% 75/75 [00:01<00:00, 59.26it/s]\u001B[A\n",
      "{'train_runtime': 732.2443, 'train_samples_per_second': 65.552, 'train_steps_per_second': 8.194, 'train_loss': 0.16488857330060472, 'epoch': 17.0}\n",
      " 85% 5100/6000 [12:12<01:46,  8.46it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      " 85% 5100/6000 [12:12<02:09,  6.96it/s]\n",
      "100% 75/75 [00:02<00:00, 30.34it/s]\n",
      "f1 0.8070442806107355\n",
      "efl_f1 0.902254414215717\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7083    0.7083    0.7083        24\n",
      "           1     0.7708    0.7872    0.7789        47\n",
      "           2     0.9375    0.9302    0.9339       129\n",
      "\n",
      "    accuracy                         0.8700       200\n",
      "   macro avg     0.8056    0.8086    0.8070       200\n",
      "weighted avg     0.8708    0.8700    0.8704       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7594887291398417\n",
      "efl_f1 0.8927423214480253\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 26 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-80\n"
     ]
    }
   ],
   "source": [
    "!python efl.py --base-model=roberta-base --cross-validation=5 --config-name=efl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQzkJpWoR825",
    "outputId": "c8e7e63c-4369-497c-b33a-be46e32744c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-15 23:09:23.219070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 23:09:24.232621: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 23:09:24.232747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-15 23:09:24.232767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 4.03kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 86.4kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 1.23MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 1.86MB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 40 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 40 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 434.82it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a7d6048ffa37a8c2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-031c3931682d9ad4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a9a96ec33cd20cff.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-575c16c13f11d43a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1c3c0bfde0e49168.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-21141121ca7040a9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4f34d35c3993c230.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eb5397f716e093bd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-403ec24ed809ad67.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1c89d0bc3b5dda29.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7c72d992e9575749.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e181d17c1770f046.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2cfa008ef11960f7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-95a47a36b33af419.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6c44bfbcd81baca4.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fc1a03893284d29d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-21300b452955922c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c2d18e12a79bf379.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-07fe49a0fa4e2cae.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9549c37aaac0b483.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d9f06bd20c5c1eec.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d1fcc1039f1e48f1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1e936f820960d8b5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6f29fa23078781f3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f50ac7845eec1a7a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-57959795035ba72b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-09a8b720263d1b15.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-680fdf56336dc414.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eb51fa6cb4cb4c56.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-97069ca6371e9593.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fc5dec89386fee7d.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-852893d8f4a71e99.arrow\n",
      "                                                   \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "Downloading pytorch_model.bin: 100% 440M/440M [00:02<00:00, 214MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1276: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/6000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.6019, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:32<11:44,  8.09it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 59.69it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 59.59it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:00, 59.04it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 58.47it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 58.52it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 58.80it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 51.65it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 53.71it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 56.34it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.72it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 50.20it/s]\u001B[A\n",
      "100% 75/75 [00:01<00:00, 46.73it/s]\u001B[A\n",
      "{'eval_loss': 0.45725545287132263, 'eval_f1': 0.742103549332465, 'eval_runtime': 2.0234, 'eval_samples_per_second': 296.526, 'eval_steps_per_second': 37.066, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:34<11:44,  8.09it/s]\n",
      "{'loss': 0.3862, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:12<11:53,  7.57it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 44.30it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 40.24it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 38.37it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 38.00it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 37.93it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 37.73it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 37.09it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 37.03it/s]\u001B[A\n",
      " 52% 39/75 [00:01<00:00, 37.52it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 33.94it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 35.34it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 36.30it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 36.47it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 36.90it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 37.19it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 37.62it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 33.01it/s]\u001B[A\n",
      "100% 75/75 [00:02<00:00, 30.87it/s]\u001B[A\n",
      "{'eval_loss': 0.3345811069011688, 'eval_f1': 0.7767825806179893, 'eval_runtime': 2.7435, 'eval_samples_per_second': 218.702, 'eval_steps_per_second': 27.338, 'epoch': 2.0}\n",
      "\n",
      " 10% 600/6000 [01:14<11:53,  7.57it/s]\n",
      "{'loss': 0.2587, 'learning_rate': 8.950877192982457e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:52<08:53,  9.56it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.09it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.98it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.46it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 62.09it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.69it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 52.25it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 55.47it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 55.66it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 56.66it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.80it/s]\u001B[A\n",
      " 99% 74/75 [00:01<00:00, 45.96it/s]\u001B[A\n",
      "{'eval_loss': 0.3454705774784088, 'eval_f1': 0.8714185883997205, 'eval_runtime': 1.9972, 'eval_samples_per_second': 300.415, 'eval_steps_per_second': 37.552, 'epoch': 3.0}\n",
      "\n",
      " 15% 900/6000 [01:54<08:53,  9.56it/s]\n",
      "{'loss': 0.1848, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:31<08:55,  8.97it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 44.11it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 44.13it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 44.21it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 44.09it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 41.52it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 40.86it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 40.54it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 40.64it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 36.01it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 36.66it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 37.28it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 37.64it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 38.69it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 39.32it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 35.52it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4536885619163513, 'eval_f1': 0.8880876583448987, 'eval_runtime': 2.6428, 'eval_samples_per_second': 227.036, 'eval_steps_per_second': 28.379, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:34<08:55,  8.97it/s]\n",
      "100% 75/75 [00:02<00:00, 33.49it/s]\u001B[A\n",
      "{'loss': 0.111, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:11<07:21, 10.20it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.93it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.58it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.37it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.73it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.88it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 52.50it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 55.04it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 57.26it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 57.58it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 58.18it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5272349119186401, 'eval_f1': 0.8825995807127882, 'eval_runtime': 1.978, 'eval_samples_per_second': 303.335, 'eval_steps_per_second': 37.917, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:13<07:21, 10.20it/s]\n",
      "100% 75/75 [00:01<00:00, 45.50it/s]\u001B[A\n",
      "{'loss': 0.0631, 'learning_rate': 7.373684210526316e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:50<07:14,  9.66it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 56.62it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 49.68it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 48.48it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 47.05it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 46.66it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 45.59it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 45.17it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 40.10it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 41.57it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 43.38it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 44.30it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 45.04it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 44.87it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6063130497932434, 'eval_f1': 0.878576921520217, 'eval_runtime': 2.3786, 'eval_samples_per_second': 252.246, 'eval_steps_per_second': 31.531, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:52<07:14,  9.66it/s]\n",
      "100% 75/75 [00:02<00:00, 36.21it/s]\u001B[A\n",
      "{'loss': 0.031, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:30<06:27, 10.05it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 60.18it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.28it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 59.47it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.42it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.26it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 52.62it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 56.40it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 57.86it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 58.25it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 58.28it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6099791526794434, 'eval_f1': 0.8924443745577482, 'eval_runtime': 2.0179, 'eval_samples_per_second': 297.343, 'eval_steps_per_second': 37.168, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:32<06:27, 10.05it/s]\n",
      "100% 75/75 [00:01<00:00, 46.12it/s]\u001B[A\n",
      "{'loss': 0.0224, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:09<05:38, 10.64it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.83it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.74it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.36it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 62.03it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.55it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 53.18it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 55.47it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 57.22it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.92it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 51.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7485020756721497, 'eval_f1': 0.8906980250263833, 'eval_runtime': 2.0237, 'eval_samples_per_second': 296.482, 'eval_steps_per_second': 37.06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:11<05:38, 10.64it/s]\n",
      "100% 75/75 [00:02<00:00, 47.80it/s]\u001B[A\n",
      "{'loss': 0.0123, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:48<07:02,  7.82it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.96it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.86it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 40.05it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 40.30it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 43.03it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 49.16it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 54.12it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 48.38it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 52.59it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 55.11it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 57.03it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7247101664543152, 'eval_f1': 0.8880876583448987, 'eval_runtime': 2.1508, 'eval_samples_per_second': 278.968, 'eval_steps_per_second': 34.871, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:50<07:02,  7.82it/s]\n",
      "100% 75/75 [00:02<00:00, 50.92it/s]\u001B[A\n",
      "{'loss': 0.0075, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:27<04:55, 10.14it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.26it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.71it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.50it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.32it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.12it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 53.28it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 54.28it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 56.90it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 57.67it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 51.94it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7528657913208008, 'eval_f1': 0.886340674883815, 'eval_runtime': 1.9716, 'eval_samples_per_second': 304.321, 'eval_steps_per_second': 38.04, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:29<04:55, 10.14it/s]\n",
      "100% 75/75 [00:01<00:00, 48.67it/s]\u001B[A\n",
      "{'loss': 0.0083, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:06<05:39,  7.95it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 47.80it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.74it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 40.56it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 40.66it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 39.80it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:01, 38.77it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:01, 37.35it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:01, 37.57it/s]\u001B[A\n",
      " 55% 41/75 [00:01<00:01, 33.69it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 34.93it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 35.74it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 36.90it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 37.54it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 37.58it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 38.17it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 33.82it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8136325478553772, 'eval_f1': 0.883457186180278, 'eval_runtime': 2.6823, 'eval_samples_per_second': 223.687, 'eval_steps_per_second': 27.961, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:09<05:39,  7.95it/s]\n",
      "100% 75/75 [00:02<00:00, 31.75it/s]\u001B[A\n",
      "{'loss': 0.0022, 'learning_rate': 4.217543859649123e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:46<03:53, 10.28it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.25it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.03it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.38it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.23it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.63it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 52.13it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 53.54it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 56.47it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 57.32it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 57.82it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8221664428710938, 'eval_f1': 0.883457186180278, 'eval_runtime': 1.9888, 'eval_samples_per_second': 301.687, 'eval_steps_per_second': 37.711, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:48<03:53, 10.28it/s]\n",
      "100% 75/75 [00:01<00:00, 51.15it/s]\u001B[A\n",
      "{'train_runtime': 476.6156, 'train_samples_per_second': 100.71, 'train_steps_per_second': 12.589, 'train_loss': 0.1407813173201349, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:56<03:53, 10.28it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      " 60% 3600/6000 [07:57<05:18,  7.55it/s]\n",
      "100% 75/75 [00:02<00:00, 33.74it/s]\n",
      "f1 0.7638567138567139\n",
      "efl_f1 0.8924443745577482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.5000    0.5714        12\n",
      "           1     0.8596    0.7778    0.8167        63\n",
      "           2     0.8731    0.9360    0.9035       125\n",
      "\n",
      "    accuracy                         0.8600       200\n",
      "   macro avg     0.7998    0.7379    0.7639       200\n",
      "weighted avg     0.8565    0.8600    0.8562       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-66e12c250a32db91.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ddf3052e61361864.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7013e08c7a517995.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-811f2d5250f2dccf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ebc1890012864818.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bd1bae00f3f83c9e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-589590c69bf9bbd9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fedf89db858f2461.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-49242d902b59ca53.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e302f3721e77c7e2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5b4c59ace3ca429f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f1dd50fe9affdc27.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f118c1f7973fc628.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a61c5df896c1c4df.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c937a37ae4f5a751.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3e9a9c0443a30b22.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-81c276ae543abd22.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c980cbe4f5b1edfc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ee7f40c1f6e31cac.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-84039628c04288a4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-41a467ecc8901634.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2d162ef3e11c6d44.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2bd310141fcbe19.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ae32580a2cab2b8f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-86cb8c45beaecffa.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-95c7661cdf76de72.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63ff2bd7cd0f56e5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f07a0082966c0acd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-199cd9544b8bd57f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cd338f747d6f3e95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-912ae878320c1650.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-742e569161d13bef.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5538, 'learning_rate': 9.9e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:32<11:35,  8.19it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 41.78it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 40.30it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 39.14it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 39.13it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 39.20it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 39.48it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 39.43it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 39.34it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 39.51it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 39.75it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 39.49it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 39.45it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 39.59it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 39.87it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 40.08it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4489235579967499, 'eval_f1': 0.7698737475394559, 'eval_runtime': 2.4951, 'eval_samples_per_second': 240.47, 'eval_steps_per_second': 30.059, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:35<11:35,  8.19it/s]\n",
      "100% 75/75 [00:02<00:00, 39.85it/s]\u001B[A\n",
      "{'loss': 0.4108, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:13<09:00,  9.99it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.07it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.74it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 57.87it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.52it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.24it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.52it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 59.20it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 60.59it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 59.10it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.25it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.34389162063598633, 'eval_f1': 0.8343761501656237, 'eval_runtime': 1.8636, 'eval_samples_per_second': 321.951, 'eval_steps_per_second': 40.244, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:14<09:00,  9.99it/s]\n",
      "100% 75/75 [00:01<00:00, 57.85it/s]\u001B[A\n",
      "{'loss': 0.2817, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:52<10:23,  8.17it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.75it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 41.47it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 40.30it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 39.31it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 38.93it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 38.99it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 37.32it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 37.36it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 37.77it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 37.12it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 37.80it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 38.38it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 38.43it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 38.15it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 37.99it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 36.62it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4360368251800537, 'eval_f1': 0.8434138330511388, 'eval_runtime': 2.5793, 'eval_samples_per_second': 232.619, 'eval_steps_per_second': 29.077, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:55<10:23,  8.17it/s]\n",
      "100% 75/75 [00:02<00:00, 37.13it/s]\u001B[A\n",
      "{'loss': 0.2096, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:33<08:14,  9.71it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.53it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 59.66it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 58.82it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 58.30it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 57.76it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.05it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 56.80it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 57.64it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 57.53it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 58.62it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4296536147594452, 'eval_f1': 0.8725, 'eval_runtime': 1.8724, 'eval_samples_per_second': 320.452, 'eval_steps_per_second': 40.056, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:34<08:14,  9.71it/s]\n",
      "100% 75/75 [00:01<00:00, 58.05it/s]\u001B[A\n",
      "{'loss': 0.1394, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:12<08:31,  8.79it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.23it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 46.23it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 45.33it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 43.94it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 42.03it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 39.97it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 39.80it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 39.86it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 38.84it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 38.83it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 39.16it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 38.29it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 38.95it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 39.14it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 38.52it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5108416676521301, 'eval_f1': 0.8831599577868234, 'eval_runtime': 2.4974, 'eval_samples_per_second': 240.249, 'eval_steps_per_second': 30.031, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:15<08:31,  8.79it/s]\n",
      "100% 75/75 [00:02<00:00, 38.87it/s]\u001B[A\n",
      "{'loss': 0.0869, 'learning_rate': 7.373684210526316e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:52<06:56, 10.10it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.76it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.80it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 59.83it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.59it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.20it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.13it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 59.96it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 58.00it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 58.16it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.95it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6217039227485657, 'eval_f1': 0.8646138248449087, 'eval_runtime': 1.8504, 'eval_samples_per_second': 324.262, 'eval_steps_per_second': 40.533, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:54<06:56, 10.10it/s]\n",
      "100% 75/75 [00:01<00:00, 59.09it/s]\u001B[A\n",
      "{'loss': 0.0547, 'learning_rate': 6.8473684210526325e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:32<08:43,  7.45it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 47.25it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 44.35it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 43.20it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 42.31it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 43.16it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 43.73it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 43.32it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 44.30it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 42.58it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 43.12it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 41.93it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 40.30it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 39.54it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 39.48it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6066966652870178, 'eval_f1': 0.8854814939971651, 'eval_runtime': 2.4213, 'eval_samples_per_second': 247.796, 'eval_steps_per_second': 30.975, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:34<08:43,  7.45it/s]\n",
      "100% 75/75 [00:02<00:00, 39.54it/s]\u001B[A\n",
      "{'loss': 0.0261, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:12<05:48, 10.34it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.80it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 56.74it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 53.90it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 55.69it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 57.69it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.03it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 58.39it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 58.95it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 57.76it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 59.49it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6632959246635437, 'eval_f1': 0.8748408267977621, 'eval_runtime': 1.9194, 'eval_samples_per_second': 312.604, 'eval_steps_per_second': 39.075, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:14<05:48, 10.34it/s]\n",
      "100% 75/75 [00:01<00:00, 58.51it/s]\u001B[A\n",
      "{'loss': 0.0175, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:51<05:16, 10.42it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 53.27it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 47.14it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 45.88it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 46.97it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 47.58it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 45.89it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 46.08it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 45.47it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 44.09it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 43.03it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 44.39it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 45.77it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 45.70it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6947693228721619, 'eval_f1': 0.8731280163192199, 'eval_runtime': 2.2607, 'eval_samples_per_second': 265.399, 'eval_steps_per_second': 33.175, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:53<05:16, 10.42it/s]\n",
      "100% 75/75 [00:02<00:00, 46.07it/s]\u001B[A\n",
      "{'loss': 0.0107, 'learning_rate': 5.271929824561403e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:31<04:53, 10.21it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.28it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.49it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 58.85it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.90it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.27it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.75it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.75it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 58.54it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 58.53it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.71it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7061262726783752, 'eval_f1': 0.8739672314801848, 'eval_runtime': 1.8585, 'eval_samples_per_second': 322.833, 'eval_steps_per_second': 40.354, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:33<04:53, 10.21it/s]\n",
      "100% 75/75 [00:01<00:00, 58.11it/s]\u001B[A\n",
      "{'loss': 0.0142, 'learning_rate': 4.74561403508772e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:11<05:08,  8.75it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.22it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.75it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.10it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 58.47it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 58.15it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 59.97it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 57.62it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 55.51it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 50.80it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 49.15it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 48.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8178883790969849, 'eval_f1': 0.8639644292174176, 'eval_runtime': 2.035, 'eval_samples_per_second': 294.836, 'eval_steps_per_second': 36.855, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:13<05:08,  8.75it/s]\n",
      "100% 75/75 [00:02<00:00, 46.82it/s]\u001B[A\n",
      "{'loss': 0.0058, 'learning_rate': 4.219298245614035e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:51<04:08,  9.64it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.33it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.40it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 59.45it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.09it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.90it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.78it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 58.92it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 59.80it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 60.39it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 59.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7475452423095703, 'eval_f1': 0.8817267888823181, 'eval_runtime': 1.8572, 'eval_samples_per_second': 323.069, 'eval_steps_per_second': 40.384, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:52<04:08,  9.64it/s]\n",
      "100% 75/75 [00:01<00:00, 59.69it/s]\u001B[A\n",
      "{'train_runtime': 480.3168, 'train_samples_per_second': 99.934, 'train_steps_per_second': 12.492, 'train_loss': 0.15094027946392696, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:00<04:08,  9.64it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 18 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 18 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      " 60% 3600/6000 [08:00<05:20,  7.49it/s]\n",
      "100% 75/75 [00:02<00:00, 33.27it/s]\n",
      "f1 0.7311572124656237\n",
      "efl_f1 0.8854814939971651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.3571    0.5000        14\n",
      "           1     0.7925    0.7778    0.7850        54\n",
      "           2     0.8794    0.9394    0.9084       132\n",
      "\n",
      "    accuracy                         0.8550       200\n",
      "   macro avg     0.8351    0.6914    0.7312       200\n",
      "weighted avg     0.8527    0.8550    0.8465       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-af05cab7cac3ff26.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a65cf7b20c5fb4b7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-621d03c153854877.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-87ba67d32abdc71d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6623d15157329121.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a34ade94fbe2f1cd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-361929a39f50e928.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e78c033794c2779d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-61181f4db0061302.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-36e43347bb361904.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cb74597f2671c3ae.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fbfc6dc770e03175.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c82e93da6a3f6072.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0498c174f0272723.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0f65e730fb292f43.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f289dacb219b9ab4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4f7d488f4d405a88.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cd2c0abd2ae01a9e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a9766adadcabe42d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-07c4cfb93fb176fc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ff9ffa1318471eca.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-53599013f134f7ca.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3e50770607b3d2a8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1a5050c6ae9821b2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d26d5296010cce57.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cd9764ae45dfbd01.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-98faaa7b71dfa92d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-99a474b1c130db00.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4f5c13f39bc40c12.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22c2f3df7345b755.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8ca246364be9df3c.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0fe93c0abe733005.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5582, 'learning_rate': 9.9e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:32<09:03, 10.50it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 65.70it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 53.03it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 51.99it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 54.38it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 56.87it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 56.74it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 54.73it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 52.83it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 53.68it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 53.76it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 54.82it/s]\u001B[A\n",
      "{'eval_loss': 0.4701597988605499, 'eval_f1': 0.7653333333333333, 'eval_runtime': 1.968, 'eval_samples_per_second': 304.877, 'eval_steps_per_second': 38.11, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:34<09:03, 10.50it/s]\n",
      "{'loss': 0.3896, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:11<11:07,  8.09it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.74it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 41.27it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 39.83it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 38.59it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 37.81it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:01, 38.85it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 38.20it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 38.34it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 39.98it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 44.20it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 48.94it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 51.91it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 54.64it/s]\u001B[A\n",
      "100% 75/75 [00:01<00:00, 55.53it/s]\u001B[A\n",
      "{'eval_loss': 0.34847044944763184, 'eval_f1': 0.8374631503465859, 'eval_runtime': 2.2316, 'eval_samples_per_second': 268.863, 'eval_steps_per_second': 33.608, 'epoch': 2.0}\n",
      "\n",
      " 10% 600/6000 [01:13<11:07,  8.09it/s]\n",
      "{'loss': 0.2842, 'learning_rate': 8.95438596491228e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:54<08:38,  9.84it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 65.56it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 53.80it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 53.36it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 55.55it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 55.96it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 56.55it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 54.30it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 48.97it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 46.74it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 46.51it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 47.04it/s]\u001B[A\n",
      " 96% 72/75 [00:01<00:00, 47.21it/s]\u001B[A\n",
      "{'eval_loss': 0.4006058871746063, 'eval_f1': 0.8649878195532857, 'eval_runtime': 2.1217, 'eval_samples_per_second': 282.799, 'eval_steps_per_second': 35.35, 'epoch': 3.0}\n",
      "\n",
      " 15% 900/6000 [01:56<08:38,  9.84it/s]\n",
      "{'loss': 0.2182, 'learning_rate': 8.428070175438597e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:34<08:27,  9.47it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 66.36it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 53.79it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 51.98it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 52.91it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 54.78it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 56.07it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 55.08it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 56.20it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 55.78it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.95it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 57.16it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.499197393655777, 'eval_f1': 0.8622533218020285, 'eval_runtime': 1.9192, 'eval_samples_per_second': 312.625, 'eval_steps_per_second': 39.078, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:36<08:27,  9.47it/s]\n",
      "100% 75/75 [00:01<00:00, 57.62it/s]\u001B[A\n",
      "{'loss': 0.1589, 'learning_rate': 7.901754385964913e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:13<07:15, 10.34it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 64.81it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 52.49it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 50.16it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 51.31it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 53.31it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 54.37it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 53.06it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 56.79it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 55.15it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 55.70it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5336073040962219, 'eval_f1': 0.8614899905384007, 'eval_runtime': 1.9833, 'eval_samples_per_second': 302.519, 'eval_steps_per_second': 37.815, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:15<07:15, 10.34it/s]\n",
      "100% 75/75 [00:01<00:00, 56.10it/s]\u001B[A\n",
      "{'loss': 0.0982, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:52<08:57,  7.81it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.66it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 41.54it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 38.98it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 38.07it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 37.54it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 37.01it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 37.70it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 38.20it/s]\u001B[A\n",
      " 52% 39/75 [00:01<00:00, 38.05it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 40.99it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 48.67it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 50.62it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 52.58it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5681453347206116, 'eval_f1': 0.8690756742602777, 'eval_runtime': 2.2451, 'eval_samples_per_second': 267.249, 'eval_steps_per_second': 33.406, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:54<08:57,  7.81it/s]\n",
      "100% 75/75 [00:02<00:00, 55.39it/s]\u001B[A\n",
      "{'loss': 0.053, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:31<06:14, 10.42it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 66.04it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 53.42it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 53.52it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 56.54it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 58.95it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 59.25it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 57.14it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 60.32it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 58.85it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 58.14it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5566501617431641, 'eval_f1': 0.8904157902029197, 'eval_runtime': 1.9091, 'eval_samples_per_second': 314.281, 'eval_steps_per_second': 39.285, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:33<06:14, 10.42it/s]\n",
      "100% 75/75 [00:01<00:00, 58.13it/s]\u001B[A\n",
      "{'loss': 0.0439, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:11<06:38,  9.04it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 67.39it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 54.48it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 53.99it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 55.78it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 59.31it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 59.11it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 58.16it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 59.78it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 58.11it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 59.31it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6137465238571167, 'eval_f1': 0.8773488481798883, 'eval_runtime': 1.9263, 'eval_samples_per_second': 311.476, 'eval_steps_per_second': 38.934, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:13<06:38,  9.04it/s]\n",
      "100% 75/75 [00:01<00:00, 59.26it/s]\u001B[A\n",
      "{'loss': 0.031, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:49<05:19, 10.34it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 66.36it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 54.12it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 52.86it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 56.26it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.00it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 57.23it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 59.85it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 58.84it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 59.71it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6048980951309204, 'eval_f1': 0.883457186180278, 'eval_runtime': 1.9821, 'eval_samples_per_second': 302.703, 'eval_steps_per_second': 37.838, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:51<05:19, 10.34it/s]\n",
      "100% 75/75 [00:01<00:00, 53.99it/s]\u001B[A\n",
      "{'loss': 0.0262, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:28<05:11,  9.63it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 67.07it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 54.09it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 54.05it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 55.29it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 58.82it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 59.96it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 58.72it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 61.00it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 59.64it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 59.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6360110640525818, 'eval_f1': 0.8817267888823181, 'eval_runtime': 1.8897, 'eval_samples_per_second': 317.513, 'eval_steps_per_second': 39.689, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:30<05:11,  9.63it/s]\n",
      "100% 75/75 [00:01<00:00, 58.30it/s]\u001B[A\n",
      "{'loss': 0.0149, 'learning_rate': 4.74561403508772e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:07<04:52,  9.22it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 56.41it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 46.70it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 47.19it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 44.61it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 43.55it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 43.37it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 43.81it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 42.33it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 41.93it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 41.93it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 41.96it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 41.31it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 39.64it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 39.63it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6944510340690613, 'eval_f1': 0.878576921520217, 'eval_runtime': 2.3924, 'eval_samples_per_second': 250.799, 'eval_steps_per_second': 31.35, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:09<04:52,  9.22it/s]\n",
      "100% 75/75 [00:02<00:00, 39.69it/s]\u001B[A\n",
      "{'loss': 0.0145, 'learning_rate': 4.219298245614035e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:46<03:57, 10.10it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 66.80it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 53.98it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 54.02it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 54.86it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.61it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 58.30it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 57.21it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 57.85it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 57.30it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 57.49it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7242323756217957, 'eval_f1': 0.878576921520217, 'eval_runtime': 1.9083, 'eval_samples_per_second': 314.413, 'eval_steps_per_second': 39.302, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:48<03:57, 10.10it/s]\n",
      "100% 75/75 [00:01<00:00, 57.26it/s]\u001B[A\n",
      "{'train_runtime': 475.9918, 'train_samples_per_second': 100.842, 'train_steps_per_second': 12.605, 'train_loss': 0.1575715630584293, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:55<03:57, 10.10it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      " 60% 3600/6000 [07:56<05:17,  7.55it/s]\n",
      "100% 75/75 [00:01<00:00, 39.67it/s]\n",
      "f1 0.7177933177933178\n",
      "efl_f1 0.8904157902029197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.4375    0.4667        16\n",
      "           1     0.7636    0.7636    0.7636        55\n",
      "           2     0.9160    0.9302    0.9231       129\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.7266    0.7105    0.7178       200\n",
      "weighted avg     0.8408    0.8450    0.8427       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2c1446007ecba933.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0d4b73a5d91a5e38.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c92f47ec9e90095a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9951d9b051fc6832.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ebbb6d566a846a95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1c09cb4c8f539a2d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7981181b1ba2e5ce.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6f1d3b2a7622a4fc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5cbd4872d09a8006.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5dc41b1288f5f353.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-030073b44817630a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a4a1e5f9c20d9374.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-862f6cf180952435.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a06db00e57b04ff0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d009774e0b8dd278.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-15474a7780e3ff1c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ec90f0a2d07b0689.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-72cfe16916a12da3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-92e64144f6715077.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dd776867830dce30.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2531db1a03cc7757.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d1ceba43d6d10c61.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ce968c4d6eca175f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e63977f918673a95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6ad58e456ffce1fd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22309baa5758f0e8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b80a8bdf50df3bc3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-067920841d5e0b9d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-10b8be12b8491bae.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-12200c1316ae5bfc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b71949f3ef0762a5.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d8c75b1a79e0ddb8.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5558, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:32<11:09,  8.51it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.67it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 64.11it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.25it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.20it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 63.41it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.43it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.73it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 61.28it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 59.22it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.46944427490234375, 'eval_f1': 0.7641231277273264, 'eval_runtime': 1.8227, 'eval_samples_per_second': 329.174, 'eval_steps_per_second': 41.147, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:34<11:09,  8.51it/s]\n",
      "100% 75/75 [00:01<00:00, 61.93it/s]\u001B[A\n",
      "{'loss': 0.4059, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:11<09:10,  9.82it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.85it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 64.15it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.64it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.42it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.81it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.29it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 62.31it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 62.44it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 60.13it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3204617202281952, 'eval_f1': 0.8526281316522024, 'eval_runtime': 1.8582, 'eval_samples_per_second': 322.897, 'eval_steps_per_second': 40.362, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:13<09:10,  9.82it/s]\n",
      "100% 75/75 [00:01<00:00, 62.50it/s]\u001B[A\n",
      "{'loss': 0.2878, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:50<08:08, 10.44it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.68it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.64it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.09it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.21it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.89it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.41it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.29it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.96it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 59.91it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3539443910121918, 'eval_f1': 0.8708056009949897, 'eval_runtime': 1.8645, 'eval_samples_per_second': 321.804, 'eval_steps_per_second': 40.225, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:52<08:08, 10.44it/s]\n",
      "100% 75/75 [00:01<00:00, 60.31it/s]\u001B[A\n",
      "{'loss': 0.209, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:28<07:58, 10.03it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 57.17it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 49.55it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 49.52it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 47.81it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 46.72it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 48.25it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 47.89it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 47.99it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 47.52it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 45.45it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 45.67it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 44.28it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 43.49it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.43636998534202576, 'eval_f1': 0.8774509803921569, 'eval_runtime': 2.2812, 'eval_samples_per_second': 263.014, 'eval_steps_per_second': 32.877, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:31<07:58, 10.03it/s]\n",
      "100% 75/75 [00:02<00:00, 41.86it/s]\u001B[A\n",
      "{'loss': 0.1492, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:08<07:30,  9.99it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.08it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.21it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 65.56it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.32it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 63.61it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 61.92it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 62.07it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 62.03it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 59.27it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.46634891629219055, 'eval_f1': 0.8701343513461799, 'eval_runtime': 1.8156, 'eval_samples_per_second': 330.469, 'eval_steps_per_second': 41.309, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:10<07:30,  9.99it/s]\n",
      "100% 75/75 [00:01<00:00, 61.74it/s]\u001B[A\n",
      "{'loss': 0.0802, 'learning_rate': 7.373684210526316e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:47<08:21,  8.38it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 50.14it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 44.72it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 43.30it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 42.46it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 39.12it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 39.35it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 39.32it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 39.77it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 39.73it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 40.01it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 40.63it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 40.53it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 39.10it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 39.66it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5800658464431763, 'eval_f1': 0.8779720837674712, 'eval_runtime': 2.4815, 'eval_samples_per_second': 241.786, 'eval_steps_per_second': 30.223, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:49<08:21,  8.38it/s]\n",
      "100% 75/75 [00:02<00:00, 39.24it/s]\u001B[A\n",
      "{'loss': 0.0451, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:26<06:17, 10.33it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.11it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.22it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.04it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.64it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 63.18it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 61.72it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 62.44it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 61.36it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 58.46it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6642665266990662, 'eval_f1': 0.8799999999999999, 'eval_runtime': 1.827, 'eval_samples_per_second': 328.406, 'eval_steps_per_second': 41.051, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:28<06:17, 10.33it/s]\n",
      "100% 75/75 [00:01<00:00, 60.94it/s]\u001B[A\n",
      "{'loss': 0.0272, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:06<07:17,  8.23it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.60it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.24it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.44it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.34it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.22it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.21it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.34it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 61.65it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 60.07it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.747555136680603, 'eval_f1': 0.8582019005920691, 'eval_runtime': 1.8295, 'eval_samples_per_second': 327.956, 'eval_steps_per_second': 40.995, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:08<07:17,  8.23it/s]\n",
      "100% 75/75 [00:01<00:00, 61.40it/s]\u001B[A\n",
      "{'loss': 0.0184, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:44<05:33,  9.88it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.69it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.51it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.17it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.68it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.59it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.03it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.48it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.38it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 58.81it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7283809781074524, 'eval_f1': 0.8676915909620313, 'eval_runtime': 1.891, 'eval_samples_per_second': 317.285, 'eval_steps_per_second': 39.661, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:46<05:33,  9.88it/s]\n",
      "100% 75/75 [00:01<00:00, 59.99it/s]\u001B[A\n",
      "{'loss': 0.0112, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:24<04:52, 10.26it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 66.21it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.59it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.81it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.46it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 62.08it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.97it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 59.69it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 59.37it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 57.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8129409551620483, 'eval_f1': 0.8601856633623459, 'eval_runtime': 1.8408, 'eval_samples_per_second': 325.949, 'eval_steps_per_second': 40.744, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:25<04:52, 10.26it/s]\n",
      "100% 75/75 [00:01<00:00, 60.19it/s]\u001B[A\n",
      "{'loss': 0.0118, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:02<04:15, 10.57it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.74it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.65it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.85it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.96it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 63.71it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 57.50it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 51.68it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 47.49it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 46.98it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 45.81it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 44.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6497236490249634, 'eval_f1': 0.8820221036894622, 'eval_runtime': 2.0959, 'eval_samples_per_second': 286.27, 'eval_steps_per_second': 35.784, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:04<04:15, 10.57it/s]\n",
      "100% 75/75 [00:02<00:00, 44.43it/s]\u001B[A\n",
      "{'loss': 0.0066, 'learning_rate': 4.217543859649123e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:41<03:53, 10.28it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.48it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.99it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.03it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.27it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.95it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.00it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.97it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.95it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 57.48it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6628699898719788, 'eval_f1': 0.8913012385937813, 'eval_runtime': 1.8905, 'eval_samples_per_second': 317.378, 'eval_steps_per_second': 39.672, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:43<03:53, 10.28it/s]\n",
      "100% 75/75 [00:01<00:00, 59.73it/s]\u001B[A\n",
      "{'loss': 0.006, 'learning_rate': 3.6912280701754386e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:20<04:08,  8.46it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 49.59it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 44.47it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 42.66it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 42.07it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 40.71it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 40.63it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 41.19it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 41.53it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 41.63it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 41.90it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 41.27it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 39.61it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 40.33it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.689775288105011, 'eval_f1': 0.8802977593236823, 'eval_runtime': 2.4608, 'eval_samples_per_second': 243.824, 'eval_steps_per_second': 30.478, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:23<04:08,  8.46it/s]\n",
      "100% 75/75 [00:02<00:00, 39.68it/s]\u001B[A\n",
      "{'loss': 0.006, 'learning_rate': 3.1649122807017546e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:00<03:04,  9.75it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.92it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.48it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.48it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.84it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 62.25it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 61.10it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.50it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 61.36it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 59.49it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 59.15it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7308962941169739, 'eval_f1': 0.8842940196447256, 'eval_runtime': 1.858, 'eval_samples_per_second': 322.927, 'eval_steps_per_second': 40.366, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:02<03:04,  9.75it/s]\n",
      "100% 75/75 [00:01<00:00, 57.62it/s]\u001B[A\n",
      "{'loss': 0.0025, 'learning_rate': 2.6385964912280705e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [09:39<03:12,  7.77it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.25it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 38.13it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 38.50it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 38.99it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 39.49it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 37.70it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 38.83it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 39.88it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 40.38it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 44.96it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 51.00it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 50.91it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 54.25it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7049567103385925, 'eval_f1': 0.88838124426535, 'eval_runtime': 2.2508, 'eval_samples_per_second': 266.573, 'eval_steps_per_second': 33.322, 'epoch': 15.0}\n",
      " 75% 4500/6000 [09:41<03:12,  7.77it/s]\n",
      "100% 75/75 [00:02<00:00, 53.62it/s]\u001B[A\n",
      "{'loss': 0.0015, 'learning_rate': 2.112280701754386e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [10:18<01:56, 10.30it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.64it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.06it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.84it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.04it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.40it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.55it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.35it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.28it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 58.67it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8474482297897339, 'eval_f1': 0.8732464977250585, 'eval_runtime': 1.8533, 'eval_samples_per_second': 323.74, 'eval_steps_per_second': 40.468, 'epoch': 16.0}\n",
      " 80% 4800/6000 [10:20<01:56, 10.30it/s]\n",
      "100% 75/75 [00:01<00:00, 59.27it/s]\u001B[A\n",
      "{'loss': 0.0003, 'learning_rate': 1.585964912280702e-06, 'epoch': 17.0}\n",
      " 85% 5100/6000 [10:58<02:05,  7.18it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 60.74it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.93it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.79it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.49it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 62.19it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.75it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.55it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 61.70it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 58.79it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8096143007278442, 'eval_f1': 0.8817267888823181, 'eval_runtime': 1.8973, 'eval_samples_per_second': 316.244, 'eval_steps_per_second': 39.53, 'epoch': 17.0}\n",
      " 85% 5100/6000 [11:00<02:05,  7.18it/s]\n",
      "100% 75/75 [00:01<00:00, 59.80it/s]\u001B[A\n",
      "{'train_runtime': 667.5659, 'train_samples_per_second': 71.903, 'train_steps_per_second': 8.988, 'train_loss': 0.10732876393432711, 'epoch': 17.0}\n",
      " 85% 5100/6000 [11:07<02:05,  7.18it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      " 85% 5100/6000 [11:07<01:57,  7.63it/s]\n",
      "100% 75/75 [00:02<00:00, 31.80it/s]\n",
      "f1 0.7053435114503817\n",
      "efl_f1 0.8913012385937813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.3846    0.4348        13\n",
      "           1     0.8148    0.7213    0.7652        61\n",
      "           2     0.8824    0.9524    0.9160       126\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.7324    0.6861    0.7053       200\n",
      "weighted avg     0.8369    0.8450    0.8388       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-144ede4e064d1fa2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-220624f57548f560.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a3c4f9951fe52cfd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b10d929e83043941.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-42fb794701d1e885.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e86617015e94cbe8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a9a3634da9169bca.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fafe26e9d2a8c52a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cdc07dc0660cfa1e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4bd7979f1a4947d6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b66bd300dc7ebb90.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-71cb55f15e161ce6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d1402c3480df3039.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cb97a40deb71ffa2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a39fc7fb2098b423.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8ebd6028ef2d4a1a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a48a4a86091ff8d6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a8fb8610e8513b51.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-87cbbe0fcc3b95a2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-94c606494d20c0ba.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-96c183286c55a0f6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-84cd555305d2a26e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-46eb88d5bb4ce3a5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-abe7e2bc59c07fdb.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dd00c640fa32aba7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-226162d57fd59eaa.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f94dceba4dff9fdf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a29b0ea789fe9c6f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f52a6196146ad645.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ef6e1ba179c1dc4d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dd7426ca1386eb12.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-e461d544ec25bd26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c3f543dc9a898eb9.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5842, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:32<09:54,  9.59it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 54.00it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 49.41it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 46.10it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 44.79it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 45.13it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 46.31it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 45.36it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 43.62it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 43.06it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 43.21it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 43.16it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 42.89it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 42.13it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5087867975234985, 'eval_f1': 0.7428026996329775, 'eval_runtime': 2.3416, 'eval_samples_per_second': 256.23, 'eval_steps_per_second': 32.029, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:34<09:54,  9.59it/s]\n",
      "100% 75/75 [00:02<00:00, 41.02it/s]\u001B[A\n",
      "{'loss': 0.3845, 'learning_rate': 9.475438596491228e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:11<09:09,  9.83it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 75.93it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 62.43it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 58.25it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.17it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 58.56it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 55.40it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 53.26it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 56.22it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.08it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 58.43it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4375070631504059, 'eval_f1': 0.8062841896221933, 'eval_runtime': 1.894, 'eval_samples_per_second': 316.789, 'eval_steps_per_second': 39.599, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:13<09:09,  9.83it/s]\n",
      "100% 75/75 [00:01<00:00, 57.32it/s]\u001B[A\n",
      "{'loss': 0.2742, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:50<10:04,  8.43it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 49.47it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 43.12it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 39.74it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 39.40it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 38.92it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 38.53it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 38.44it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 38.03it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 37.92it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 37.59it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 37.39it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 37.07it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 37.47it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 37.42it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 38.03it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 37.99it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.47407662868499756, 'eval_f1': 0.842144802031197, 'eval_runtime': 2.5822, 'eval_samples_per_second': 232.361, 'eval_steps_per_second': 29.045, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:53<10:04,  8.43it/s]\n",
      "100% 75/75 [00:02<00:00, 37.93it/s]\u001B[A\n",
      "{'loss': 0.1946, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:29<07:43, 10.36it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 68.56it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:00, 60.97it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 59.33it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 60.10it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 59.26it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 55.96it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 54.52it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 57.26it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 56.11it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.97it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5982012152671814, 'eval_f1': 0.8429770046444753, 'eval_runtime': 1.8984, 'eval_samples_per_second': 316.048, 'eval_steps_per_second': 39.506, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:31<07:43, 10.36it/s]\n",
      "100% 75/75 [00:01<00:00, 57.93it/s]\u001B[A\n",
      "{'loss': 0.1461, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:08<09:00,  8.33it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.80it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 43.48it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 41.34it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 40.07it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 39.04it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 39.71it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 39.89it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 39.16it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 39.48it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 39.44it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 38.83it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 39.19it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 44.08it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5394814014434814, 'eval_f1': 0.8592158073479469, 'eval_runtime': 2.3614, 'eval_samples_per_second': 254.086, 'eval_steps_per_second': 31.761, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:11<09:00,  8.33it/s]\n",
      "100% 75/75 [00:02<00:00, 48.69it/s]\u001B[A\n",
      "{'loss': 0.0921, 'learning_rate': 7.373684210526316e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:47<07:31,  9.31it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 79.19it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 64.06it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 58.28it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 62.58it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 57.07it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 57.59it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 54.19it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 56.38it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 57.26it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6898161768913269, 'eval_f1': 0.8577385487435152, 'eval_runtime': 1.8764, 'eval_samples_per_second': 319.758, 'eval_steps_per_second': 39.97, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:49<07:31,  9.31it/s]\n",
      "100% 75/75 [00:01<00:00, 59.52it/s]\u001B[A\n",
      "{'loss': 0.0657, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:26<06:39,  9.76it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 75.15it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 60.69it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 56.55it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 59.00it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 56.37it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 54.01it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 53.12it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 55.03it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 55.97it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 56.41it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7178165316581726, 'eval_f1': 0.8632954534814403, 'eval_runtime': 1.9037, 'eval_samples_per_second': 315.182, 'eval_steps_per_second': 39.398, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:28<06:39,  9.76it/s]\n",
      "100% 75/75 [00:01<00:00, 58.20it/s]\u001B[A\n",
      "{'loss': 0.0414, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:05<05:50, 10.26it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 79.46it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:00, 65.14it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 59.21it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 62.35it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 57.53it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 56.98it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 53.87it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 57.58it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 57.39it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.725982666015625, 'eval_f1': 0.8684194037519267, 'eval_runtime': 1.9055, 'eval_samples_per_second': 314.87, 'eval_steps_per_second': 39.359, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:06<05:50, 10.26it/s]\n",
      "100% 75/75 [00:01<00:00, 59.53it/s]\u001B[A\n",
      "{'loss': 0.0293, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:43<05:31,  9.96it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.93it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.89it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 58.30it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 58.83it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 61.14it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 54.51it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 54.87it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 55.46it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 56.60it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 57.07it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8116734027862549, 'eval_f1': 0.8615516673794192, 'eval_runtime': 1.9392, 'eval_samples_per_second': 309.41, 'eval_steps_per_second': 38.676, 'epoch': 9.0}\n",
      " 45% 2700/6000 [05:45<05:31,  9.96it/s]\n",
      "100% 75/75 [00:01<00:00, 58.37it/s]\u001B[A\n",
      "{'loss': 0.0236, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:22<05:36,  8.91it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 53.59it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 49.21it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 45.87it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 45.58it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 44.60it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 45.78it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 45.89it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 45.27it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 45.89it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 45.04it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 46.02it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 45.40it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 45.54it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8769772052764893, 'eval_f1': 0.8646599581448389, 'eval_runtime': 2.2613, 'eval_samples_per_second': 265.331, 'eval_steps_per_second': 33.166, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:24<05:36,  8.91it/s]\n",
      "100% 75/75 [00:02<00:00, 44.33it/s]\u001B[A\n",
      "{'loss': 0.0173, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:01<04:26, 10.15it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 73.77it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 62.42it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 57.34it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.41it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 58.62it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 54.59it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 52.68it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 56.91it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.90it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 59.01it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8075438141822815, 'eval_f1': 0.8768595452510075, 'eval_runtime': 1.8807, 'eval_samples_per_second': 319.037, 'eval_steps_per_second': 39.88, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:03<04:26, 10.15it/s]\n",
      "100% 75/75 [00:01<00:00, 58.74it/s]\u001B[A\n",
      "{'loss': 0.0112, 'learning_rate': 4.217543859649123e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:40<04:47,  8.36it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 49.55it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 42.45it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 40.98it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 40.54it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 39.30it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 40.35it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 40.33it/s]\u001B[A\n",
      " 55% 41/75 [00:01<00:00, 39.79it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 40.84it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 40.73it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 41.16it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 40.81it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 40.74it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.831042468547821, 'eval_f1': 0.8731280163192199, 'eval_runtime': 2.4974, 'eval_samples_per_second': 240.25, 'eval_steps_per_second': 30.031, 'epoch': 12.0}\n",
      " 60% 3600/6000 [07:43<04:47,  8.36it/s]\n",
      "100% 75/75 [00:02<00:00, 39.32it/s]\u001B[A\n",
      "{'loss': 0.0114, 'learning_rate': 3.6912280701754386e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:19<03:21, 10.44it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.00it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.65it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 58.14it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 55.87it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 59.59it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 55.54it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 54.84it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 53.63it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 56.11it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 55.35it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8753840327262878, 'eval_f1': 0.86670796842293, 'eval_runtime': 1.9246, 'eval_samples_per_second': 311.761, 'eval_steps_per_second': 38.97, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:21<03:21, 10.44it/s]\n",
      "100% 75/75 [00:01<00:00, 57.44it/s]\u001B[A\n",
      "{'loss': 0.0068, 'learning_rate': 3.1649122807017546e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [08:58<03:40,  8.15it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 76.01it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 59.55it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 55.39it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 58.23it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 55.83it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 53.52it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 52.41it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 54.42it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 54.90it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 56.72it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8758123517036438, 'eval_f1': 0.8774509803921569, 'eval_runtime': 1.9366, 'eval_samples_per_second': 309.814, 'eval_steps_per_second': 38.727, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:00<03:40,  8.15it/s]\n",
      "100% 75/75 [00:01<00:00, 56.63it/s]\u001B[A\n",
      "{'loss': 0.0047, 'learning_rate': 2.6385964912280705e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [09:37<02:28, 10.07it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 74.74it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 61.41it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 57.76it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 60.39it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 58.90it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 55.58it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 53.32it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 57.01it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.83it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 58.24it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9152483344078064, 'eval_f1': 0.8663701296460691, 'eval_runtime': 1.9385, 'eval_samples_per_second': 309.516, 'eval_steps_per_second': 38.689, 'epoch': 15.0}\n",
      " 75% 4500/6000 [09:39<02:28, 10.07it/s]\n",
      "100% 75/75 [00:01<00:00, 55.90it/s]\u001B[A\n",
      "{'loss': 0.0026, 'learning_rate': 2.112280701754386e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [10:16<01:55, 10.41it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 75.69it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 59.75it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 56.88it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 60.13it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 57.96it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 54.81it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 53.26it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 56.75it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.47it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 57.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8795247673988342, 'eval_f1': 0.878872583275099, 'eval_runtime': 1.9694, 'eval_samples_per_second': 304.667, 'eval_steps_per_second': 38.083, 'epoch': 16.0}\n",
      " 80% 4800/6000 [10:18<01:55, 10.41it/s]\n",
      "100% 75/75 [00:01<00:00, 57.21it/s]\u001B[A\n",
      "{'loss': 0.003, 'learning_rate': 1.585964912280702e-06, 'epoch': 17.0}\n",
      " 85% 5100/6000 [10:55<01:29, 10.02it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 78.08it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 53.51it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 48.86it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 47.54it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 46.67it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 45.41it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 45.73it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 46.07it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 46.98it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 47.25it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 46.46it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 44.82it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8709713816642761, 'eval_f1': 0.8825520307132754, 'eval_runtime': 2.2227, 'eval_samples_per_second': 269.937, 'eval_steps_per_second': 33.742, 'epoch': 17.0}\n",
      " 85% 5100/6000 [10:57<01:29, 10.02it/s]\n",
      "100% 75/75 [00:02<00:00, 44.57it/s]\u001B[A\n",
      "{'loss': 0.002, 'learning_rate': 1.0596491228070175e-06, 'epoch': 18.0}\n",
      " 90% 5400/6000 [11:34<00:57, 10.51it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 77.60it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 62.42it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 58.13it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.17it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 58.11it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 55.20it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 52.72it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 56.75it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 55.82it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 57.80it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8906309008598328, 'eval_f1': 0.88375, 'eval_runtime': 2.0294, 'eval_samples_per_second': 295.648, 'eval_steps_per_second': 36.956, 'epoch': 18.0}\n",
      " 90% 5400/6000 [11:36<00:57, 10.51it/s]\n",
      "100% 75/75 [00:02<00:00, 57.83it/s]\u001B[A\n",
      "{'loss': 0.0045, 'learning_rate': 5.333333333333335e-07, 'epoch': 19.0}\n",
      " 95% 5700/6000 [12:13<00:35,  8.42it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 43.51it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 41.85it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 40.05it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 40.87it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 40.27it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 40.59it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 40.64it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 39.58it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 39.89it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 40.19it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 40.42it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 40.38it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 40.79it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 40.99it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8982219696044922, 'eval_f1': 0.8817267888823181, 'eval_runtime': 2.4889, 'eval_samples_per_second': 241.075, 'eval_steps_per_second': 30.134, 'epoch': 19.0}\n",
      " 95% 5700/6000 [12:16<00:35,  8.42it/s]\n",
      "100% 75/75 [00:02<00:00, 40.75it/s]\u001B[A\n",
      "{'loss': 0.0015, 'learning_rate': 7.017543859649123e-09, 'epoch': 20.0}\n",
      "100% 6000/6000 [12:52<00:00, 10.43it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 73.89it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 62.76it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 58.57it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.32it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 59.16it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 55.70it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 52.85it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 57.28it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.85it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 58.65it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9045918583869934, 'eval_f1': 0.8817267888823181, 'eval_runtime': 1.9279, 'eval_samples_per_second': 311.226, 'eval_steps_per_second': 38.903, 'epoch': 20.0}\n",
      "100% 6000/6000 [12:54<00:00, 10.43it/s]\n",
      "100% 75/75 [00:01<00:00, 57.22it/s]\u001B[A\n",
      "{'train_runtime': 780.4224, 'train_samples_per_second': 61.505, 'train_steps_per_second': 7.688, 'train_loss': 0.09503513536850611, 'epoch': 20.0}\n",
      "100% 6000/6000 [13:00<00:00, 10.43it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "100% 6000/6000 [13:00<00:00,  7.68it/s]\n",
      "100% 75/75 [00:02<00:00, 29.54it/s]\n",
      "f1 0.7503990609854129\n",
      "efl_f1 0.88375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8571    0.5000    0.6316        24\n",
      "           1     0.6731    0.7447    0.7071        47\n",
      "           2     0.8955    0.9302    0.9125       129\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.8086    0.7250    0.7504       200\n",
      "weighted avg     0.8386    0.8350    0.8305       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7374977820822816\n",
      "efl_f1 0.8886785794703229\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 26 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-82\n"
     ]
    }
   ],
   "source": [
    "!python efl.py --base-model=bert-base-uncased --cross-validation=5 --config-name=efl"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python efl.py --base-model=distilbert-base-uncased --cross-validation=5 --config-name=efl"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGfx7CJ_-Ny0",
    "outputId": "1c415e9e-5ccc-42f7-e927-494df60f6c98"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2023-03-16 15:12:13.703951: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 15:12:15.598600: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 15:12:15.598762: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 15:12:15.598787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 4.26kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 483/483 [00:00<00:00, 76.9kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 347kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 466k/466k [00:00<00:00, 525kB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 24 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 24 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100% 1/1 [00:00<00:00, 8160.12it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 1309.49it/s]\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100% 1/1 [00:00<00:00, 752.21it/s]\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "Downloading pytorch_model.bin: 100% 268M/268M [00:01<00:00, 212MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1276: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/6000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.5896, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:20<06:03, 15.68it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 116.36it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 112.85it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 106.17it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 100.93it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 102.57it/s]\u001B[A\n",
      " 92% 69/75 [00:00<00:00, 96.48it/s] \u001B[A\n",
      "\n",
      "Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 3.91MB/s]\n",
      "\n",
      "{'eval_loss': 0.43728506565093994, 'eval_f1': 0.717673836681245, 'eval_runtime': 3.5659, 'eval_samples_per_second': 168.258, 'eval_steps_per_second': 21.032, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:23<06:03, 15.68it/s]\n",
      "{'loss': 0.3918, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:46<06:03, 14.85it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 81.74it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 74.73it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 73.51it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 72.97it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 72.82it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 85.54it/s]\u001B[A\n",
      " 87% 65/75 [00:00<00:00, 91.82it/s]\u001B[A\n",
      "100% 75/75 [00:00<00:00, 85.03it/s]\u001B[A\n",
      "{'eval_loss': 0.3720657229423523, 'eval_f1': 0.8155186240605555, 'eval_runtime': 2.7827, 'eval_samples_per_second': 215.615, 'eval_steps_per_second': 26.952, 'epoch': 2.0}\n",
      "\n",
      " 10% 600/6000 [00:49<06:03, 14.85it/s]\n",
      "{'loss': 0.3076, 'learning_rate': 8.950877192982457e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:10<05:00, 16.98it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 113.80it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 113.15it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 109.38it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 101.62it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 105.00it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3992498517036438, 'eval_f1': 0.8429770046444753, 'eval_runtime': 2.6199, 'eval_samples_per_second': 229.014, 'eval_steps_per_second': 28.627, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:12<05:00, 16.98it/s]\n",
      "100% 75/75 [00:02<00:00, 99.82it/s]\u001B[A\n",
      "{'loss': 0.2374, 'learning_rate': 8.424561403508773e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:34<05:32, 14.45it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 74.68it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:00, 80.39it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 92.87it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 97.16it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 93.49it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 95.00it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4310137927532196, 'eval_f1': 0.8441639395356086, 'eval_runtime': 2.6949, 'eval_samples_per_second': 222.639, 'eval_steps_per_second': 27.83, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:37<05:32, 14.45it/s]\n",
      "100% 75/75 [00:02<00:00, 92.30it/s]\u001B[A\n",
      "{'loss': 0.1837, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:58<04:16, 17.52it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 113.51it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 109.00it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 109.16it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 99.17it/s] \u001B[A\n",
      " 76% 57/75 [00:00<00:00, 99.86it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5702561736106873, 'eval_f1': 0.8471448848127525, 'eval_runtime': 2.6496, 'eval_samples_per_second': 226.448, 'eval_steps_per_second': 28.306, 'epoch': 5.0}\n",
      " 25% 1500/6000 [02:01<04:16, 17.52it/s]\n",
      "100% 75/75 [00:02<00:00, 102.92it/s]\u001B[A\n",
      "{'loss': 0.1315, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:22<04:40, 14.98it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 110.96it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 108.23it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 104.40it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 95.84it/s] \u001B[A\n",
      " 75% 56/75 [00:00<00:00, 93.65it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5809632539749146, 'eval_f1': 0.8626059079459585, 'eval_runtime': 2.6697, 'eval_samples_per_second': 224.741, 'eval_steps_per_second': 28.093, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:25<04:40, 14.98it/s]\n",
      "100% 75/75 [00:02<00:00, 95.22it/s]\u001B[A\n",
      "{'loss': 0.1234, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:46<03:40, 17.71it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 117.53it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 111.99it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 110.33it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 98.30it/s] \u001B[A\n",
      " 80% 60/75 [00:00<00:00, 102.53it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6031045317649841, 'eval_f1': 0.8578597118428704, 'eval_runtime': 2.6484, 'eval_samples_per_second': 226.549, 'eval_steps_per_second': 28.319, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:49<03:40, 17.71it/s]\n",
      "100% 75/75 [00:02<00:00, 97.06it/s]\u001B[A\n",
      "{'loss': 0.0862, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:10<03:38, 16.48it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 107.10it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 103.91it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 105.80it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 96.73it/s] \u001B[A\n",
      " 73% 55/75 [00:00<00:00, 101.00it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7016239762306213, 'eval_f1': 0.8521512000394265, 'eval_runtime': 2.647, 'eval_samples_per_second': 226.669, 'eval_steps_per_second': 28.334, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:13<03:38, 16.48it/s]\n",
      "100% 75/75 [00:02<00:00, 100.87it/s]\u001B[A\n",
      "{'loss': 0.0679, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:34<03:06, 17.71it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 112.98it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 109.83it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 111.07it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 103.06it/s]\u001B[A\n",
      " 80% 60/75 [00:00<00:00, 106.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6464967131614685, 'eval_f1': 0.856031521519499, 'eval_runtime': 2.614, 'eval_samples_per_second': 229.533, 'eval_steps_per_second': 28.692, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:37<03:06, 17.71it/s]\n",
      "100% 75/75 [00:02<00:00, 100.58it/s]\u001B[A\n",
      "{'loss': 0.0572, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:58<02:54, 17.21it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 111.01it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 110.39it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 109.93it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 102.89it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 97.80it/s] \u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6947255730628967, 'eval_f1': 0.8646599581448389, 'eval_runtime': 2.6582, 'eval_samples_per_second': 225.719, 'eval_steps_per_second': 28.215, 'epoch': 10.0}\n",
      " 50% 3000/6000 [04:01<02:54, 17.21it/s]\n",
      "100% 75/75 [00:02<00:00, 95.60it/s]\u001B[A\n",
      "{'loss': 0.0322, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:22<02:34, 17.52it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 110.61it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 104.61it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 100.66it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 96.62it/s] \u001B[A\n",
      " 76% 57/75 [00:00<00:00, 100.40it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7699047327041626, 'eval_f1': 0.8604975587072774, 'eval_runtime': 2.6558, 'eval_samples_per_second': 225.916, 'eval_steps_per_second': 28.24, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:24<02:34, 17.52it/s]\n",
      "100% 75/75 [00:02<00:00, 102.39it/s]\u001B[A\n",
      "{'loss': 0.0203, 'learning_rate': 4.217543859649123e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:46<02:14, 17.90it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 119.75it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 110.88it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 111.61it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 102.77it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 104.04it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7938248515129089, 'eval_f1': 0.8584242424242425, 'eval_runtime': 2.6075, 'eval_samples_per_second': 230.102, 'eval_steps_per_second': 28.763, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:48<02:14, 17.90it/s]\n",
      "100% 75/75 [00:02<00:00, 99.29it/s]\u001B[A\n",
      "{'loss': 0.0216, 'learning_rate': 3.692982456140351e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:09<01:56, 17.95it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 112.91it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 111.23it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 110.41it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 101.91it/s]\u001B[A\n",
      " 80% 60/75 [00:00<00:00, 104.76it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7661309242248535, 'eval_f1': 0.8728163692814125, 'eval_runtime': 2.6344, 'eval_samples_per_second': 227.757, 'eval_steps_per_second': 28.47, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:12<01:56, 17.95it/s]\n",
      "100% 75/75 [00:02<00:00, 98.48it/s]\u001B[A\n",
      "{'loss': 0.0168, 'learning_rate': 3.1666666666666667e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:33<01:44, 17.19it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:00, 120.44it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 112.82it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 113.32it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 105.39it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 107.34it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7992215156555176, 'eval_f1': 0.870330348873109, 'eval_runtime': 2.6013, 'eval_samples_per_second': 230.652, 'eval_steps_per_second': 28.832, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:36<01:44, 17.19it/s]\n",
      "100% 75/75 [00:02<00:00, 92.51it/s]\u001B[A\n",
      "{'loss': 0.0082, 'learning_rate': 2.6403508771929826e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [05:57<01:26, 17.38it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 113.30it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 110.11it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 109.17it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 102.14it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 101.41it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9278980493545532, 'eval_f1': 0.8568555945605125, 'eval_runtime': 2.7067, 'eval_samples_per_second': 221.67, 'eval_steps_per_second': 27.709, 'epoch': 15.0}\n",
      " 75% 4500/6000 [05:59<01:26, 17.38it/s]\n",
      "100% 75/75 [00:02<00:00, 84.91it/s]\u001B[A\n",
      "{'loss': 0.0084, 'learning_rate': 2.1157894736842107e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [06:20<01:06, 18.01it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 115.90it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 114.52it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 111.00it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 97.00it/s] \u001B[A\n",
      " 79% 59/75 [00:00<00:00, 100.84it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8384252190589905, 'eval_f1': 0.8655896772872157, 'eval_runtime': 2.6185, 'eval_samples_per_second': 229.143, 'eval_steps_per_second': 28.643, 'epoch': 16.0}\n",
      " 80% 4800/6000 [06:23<01:06, 18.01it/s]\n",
      "100% 75/75 [00:02<00:00, 94.48it/s]\u001B[A\n",
      "{'loss': 0.0069, 'learning_rate': 1.5894736842105265e-06, 'epoch': 17.0}\n",
      " 85% 5100/6000 [06:44<00:52, 17.13it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 110.44it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 86.20it/s] \u001B[A\n",
      " 44% 33/75 [00:00<00:00, 81.04it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 73.61it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 72.62it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 75.04it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8386250734329224, 'eval_f1': 0.8770301843601312, 'eval_runtime': 2.881, 'eval_samples_per_second': 208.258, 'eval_steps_per_second': 26.032, 'epoch': 17.0}\n",
      " 85% 5100/6000 [06:47<00:52, 17.13it/s]\n",
      "100% 75/75 [00:02<00:00, 77.08it/s]\u001B[A\n",
      "{'loss': 0.009, 'learning_rate': 1.0631578947368422e-06, 'epoch': 18.0}\n",
      " 90% 5400/6000 [07:08<00:34, 17.58it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 116.42it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 108.74it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 106.70it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 97.93it/s] \u001B[A\n",
      " 76% 57/75 [00:00<00:00, 101.26it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8355662822723389, 'eval_f1': 0.8781169062008025, 'eval_runtime': 2.6704, 'eval_samples_per_second': 224.688, 'eval_steps_per_second': 28.086, 'epoch': 18.0}\n",
      " 90% 5400/6000 [07:11<00:34, 17.58it/s]\n",
      "100% 75/75 [00:02<00:00, 96.65it/s]\u001B[A\n",
      "{'loss': 0.0114, 'learning_rate': 5.368421052631579e-07, 'epoch': 19.0}\n",
      " 95% 5700/6000 [07:32<00:19, 15.38it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:00, 91.29it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 83.65it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 82.43it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 82.56it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 78.16it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 79.70it/s]\u001B[A\n",
      " 87% 65/75 [00:00<00:00, 81.83it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8581812381744385, 'eval_f1': 0.8739672314801848, 'eval_runtime': 2.8248, 'eval_samples_per_second': 212.402, 'eval_steps_per_second': 26.55, 'epoch': 19.0}\n",
      " 95% 5700/6000 [07:35<00:19, 15.38it/s]\n",
      "100% 75/75 [00:02<00:00, 74.07it/s]\u001B[A\n",
      "{'loss': 0.0064, 'learning_rate': 1.0526315789473684e-08, 'epoch': 20.0}\n",
      "100% 6000/6000 [07:56<00:00, 17.12it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 111.32it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 107.18it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 109.57it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 101.73it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 104.97it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8338599801063538, 'eval_f1': 0.8743080595195775, 'eval_runtime': 2.6489, 'eval_samples_per_second': 226.513, 'eval_steps_per_second': 28.314, 'epoch': 20.0}\n",
      "100% 6000/6000 [07:59<00:00, 17.12it/s]\n",
      "100% 75/75 [00:02<00:00, 98.67it/s]\u001B[A\n",
      "{'train_runtime': 482.8546, 'train_samples_per_second': 99.409, 'train_steps_per_second': 12.426, 'train_loss': 0.11588012111186981, 'epoch': 20.0}\n",
      "100% 6000/6000 [08:02<00:00, 17.12it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "100% 6000/6000 [08:03<00:00, 12.41it/s]\n",
      "100% 75/75 [00:02<00:00, 25.92it/s]\n",
      "f1 0.7598963591089575\n",
      "efl_f1 0.8781169062008025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.6667    0.6154        12\n",
      "           1     0.8070    0.7302    0.7667        63\n",
      "           2     0.8837    0.9120    0.8976       125\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.7541    0.7696    0.7599       200\n",
      "weighted avg     0.8408    0.8400    0.8394       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5983, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:18<06:05, 15.58it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 85.69it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 78.50it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 77.16it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 77.49it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 75.94it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 74.96it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 73.76it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 72.64it/s]\u001B[A\n",
      " 99% 74/75 [00:01<00:00, 69.47it/s]\u001B[A\n",
      "{'eval_loss': 0.4460350573062897, 'eval_f1': 0.7496580027359782, 'eval_runtime': 2.9094, 'eval_samples_per_second': 206.227, 'eval_steps_per_second': 25.778, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:21<06:05, 15.58it/s]\n",
      "{'loss': 0.4058, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:42<05:11, 17.32it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 110.81it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 108.12it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 106.49it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 105.12it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 104.98it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.35818618535995483, 'eval_f1': 0.81689262895434, 'eval_runtime': 2.5669, 'eval_samples_per_second': 233.742, 'eval_steps_per_second': 29.218, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:45<05:11, 17.32it/s]\n",
      "100% 75/75 [00:02<00:00, 102.49it/s]\u001B[A\n",
      "{'loss': 0.3171, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:06<05:49, 14.59it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 59.56it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 59.74it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 63.16it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 66.07it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 66.48it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 67.00it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 68.51it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 68.09it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 68.58it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3885190784931183, 'eval_f1': 0.8253712552963951, 'eval_runtime': 3.0126, 'eval_samples_per_second': 199.162, 'eval_steps_per_second': 24.895, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:09<05:49, 14.59it/s]\n",
      "100% 75/75 [00:02<00:00, 67.81it/s]\u001B[A\n",
      "{'loss': 0.2493, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:31<04:38, 17.21it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 111.15it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 106.58it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 107.89it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 106.20it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 104.85it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4001600742340088, 'eval_f1': 0.8687499999999999, 'eval_runtime': 2.5659, 'eval_samples_per_second': 233.839, 'eval_steps_per_second': 29.23, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:33<04:38, 17.21it/s]\n",
      "100% 75/75 [00:02<00:00, 105.10it/s]\u001B[A\n",
      "{'loss': 0.1824, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:55<05:20, 14.03it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 81.70it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 71.88it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 68.97it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 68.86it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 68.73it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 69.55it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 70.25it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 69.33it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4290224313735962, 'eval_f1': 0.8684172888240509, 'eval_runtime': 2.9349, 'eval_samples_per_second': 204.435, 'eval_steps_per_second': 25.554, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:58<05:20, 14.03it/s]\n",
      "100% 75/75 [00:02<00:00, 70.49it/s]\u001B[A\n",
      "{'loss': 0.1248, 'learning_rate': 7.373684210526316e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:19<03:54, 17.89it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 116.78it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 109.22it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 109.96it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 109.04it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 107.76it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5231772661209106, 'eval_f1': 0.8718528569274838, 'eval_runtime': 2.5796, 'eval_samples_per_second': 232.591, 'eval_steps_per_second': 29.074, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:21<03:54, 17.89it/s]\n",
      "100% 75/75 [00:02<00:00, 104.79it/s]\u001B[A\n",
      "{'loss': 0.0968, 'learning_rate': 6.8473684210526325e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:43<05:03, 12.85it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 81.32it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 75.29it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 72.91it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 71.48it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 70.92it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 68.40it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 67.79it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 67.82it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5543986558914185, 'eval_f1': 0.8773488481798883, 'eval_runtime': 2.9195, 'eval_samples_per_second': 205.512, 'eval_steps_per_second': 25.689, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:46<05:03, 12.85it/s]\n",
      "100% 75/75 [00:02<00:00, 68.04it/s]\u001B[A\n",
      "{'loss': 0.0685, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:07<03:23, 17.68it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 115.55it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 101.62it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 100.10it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 99.52it/s] \u001B[A\n",
      " 76% 57/75 [00:00<00:00, 100.53it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5908018350601196, 'eval_f1': 0.878576921520217, 'eval_runtime': 2.5845, 'eval_samples_per_second': 232.154, 'eval_steps_per_second': 29.019, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:10<03:23, 17.68it/s]\n",
      "100% 75/75 [00:02<00:00, 102.31it/s]\u001B[A\n",
      "{'loss': 0.0414, 'learning_rate': 5.798245614035089e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:32<03:45, 14.61it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 77.78it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:00, 93.93it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 100.82it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 102.46it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 101.32it/s]\u001B[A\n",
      " 84% 63/75 [00:00<00:00, 101.95it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6820638179779053, 'eval_f1': 0.8799999999999999, 'eval_runtime': 2.6072, 'eval_samples_per_second': 230.133, 'eval_steps_per_second': 28.767, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:34<03:45, 14.61it/s]\n",
      "100% 75/75 [00:02<00:00, 104.35it/s]\u001B[A\n",
      "{'loss': 0.0344, 'learning_rate': 5.271929824561403e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:56<02:43, 18.35it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 113.27it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 104.60it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 102.02it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 101.69it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 104.30it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7351502776145935, 'eval_f1': 0.8700237670825905, 'eval_runtime': 2.5897, 'eval_samples_per_second': 231.689, 'eval_steps_per_second': 28.961, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:58<02:43, 18.35it/s]\n",
      "100% 75/75 [00:02<00:00, 104.42it/s]\u001B[A\n",
      "{'loss': 0.0241, 'learning_rate': 4.74561403508772e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:20<03:15, 13.79it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 118.78it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 107.41it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 107.91it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 106.31it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 105.67it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7517942190170288, 'eval_f1': 0.878872583275099, 'eval_runtime': 2.5724, 'eval_samples_per_second': 233.242, 'eval_steps_per_second': 29.155, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:22<03:15, 13.79it/s]\n",
      "100% 75/75 [00:02<00:00, 105.96it/s]\u001B[A\n",
      "{'loss': 0.0188, 'learning_rate': 4.219298245614035e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:44<02:19, 17.25it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 108.68it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 105.02it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 106.12it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 105.19it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 104.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.777996301651001, 'eval_f1': 0.8799999999999999, 'eval_runtime': 2.5964, 'eval_samples_per_second': 231.093, 'eval_steps_per_second': 28.887, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:46<02:19, 17.25it/s]\n",
      "100% 75/75 [00:02<00:00, 104.10it/s]\u001B[A\n",
      "{'loss': 0.0096, 'learning_rate': 3.692982456140351e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:08<02:13, 15.75it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 114.27it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 106.37it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 107.54it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 107.05it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 107.48it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7956748008728027, 'eval_f1': 0.8728163692814125, 'eval_runtime': 2.555, 'eval_samples_per_second': 234.831, 'eval_steps_per_second': 29.354, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:10<02:13, 15.75it/s]\n",
      "100% 75/75 [00:02<00:00, 106.67it/s]\u001B[A\n",
      "{'loss': 0.0102, 'learning_rate': 3.1666666666666667e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:32<01:45, 17.10it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 106.03it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 104.16it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 103.46it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 106.38it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 104.62it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8413124680519104, 'eval_f1': 0.8749747423721963, 'eval_runtime': 2.6053, 'eval_samples_per_second': 230.297, 'eval_steps_per_second': 28.787, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:34<01:45, 17.10it/s]\n",
      "100% 75/75 [00:02<00:00, 103.65it/s]\u001B[A\n",
      "{'train_runtime': 338.4743, 'train_samples_per_second': 141.813, 'train_steps_per_second': 17.727, 'train_loss': 0.15583309008961632, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:38<01:45, 17.10it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 42 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 42 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      " 70% 4200/6000 [05:39<02:25, 12.38it/s]\n",
      "100% 75/75 [00:02<00:00, 28.83it/s] \n",
      "f1 0.7037037037037037\n",
      "efl_f1 0.8799999999999999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.2857    0.4000        14\n",
      "           1     0.7857    0.8148    0.8000        54\n",
      "           2     0.8913    0.9318    0.9111       132\n",
      "\n",
      "    accuracy                         0.8550       200\n",
      "   macro avg     0.7812    0.6774    0.7037       200\n",
      "weighted avg     0.8471    0.8550    0.8453       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5736, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:18<05:17, 17.93it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 103.66it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 97.78it/s] \u001B[A\n",
      " 44% 33/75 [00:00<00:00, 100.78it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 100.07it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 103.20it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 100.56it/s]\u001B[A\n",
      "{'eval_loss': 0.4377286732196808, 'eval_f1': 0.7676002724686461, 'eval_runtime': 2.5966, 'eval_samples_per_second': 231.07, 'eval_steps_per_second': 28.884, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:21<05:17, 17.93it/s]\n",
      "{'loss': 0.4151, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:42<05:08, 17.50it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 109.45it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 102.21it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 101.13it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 99.49it/s] \u001B[A\n",
      " 75% 56/75 [00:00<00:00, 101.69it/s]\u001B[A\n",
      " 89% 67/75 [00:00<00:00, 87.75it/s] \u001B[A\n",
      "{'eval_loss': 0.342985063791275, 'eval_f1': 0.8155349318053589, 'eval_runtime': 2.6922, 'eval_samples_per_second': 222.864, 'eval_steps_per_second': 27.858, 'epoch': 2.0}\n",
      "\n",
      " 10% 600/6000 [00:45<05:08, 17.50it/s]\n",
      "{'loss': 0.3312, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:06<04:55, 17.25it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 102.48it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 98.31it/s] \u001B[A\n",
      " 44% 33/75 [00:00<00:00, 101.97it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 99.99it/s] \u001B[A\n",
      " 73% 55/75 [00:00<00:00, 102.66it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.34075793623924255, 'eval_f1': 0.8462064251537935, 'eval_runtime': 2.6111, 'eval_samples_per_second': 229.786, 'eval_steps_per_second': 28.723, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:09<04:55, 17.25it/s]\n",
      "100% 75/75 [00:02<00:00, 100.49it/s]\u001B[A\n",
      "{'loss': 0.2789, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:30<05:13, 15.33it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 84.32it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 80.82it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 78.67it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 80.85it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 78.24it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 79.89it/s]\u001B[A\n",
      " 84% 63/75 [00:00<00:00, 80.48it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.37132179737091064, 'eval_f1': 0.8583026960784313, 'eval_runtime': 2.8346, 'eval_samples_per_second': 211.673, 'eval_steps_per_second': 26.459, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:33<05:13, 15.33it/s]\n",
      "100% 75/75 [00:02<00:00, 77.78it/s]\u001B[A\n",
      "{'loss': 0.2114, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:54<04:12, 17.81it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 107.88it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 100.04it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 103.00it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 101.51it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 104.22it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.43916743993759155, 'eval_f1': 0.8684194037519267, 'eval_runtime': 2.6097, 'eval_samples_per_second': 229.909, 'eval_steps_per_second': 28.739, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:57<04:12, 17.81it/s]\n",
      "100% 75/75 [00:02<00:00, 100.92it/s]\u001B[A\n",
      "{'loss': 0.1553, 'learning_rate': 7.373684210526316e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:18<04:24, 15.86it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 81.53it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 74.87it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 76.55it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 75.23it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 74.60it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 74.36it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 73.30it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 71.09it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4948081970214844, 'eval_f1': 0.8798592637089162, 'eval_runtime': 2.923, 'eval_samples_per_second': 205.265, 'eval_steps_per_second': 25.658, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:21<04:24, 15.86it/s]\n",
      "100% 75/75 [00:02<00:00, 68.17it/s]\u001B[A\n",
      "{'loss': 0.1346, 'learning_rate': 6.8473684210526325e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:42<03:47, 17.17it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:00, 98.43it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 95.22it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 99.91it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 97.99it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 99.84it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 100.26it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5364110469818115, 'eval_f1': 0.8632367307066102, 'eval_runtime': 2.5966, 'eval_samples_per_second': 231.075, 'eval_steps_per_second': 28.884, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:45<03:47, 17.17it/s]\n",
      "100% 75/75 [00:02<00:00, 102.47it/s]\u001B[A\n",
      "{'loss': 0.0899, 'learning_rate': 6.321052631578948e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:07<04:15, 14.08it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 77.55it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 72.62it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 71.21it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 69.12it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 66.39it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 65.86it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 64.84it/s]\u001B[A\n",
      " 80% 60/75 [00:00<00:00, 65.99it/s]\u001B[A\n",
      " 89% 67/75 [00:00<00:00, 66.45it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5206488370895386, 'eval_f1': 0.8823752486670802, 'eval_runtime': 2.9886, 'eval_samples_per_second': 200.761, 'eval_steps_per_second': 25.095, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:10<04:15, 14.08it/s]\n",
      "100% 75/75 [00:02<00:00, 65.85it/s]\u001B[A\n",
      "{'loss': 0.0721, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:31<03:03, 17.98it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 100.80it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 96.18it/s] \u001B[A\n",
      " 44% 33/75 [00:00<00:00, 96.29it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 97.45it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 103.70it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.578150749206543, 'eval_f1': 0.8886314468951215, 'eval_runtime': 2.5966, 'eval_samples_per_second': 231.068, 'eval_steps_per_second': 28.884, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:33<03:03, 17.98it/s]\n",
      "100% 75/75 [00:02<00:00, 103.38it/s]\u001B[A\n",
      "{'loss': 0.0448, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:55<03:30, 14.27it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 78.40it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:00, 65.82it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 67.14it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 67.92it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 65.37it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 64.51it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 65.94it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 64.48it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 65.31it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5956112742424011, 'eval_f1': 0.881280637254902, 'eval_runtime': 3.0359, 'eval_samples_per_second': 197.632, 'eval_steps_per_second': 24.704, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:58<03:30, 14.27it/s]\n",
      "100% 75/75 [00:03<00:00, 66.97it/s]\u001B[A\n",
      "{'loss': 0.0387, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:19<02:30, 17.88it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 101.98it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 94.66it/s] \u001B[A\n",
      " 44% 33/75 [00:00<00:00, 98.43it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 98.63it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 103.33it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6531277298927307, 'eval_f1': 0.870632300034498, 'eval_runtime': 5.4073, 'eval_samples_per_second': 110.961, 'eval_steps_per_second': 13.87, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:25<02:30, 17.88it/s]\n",
      "100% 75/75 [00:05<00:00, 103.11it/s]\u001B[A\n",
      "{'loss': 0.0268, 'learning_rate': 4.217543859649123e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:46<02:16, 17.62it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 107.58it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 99.82it/s] \u001B[A\n",
      " 45% 34/75 [00:00<00:00, 104.93it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 104.21it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 102.76it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.673404335975647, 'eval_f1': 0.8711047320753073, 'eval_runtime': 2.5806, 'eval_samples_per_second': 232.507, 'eval_steps_per_second': 29.063, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:49<02:16, 17.62it/s]\n",
      "100% 75/75 [00:02<00:00, 104.37it/s]\u001B[A\n",
      "{'loss': 0.0238, 'learning_rate': 3.6912280701754386e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:10<02:15, 15.55it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 76.82it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 72.11it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 74.12it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 77.51it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 78.62it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 78.20it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 73.39it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 74.46it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6798698902130127, 'eval_f1': 0.8808138709952041, 'eval_runtime': 2.8684, 'eval_samples_per_second': 209.176, 'eval_steps_per_second': 26.147, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:13<02:15, 15.55it/s]\n",
      "100% 75/75 [00:02<00:00, 77.44it/s]\u001B[A\n",
      "{'loss': 0.0231, 'learning_rate': 3.1649122807017546e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:34<01:48, 16.58it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:00, 99.21it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 91.51it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 89.00it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 91.78it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 92.78it/s]\u001B[A\n",
      " 81% 61/75 [00:00<00:00, 97.08it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6904734969139099, 'eval_f1': 0.8886314468951215, 'eval_runtime': 2.6426, 'eval_samples_per_second': 227.052, 'eval_steps_per_second': 28.382, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:37<01:48, 16.58it/s]\n",
      "100% 75/75 [00:02<00:00, 99.62it/s]\u001B[A\n",
      "{'train_runtime': 340.7186, 'train_samples_per_second': 140.879, 'train_steps_per_second': 17.61, 'train_loss': 0.17280677250453405, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:40<01:48, 16.58it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      " 70% 4200/6000 [05:41<02:26, 12.31it/s]\n",
      "100% 75/75 [00:02<00:00, 25.95it/s]\n",
      "f1 0.7435349737981317\n",
      "efl_f1 0.8886314468951215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.3750    0.5455        16\n",
      "           1     0.7544    0.7818    0.7679        55\n",
      "           2     0.8905    0.9457    0.9173       129\n",
      "\n",
      "    accuracy                         0.8550       200\n",
      "   macro avg     0.8816    0.7009    0.7435       200\n",
      "weighted avg     0.8618    0.8550    0.8465       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5778, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:18<05:19, 17.85it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 79.20it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:00, 71.57it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 72.08it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 73.64it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 74.57it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 75.06it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 78.10it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 75.72it/s]\u001B[A\n",
      "100% 75/75 [00:00<00:00, 78.33it/s]\u001B[A\n",
      "{'eval_loss': 0.44168969988822937, 'eval_f1': 0.7554879374049119, 'eval_runtime': 2.8811, 'eval_samples_per_second': 208.254, 'eval_steps_per_second': 26.032, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:21<05:19, 17.85it/s]\n",
      "{'loss': 0.4063, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:42<05:16, 17.04it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 103.68it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 105.02it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 103.47it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 104.34it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 106.38it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3650343418121338, 'eval_f1': 0.8251997254988284, 'eval_runtime': 2.582, 'eval_samples_per_second': 232.378, 'eval_steps_per_second': 29.047, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:45<05:16, 17.04it/s]\n",
      "100% 75/75 [00:02<00:00, 102.25it/s]\u001B[A\n",
      "{'loss': 0.3136, 'learning_rate': 8.950877192982457e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:06<05:30, 15.43it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:00, 90.40it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 84.85it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 80.79it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 81.61it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 81.85it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 81.31it/s]\u001B[A\n",
      " 87% 65/75 [00:00<00:00, 78.86it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.351057767868042, 'eval_f1': 0.8552638261134529, 'eval_runtime': 2.842, 'eval_samples_per_second': 211.117, 'eval_steps_per_second': 26.39, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:09<05:30, 15.43it/s]\n",
      "100% 75/75 [00:02<00:00, 73.96it/s]\u001B[A\n",
      "{'loss': 0.2685, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:30<04:31, 17.69it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 115.08it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 105.54it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 99.21it/s] \u001B[A\n",
      " 60% 45/75 [00:00<00:00, 97.66it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 100.87it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.3598742187023163, 'eval_f1': 0.8661818181818182, 'eval_runtime': 2.6096, 'eval_samples_per_second': 229.918, 'eval_steps_per_second': 28.74, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:32<04:31, 17.69it/s]\n",
      "100% 75/75 [00:02<00:00, 99.24it/s]\u001B[A\n",
      "{'loss': 0.1975, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:54<05:13, 14.34it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 81.33it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 76.43it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 75.98it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 77.37it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 71.84it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 70.82it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 70.36it/s]\u001B[A\n",
      " 89% 67/75 [00:00<00:00, 69.95it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4540216028690338, 'eval_f1': 0.8660272033964934, 'eval_runtime': 2.9395, 'eval_samples_per_second': 204.118, 'eval_steps_per_second': 25.515, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:57<05:13, 14.34it/s]\n",
      "100% 75/75 [00:02<00:00, 69.16it/s]\u001B[A\n",
      "{'loss': 0.1405, 'learning_rate': 7.373684210526316e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:18<03:57, 17.69it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 117.57it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 104.67it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 106.32it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 108.97it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 107.96it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.48231202363967896, 'eval_f1': 0.8819257528820272, 'eval_runtime': 2.5847, 'eval_samples_per_second': 232.136, 'eval_steps_per_second': 29.017, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:21<03:57, 17.69it/s]\n",
      "100% 75/75 [00:02<00:00, 106.74it/s]\u001B[A\n",
      "{'loss': 0.1036, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:42<04:26, 14.63it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 80.53it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 66.90it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 66.02it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 66.01it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 63.35it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 62.07it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 62.44it/s]\u001B[A\n",
      " 80% 60/75 [00:00<00:00, 63.21it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 63.50it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5963729619979858, 'eval_f1': 0.8660272033964934, 'eval_runtime': 3.0181, 'eval_samples_per_second': 198.799, 'eval_steps_per_second': 24.85, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:45<04:26, 14.63it/s]\n",
      "100% 75/75 [00:03<00:00, 64.52it/s]\u001B[A\n",
      "{'loss': 0.0805, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:07<03:23, 17.68it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:00, 121.84it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 109.36it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 111.85it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 111.56it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 106.49it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6445369124412537, 'eval_f1': 0.8739672314801848, 'eval_runtime': 2.5597, 'eval_samples_per_second': 234.407, 'eval_steps_per_second': 29.301, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:09<03:23, 17.68it/s]\n",
      "100% 75/75 [00:02<00:00, 104.13it/s]\u001B[A\n",
      "{'loss': 0.0592, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:31<03:46, 14.57it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 78.97it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:00, 69.15it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 66.53it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 68.19it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 68.98it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 69.67it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 70.59it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 69.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7091126441955566, 'eval_f1': 0.8673242767730958, 'eval_runtime': 3.0452, 'eval_samples_per_second': 197.03, 'eval_steps_per_second': 24.629, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:34<03:46, 14.57it/s]\n",
      "100% 75/75 [00:03<00:00, 69.60it/s]\u001B[A\n",
      "{'loss': 0.0368, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:55<02:47, 17.93it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 117.06it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 108.79it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 107.94it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 100.32it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 101.55it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7420546412467957, 'eval_f1': 0.8653258261438611, 'eval_runtime': 2.6137, 'eval_samples_per_second': 229.562, 'eval_steps_per_second': 28.695, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:58<02:47, 17.93it/s]\n",
      "100% 75/75 [00:02<00:00, 101.30it/s]\u001B[A\n",
      "{'loss': 0.0331, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:20<03:03, 14.74it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 80.97it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 73.67it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 70.98it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 70.44it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 70.28it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 69.20it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 67.20it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 66.74it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7354516386985779, 'eval_f1': 0.8663288818729233, 'eval_runtime': 2.969, 'eval_samples_per_second': 202.087, 'eval_steps_per_second': 25.261, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:23<03:03, 14.74it/s]\n",
      "100% 75/75 [00:02<00:00, 65.61it/s]\u001B[A\n",
      "{'train_runtime': 266.5232, 'train_samples_per_second': 180.097, 'train_steps_per_second': 22.512, 'train_loss': 0.2015885809696082, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:26<03:03, 14.74it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 47 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 47 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      " 55% 3300/6000 [04:26<03:38, 12.37it/s]\n",
      "100% 75/75 [00:02<00:00, 28.51it/s] \n",
      "f1 0.6400489011141675\n",
      "efl_f1 0.8819257528820272\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4000    0.1538    0.2222        13\n",
      "           1     0.7937    0.8197    0.8065        61\n",
      "           2     0.8712    0.9127    0.8915       126\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.6883    0.6287    0.6400       200\n",
      "weighted avg     0.8169    0.8350    0.8220       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5769, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:18<05:40, 16.74it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 99.68it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 93.06it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 95.10it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 94.17it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 95.41it/s]\u001B[A\n",
      " 81% 61/75 [00:00<00:00, 95.65it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3990665078163147, 'eval_f1': 0.7818303871112107, 'eval_runtime': 2.6452, 'eval_samples_per_second': 226.824, 'eval_steps_per_second': 28.353, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:21<05:40, 16.74it/s]\n",
      "100% 75/75 [00:02<00:00, 98.68it/s]\u001B[A\n",
      "{'loss': 0.387, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:42<06:23, 14.08it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 79.06it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 70.82it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 70.08it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 67.02it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 67.24it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 67.03it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 67.11it/s]\u001B[A\n",
      " 80% 60/75 [00:00<00:00, 67.15it/s]\u001B[A\n",
      " 89% 67/75 [00:00<00:00, 67.92it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4270850718021393, 'eval_f1': 0.8022729540239267, 'eval_runtime': 2.9803, 'eval_samples_per_second': 201.323, 'eval_steps_per_second': 25.165, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:45<06:23, 14.08it/s]\n",
      "100% 75/75 [00:02<00:00, 66.12it/s]\u001B[A\n",
      "{'loss': 0.3053, 'learning_rate': 8.950877192982457e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:07<04:58, 17.06it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 104.32it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 95.55it/s] \u001B[A\n",
      " 43% 32/75 [00:00<00:00, 94.67it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 91.52it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 89.21it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 92.27it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4791579246520996, 'eval_f1': 0.8355779040474909, 'eval_runtime': 2.6471, 'eval_samples_per_second': 226.662, 'eval_steps_per_second': 28.333, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:09<04:58, 17.06it/s]\n",
      "100% 75/75 [00:02<00:00, 96.62it/s]\u001B[A\n",
      "{'loss': 0.2412, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:31<05:39, 14.16it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 76.84it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 71.23it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 68.80it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 68.26it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 68.54it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 69.01it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 68.73it/s]\u001B[A\n",
      " 81% 61/75 [00:00<00:00, 70.13it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.4917745888233185, 'eval_f1': 0.8442545109211776, 'eval_runtime': 2.8886, 'eval_samples_per_second': 207.712, 'eval_steps_per_second': 25.964, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:34<05:39, 14.16it/s]\n",
      "100% 75/75 [00:02<00:00, 80.26it/s]\u001B[A\n",
      "{'loss': 0.1686, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:56<04:16, 17.57it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 117.72it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 103.17it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 101.52it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 99.11it/s] \u001B[A\n",
      " 75% 56/75 [00:00<00:00, 99.08it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5409571528434753, 'eval_f1': 0.8400284393885532, 'eval_runtime': 2.6094, 'eval_samples_per_second': 229.941, 'eval_steps_per_second': 28.743, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:58<04:16, 17.57it/s]\n",
      "100% 75/75 [00:02<00:00, 99.76it/s]\u001B[A\n",
      "{'loss': 0.1334, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:20<05:10, 13.53it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 75.75it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 72.17it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 66.18it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 67.22it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 67.30it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 64.97it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 64.99it/s]\u001B[A\n",
      " 84% 63/75 [00:00<00:00, 76.85it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5965617895126343, 'eval_f1': 0.8531613223108976, 'eval_runtime': 2.889, 'eval_samples_per_second': 207.686, 'eval_steps_per_second': 25.961, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:23<05:10, 13.53it/s]\n",
      "100% 75/75 [00:02<00:00, 83.59it/s]\u001B[A\n",
      "{'loss': 0.0907, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:44<03:43, 17.48it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:00, 99.11it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 99.15it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 93.85it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 95.07it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 94.36it/s]\u001B[A\n",
      " 81% 61/75 [00:00<00:00, 96.53it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6476715207099915, 'eval_f1': 0.8698010849909584, 'eval_runtime': 2.6447, 'eval_samples_per_second': 226.868, 'eval_steps_per_second': 28.358, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:47<03:43, 17.48it/s]\n",
      "100% 75/75 [00:02<00:00, 99.58it/s]\u001B[A\n",
      "{'loss': 0.0635, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:09<04:23, 13.68it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 101.54it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 92.98it/s] \u001B[A\n",
      " 43% 32/75 [00:00<00:00, 95.39it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 93.90it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 89.78it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 91.37it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.632859468460083, 'eval_f1': 0.8639645935243421, 'eval_runtime': 2.6621, 'eval_samples_per_second': 225.387, 'eval_steps_per_second': 28.173, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:12<04:23, 13.68it/s]\n",
      "100% 75/75 [00:02<00:00, 96.47it/s]\u001B[A\n",
      "{'loss': 0.0452, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:33<03:14, 16.95it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 110.40it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 98.25it/s] \u001B[A\n",
      " 47% 35/75 [00:00<00:00, 99.52it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 95.84it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 97.05it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6863253712654114, 'eval_f1': 0.8751455858374098, 'eval_runtime': 2.6659, 'eval_samples_per_second': 225.067, 'eval_steps_per_second': 28.133, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:36<03:14, 16.95it/s]\n",
      "100% 75/75 [00:02<00:00, 97.81it/s]\u001B[A\n",
      "{'loss': 0.0404, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:58<03:11, 15.69it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 115.06it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 100.63it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 98.53it/s] \u001B[A\n",
      " 60% 45/75 [00:00<00:00, 95.95it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 96.16it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7655086517333984, 'eval_f1': 0.861933429523857, 'eval_runtime': 2.6163, 'eval_samples_per_second': 229.331, 'eval_steps_per_second': 28.666, 'epoch': 10.0}\n",
      " 50% 3000/6000 [04:01<03:11, 15.69it/s]\n",
      "100% 75/75 [00:02<00:00, 97.81it/s]\u001B[A\n",
      "{'loss': 0.0214, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:22<02:38, 17.04it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 111.01it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 100.92it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 100.19it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 97.62it/s] \u001B[A\n",
      " 76% 57/75 [00:00<00:00, 100.31it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.835119903087616, 'eval_f1': 0.8602375960866528, 'eval_runtime': 2.613, 'eval_samples_per_second': 229.625, 'eval_steps_per_second': 28.703, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:25<02:38, 17.04it/s]\n",
      "100% 75/75 [00:02<00:00, 101.26it/s]\u001B[A\n",
      "{'loss': 0.0149, 'learning_rate': 4.217543859649123e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:46<02:15, 17.71it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 108.59it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 98.45it/s] \u001B[A\n",
      " 43% 32/75 [00:00<00:00, 96.15it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 89.79it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 91.55it/s]\u001B[A\n",
      " 84% 63/75 [00:00<00:00, 94.82it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9076061248779297, 'eval_f1': 0.851313242845626, 'eval_runtime': 2.6366, 'eval_samples_per_second': 227.565, 'eval_steps_per_second': 28.446, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:49<02:15, 17.71it/s]\n",
      "100% 75/75 [00:02<00:00, 97.27it/s]\u001B[A\n",
      "{'loss': 0.0146, 'learning_rate': 3.692982456140351e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:10<02:02, 17.20it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 114.31it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 100.38it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 100.18it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 99.23it/s] \u001B[A\n",
      " 75% 56/75 [00:00<00:00, 97.85it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9161676168441772, 'eval_f1': 0.86125, 'eval_runtime': 2.6996, 'eval_samples_per_second': 222.256, 'eval_steps_per_second': 27.782, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:13<02:02, 17.20it/s]\n",
      "100% 75/75 [00:02<00:00, 95.24it/s]\u001B[A\n",
      "{'loss': 0.0101, 'learning_rate': 3.1666666666666667e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:35<01:44, 17.18it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 118.11it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 100.22it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 104.13it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 99.37it/s] \u001B[A\n",
      " 76% 57/75 [00:00<00:00, 101.26it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9297656416893005, 'eval_f1': 0.8656649584556446, 'eval_runtime': 2.6087, 'eval_samples_per_second': 230.001, 'eval_steps_per_second': 28.75, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:37<01:44, 17.18it/s]\n",
      "100% 75/75 [00:02<00:00, 102.37it/s]\u001B[A\n",
      "{'train_runtime': 341.4768, 'train_samples_per_second': 140.566, 'train_steps_per_second': 17.571, 'train_loss': 0.15095459637187775, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:41<01:44, 17.18it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 41 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 41 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      " 70% 4200/6000 [05:42<02:26, 12.28it/s]\n",
      "100% 75/75 [00:02<00:00, 28.23it/s]\n",
      "f1 0.7304480560294514\n",
      "efl_f1 0.8751455858374098\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6316    0.5000    0.5581        24\n",
      "           1     0.6863    0.7447    0.7143        47\n",
      "           2     0.9154    0.9225    0.9189       129\n",
      "\n",
      "    accuracy                         0.8300       200\n",
      "   macro avg     0.7444    0.7224    0.7304       200\n",
      "weighted avg     0.8275    0.8300    0.8275       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7255317508421205\n",
      "efl_f1 0.8807639383630722\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 26 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-90\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EFL + MNLI"
   ],
   "metadata": {
    "id": "yRfb4HUw3cTo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P-NAFZk_M127",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "255c42bd-513f-4720-d19e-fda8dfb34f14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2023-03-16 07:59:55.337099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 07:59:56.353295: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 07:59:56.353427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 07:59:56.353450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "Downloading (…)okenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 2.97kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 678/678 [00:00<00:00, 92.2kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 9.66MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 4.67MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100% 150/150 [00:00<00:00, 44.0kB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 42 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 42 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-80e72c26bd662f6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
      "Downloading data files: 100% 1/1 [00:00<00:00, 7503.23it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 1451.82it/s]\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-80e72c26bd662f6f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "100% 1/1 [00:00<00:00, 710.30it/s]\n",
      "                                       \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "Downloading pytorch_model.bin: 100% 501M/501M [00:18<00:00, 27.2MB/s]\n",
      "Some weights of the model checkpoint at textattack/roberta-base-MNLI were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at textattack/roberta-base-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1276: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/6000 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.4753, 'learning_rate': 9.9e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:36<14:28,  6.56it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.31it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 66.32it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.70it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 62.87it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 63.94it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 54.55it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 58.22it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 58.64it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.99it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 46.22it/s]\u001B[A\n",
      " 99% 74/75 [00:01<00:00, 41.07it/s]\u001B[A\n",
      "\n",
      "Downloading builder script: 100% 6.77k/6.77k [00:00<00:00, 3.63MB/s]\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4150741696357727, 'eval_f1': 0.8287279126449618, 'eval_runtime': 1.8451, 'eval_samples_per_second': 325.193, 'eval_steps_per_second': 40.649, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:38<14:28,  6.56it/s]\n",
      "100% 75/75 [00:01<00:00, 41.07it/s]\u001B[A\n",
      "{'loss': 0.3166, 'learning_rate': 9.480701754385966e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:19<10:57,  8.21it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.49it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 64.48it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.24it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.27it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.24it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 53.76it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 58.62it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 59.77it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 60.45it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5341819524765015, 'eval_f1': 0.8383410623211054, 'eval_runtime': 1.6076, 'eval_samples_per_second': 373.223, 'eval_steps_per_second': 46.653, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:20<10:57,  8.21it/s]\n",
      "100% 75/75 [00:01<00:00, 53.86it/s]\u001B[A\n",
      "{'loss': 0.216, 'learning_rate': 8.956140350877193e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:01<11:20,  7.49it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 46.94it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.02it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 41.76it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 41.55it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 41.26it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 41.28it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 40.71it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 39.52it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 36.06it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 36.77it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 38.31it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 39.50it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 40.24it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 40.72it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5796894431114197, 'eval_f1': 0.8666903661571277, 'eval_runtime': 2.235, 'eval_samples_per_second': 268.452, 'eval_steps_per_second': 33.557, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:03<11:20,  7.49it/s]\n",
      "100% 75/75 [00:02<00:00, 33.56it/s]\u001B[A\n",
      "{'loss': 0.1231, 'learning_rate': 8.42982456140351e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:43<08:18,  9.63it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 72.97it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 68.43it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 65.51it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 63.14it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 65.24it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 55.44it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 58.65it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 60.60it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 59.77it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8265925049781799, 'eval_f1': 0.8594491927825262, 'eval_runtime': 1.5648, 'eval_samples_per_second': 383.445, 'eval_steps_per_second': 47.931, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:45<08:18,  9.63it/s]\n",
      "100% 75/75 [00:01<00:00, 54.57it/s]\u001B[A\n",
      "{'loss': 0.0564, 'learning_rate': 7.903508771929826e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:24<07:54,  9.47it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 71.59it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 69.67it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 65.55it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 64.20it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 66.07it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 56.25it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 58.24it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 58.98it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 59.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9915482401847839, 'eval_f1': 0.8544730392156863, 'eval_runtime': 1.5841, 'eval_samples_per_second': 378.757, 'eval_steps_per_second': 47.345, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:26<07:54,  9.47it/s]\n",
      "100% 75/75 [00:01<00:00, 53.84it/s]\u001B[A\n",
      "{'loss': 0.0359, 'learning_rate': 7.380701754385966e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:06<09:07,  7.67it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.68it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 42.30it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 40.78it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 39.05it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 39.28it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 39.08it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 38.87it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 39.81it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 35.45it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 37.33it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 37.65it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 37.72it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 38.79it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 38.96it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 35.23it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0735446214675903, 'eval_f1': 0.8544730392156863, 'eval_runtime': 2.2886, 'eval_samples_per_second': 262.17, 'eval_steps_per_second': 32.771, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:09<09:07,  7.67it/s]\n",
      "100% 75/75 [00:02<00:00, 33.28it/s]\u001B[A\n",
      "{'loss': 0.0239, 'learning_rate': 6.854385964912281e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:49<06:39,  9.77it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 71.06it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 67.09it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 63.69it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.72it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 62.04it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 53.51it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 57.54it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 59.13it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 59.40it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0506800413131714, 'eval_f1': 0.86125, 'eval_runtime': 1.6052, 'eval_samples_per_second': 373.796, 'eval_steps_per_second': 46.725, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:50<06:39,  9.77it/s]\n",
      "100% 75/75 [00:01<00:00, 53.97it/s]\u001B[A\n",
      "{'loss': 0.0118, 'learning_rate': 6.329824561403509e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:30<06:07,  9.81it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 66.95it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 68.46it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.18it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 64.01it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 63.73it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 54.63it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 59.52it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 59.97it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 60.46it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1388989686965942, 'eval_f1': 0.8535560211614719, 'eval_runtime': 1.5782, 'eval_samples_per_second': 380.172, 'eval_steps_per_second': 47.521, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:32<06:07,  9.81it/s]\n",
      "100% 75/75 [00:01<00:00, 54.17it/s]\u001B[A\n",
      "{'train_runtime': 340.829, 'train_samples_per_second': 140.833, 'train_steps_per_second': 17.604, 'train_loss': 0.15736766984065373, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:40<06:07,  9.81it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      " 40% 2400/6000 [05:41<08:31,  7.03it/s]\n",
      "100% 75/75 [00:02<00:00, 34.22it/s]\n",
      "f1 0.6751827485380116\n",
      "efl_f1 0.8666903661571277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3333    0.5000    0.4000        12\n",
      "           1     0.8039    0.6508    0.7193        63\n",
      "           2     0.8855    0.9280    0.9063       125\n",
      "\n",
      "    accuracy                         0.8150       200\n",
      "   macro avg     0.6743    0.6929    0.6752       200\n",
      "weighted avg     0.8267    0.8150    0.8170       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "Some weights of the model checkpoint at textattack/roberta-base-MNLI were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at textattack/roberta-base-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.4717, 'learning_rate': 9.866666666666668e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:35<12:11,  7.80it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 50.69it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 43.98it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 43.00it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 42.99it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 42.36it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 41.81it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 41.76it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 40.83it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 41.08it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 40.57it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 41.00it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 41.11it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 41.67it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3659142553806305, 'eval_f1': 0.8225355006833941, 'eval_runtime': 2.0773, 'eval_samples_per_second': 288.838, 'eval_steps_per_second': 36.105, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:37<12:11,  7.80it/s]\n",
      "100% 75/75 [00:02<00:00, 41.57it/s]\u001B[A\n",
      "{'loss': 0.3382, 'learning_rate': 9.482456140350878e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:17<09:41,  9.29it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.03it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.22it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.38it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 62.81it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 62.60it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 61.78it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 63.64it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 63.93it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 62.62it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5519567728042603, 'eval_f1': 0.8483141333150228, 'eval_runtime': 1.4915, 'eval_samples_per_second': 402.275, 'eval_steps_per_second': 50.284, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:18<09:41,  9.29it/s]\n",
      "100% 75/75 [00:01<00:00, 61.13it/s]\u001B[A\n",
      "{'loss': 0.2334, 'learning_rate': 8.956140350877193e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:58<08:44,  9.72it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.77it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.90it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.32it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.35it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.44it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 61.43it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.08it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 61.03it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 60.52it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4175748825073242, 'eval_f1': 0.8630925025469294, 'eval_runtime': 1.4907, 'eval_samples_per_second': 402.504, 'eval_steps_per_second': 50.313, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:00<08:44,  9.72it/s]\n",
      "100% 75/75 [00:01<00:00, 59.55it/s]\u001B[A\n",
      "{'loss': 0.1678, 'learning_rate': 8.42982456140351e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:40<10:12,  7.83it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 51.00it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 45.75it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 45.01it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 42.88it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 42.47it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 42.83it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 42.84it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 42.50it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 42.45it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 42.54it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 42.71it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 42.62it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 42.38it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6954847574234009, 'eval_f1': 0.8526281316522024, 'eval_runtime': 2.0083, 'eval_samples_per_second': 298.755, 'eval_steps_per_second': 37.344, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:42<10:12,  7.83it/s]\n",
      "100% 75/75 [00:01<00:00, 41.80it/s]\u001B[A\n",
      "{'loss': 0.0831, 'learning_rate': 7.905263157894737e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:23<07:45,  9.66it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 67.60it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.94it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.69it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 62.05it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.94it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 62.36it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 63.58it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 64.16it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 63.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7956347465515137, 'eval_f1': 0.8677431950724893, 'eval_runtime': 1.5261, 'eval_samples_per_second': 393.164, 'eval_steps_per_second': 49.146, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:24<07:45,  9.66it/s]\n",
      "100% 75/75 [00:01<00:00, 60.55it/s]\u001B[A\n",
      "{'loss': 0.0461, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:05<07:17,  9.61it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 67.66it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.00it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.44it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.37it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.31it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 58.60it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 60.36it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 60.42it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 60.23it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8454791307449341, 'eval_f1': 0.8776627947464012, 'eval_runtime': 1.5209, 'eval_samples_per_second': 394.502, 'eval_steps_per_second': 49.313, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:06<07:17,  9.61it/s]\n",
      "100% 75/75 [00:01<00:00, 58.10it/s]\u001B[A\n",
      "{'loss': 0.0332, 'learning_rate': 6.8526315789473685e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:47<09:43,  6.68it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 50.25it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 43.20it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 41.93it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 41.76it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 40.97it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 40.67it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 41.16it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 38.68it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 39.33it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 39.81it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 39.73it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 39.76it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 41.58it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0673989057540894, 'eval_f1': 0.8565105986489634, 'eval_runtime': 2.0084, 'eval_samples_per_second': 298.744, 'eval_steps_per_second': 37.343, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:49<09:43,  6.68it/s]\n",
      "100% 75/75 [00:01<00:00, 45.49it/s]\u001B[A\n",
      "{'loss': 0.0173, 'learning_rate': 6.326315789473685e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:29<06:12,  9.67it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 67.52it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.69it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 59.93it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.24it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.61it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.00it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 53.97it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 51.06it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 49.70it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 47.44it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9743276238441467, 'eval_f1': 0.8734349989452916, 'eval_runtime': 1.6914, 'eval_samples_per_second': 354.73, 'eval_steps_per_second': 44.341, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:31<06:12,  9.67it/s]\n",
      "100% 75/75 [00:01<00:00, 47.83it/s]\u001B[A\n",
      "{'loss': 0.0065, 'learning_rate': 5.8e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:11<05:38,  9.75it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 69.32it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.61it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.99it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 61.87it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 62.76it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.74it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 62.61it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 63.66it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 62.87it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0337369441986084, 'eval_f1': 0.8735749665854233, 'eval_runtime': 1.478, 'eval_samples_per_second': 405.947, 'eval_steps_per_second': 50.743, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:13<05:38,  9.75it/s]\n",
      "100% 75/75 [00:01<00:00, 61.34it/s]\u001B[A\n",
      "{'loss': 0.0009, 'learning_rate': 5.275438596491228e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:53<06:34,  7.61it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 47.46it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 44.28it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 42.20it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 41.50it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 40.93it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 45.07it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 48.88it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 50.40it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 55.16it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 56.86it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 58.40it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.052396535873413, 'eval_f1': 0.87625, 'eval_runtime': 1.7168, 'eval_samples_per_second': 349.496, 'eval_steps_per_second': 43.687, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:55<06:34,  7.61it/s]\n",
      "100% 75/75 [00:01<00:00, 57.06it/s]\u001B[A\n",
      "{'loss': 0.0044, 'learning_rate': 4.749122807017544e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:35<05:25,  8.29it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.17it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.44it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 59.57it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 52.78it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 49.36it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 49.24it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 48.35it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 47.73it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 48.32it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 48.27it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 47.55it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 47.19it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.099870204925537, 'eval_f1': 0.8725, 'eval_runtime': 1.8506, 'eval_samples_per_second': 324.216, 'eval_steps_per_second': 40.527, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:37<05:25,  8.29it/s]\n",
      "100% 75/75 [00:01<00:00, 46.21it/s]\u001B[A\n",
      "{'train_runtime': 466.1995, 'train_samples_per_second': 102.96, 'train_steps_per_second': 12.87, 'train_loss': 0.1275232057318543, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:46<05:25,  8.29it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      " 55% 3300/6000 [07:46<06:21,  7.07it/s]\n",
      "100% 75/75 [00:01<00:00, 49.13it/s]\n",
      "f1 0.7019296231693923\n",
      "efl_f1 0.8776627947464012\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4615    0.4286    0.4444        14\n",
      "           1     0.7097    0.8148    0.7586        54\n",
      "           2     0.9280    0.8788    0.9027       132\n",
      "\n",
      "    accuracy                         0.8300       200\n",
      "   macro avg     0.6997    0.7074    0.7019       200\n",
      "weighted avg     0.8364    0.8300    0.8317       200\n",
      "\n",
      "                                                   \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "Some weights of the model checkpoint at textattack/roberta-base-MNLI were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at textattack/roberta-base-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.4412, 'learning_rate': 9.9e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:34<09:49,  9.66it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 69.86it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 55.51it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 54.36it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.56it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.66it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 58.01it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.73it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.28it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 60.72it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.34827977418899536, 'eval_f1': 0.8152176379627258, 'eval_runtime': 1.5167, 'eval_samples_per_second': 395.584, 'eval_steps_per_second': 49.448, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:36<09:49,  9.66it/s]\n",
      "100% 75/75 [00:01<00:00, 61.57it/s]\u001B[A\n",
      "{'loss': 0.3157, 'learning_rate': 9.480701754385966e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:16<10:15,  8.77it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 66.46it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 55.56it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 55.03it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.40it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.50it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 58.68it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 62.62it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 61.78it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 61.99it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4849929213523865, 'eval_f1': 0.8575, 'eval_runtime': 1.5458, 'eval_samples_per_second': 388.158, 'eval_steps_per_second': 48.52, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:18<10:15,  8.77it/s]\n",
      "100% 75/75 [00:01<00:00, 61.58it/s]\u001B[A\n",
      "{'loss': 0.2228, 'learning_rate': 8.956140350877193e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:58<10:36,  8.01it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 59.32it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 46.82it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 46.56it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 44.93it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 45.14it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 45.99it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 46.94it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 45.20it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 46.88it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 47.31it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 46.56it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 46.09it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 46.67it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5740997195243835, 'eval_f1': 0.85375, 'eval_runtime': 1.8948, 'eval_samples_per_second': 316.658, 'eval_steps_per_second': 39.582, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:00<10:36,  8.01it/s]\n",
      "100% 75/75 [00:01<00:00, 46.74it/s]\u001B[A\n",
      "{'loss': 0.1178, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:40<08:26,  9.48it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 67.64it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 54.99it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 54.14it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.83it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.82it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 57.65it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 59.46it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 59.71it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 61.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7056345343589783, 'eval_f1': 0.8573597608715635, 'eval_runtime': 1.5197, 'eval_samples_per_second': 394.806, 'eval_steps_per_second': 49.351, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:42<08:26,  9.48it/s]\n",
      "100% 75/75 [00:01<00:00, 62.67it/s]\u001B[A\n",
      "{'loss': 0.065, 'learning_rate': 7.905263157894737e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:23<08:09,  9.20it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 67.50it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 54.76it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 53.93it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.61it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.10it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 58.15it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 59.56it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 57.97it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 57.99it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 59.01it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8500043749809265, 'eval_f1': 0.8630925025469294, 'eval_runtime': 1.5174, 'eval_samples_per_second': 395.406, 'eval_steps_per_second': 49.426, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:24<08:09,  9.20it/s]\n",
      "100% 75/75 [00:01<00:00, 59.74it/s]\u001B[A\n",
      "{'loss': 0.0379, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:05<08:36,  8.13it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 59.21it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 46.57it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 45.25it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 44.38it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 45.47it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 45.78it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 45.70it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 44.26it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 45.16it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 45.10it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 45.15it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 44.75it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 45.05it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9341681003570557, 'eval_f1': 0.8594491927825262, 'eval_runtime': 1.9403, 'eval_samples_per_second': 309.23, 'eval_steps_per_second': 38.654, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:07<08:36,  8.13it/s]\n",
      "100% 75/75 [00:01<00:00, 43.67it/s]\u001B[A\n",
      "{'loss': 0.037, 'learning_rate': 6.8526315789473685e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:47<06:49,  9.53it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 70.43it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 57.16it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 54.80it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 57.89it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 60.33it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 57.71it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 60.30it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 60.27it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 60.05it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8455727100372314, 'eval_f1': 0.871521960927561, 'eval_runtime': 1.4955, 'eval_samples_per_second': 401.2, 'eval_steps_per_second': 50.15, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:48<06:49,  9.53it/s]\n",
      "100% 75/75 [00:01<00:00, 61.11it/s]\u001B[A\n",
      "{'loss': 0.0247, 'learning_rate': 6.3280701754385966e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:29<07:50,  7.66it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 72.03it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 57.64it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 54.98it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 57.93it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 60.10it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 58.71it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 62.09it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 61.52it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 61.94it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9261124730110168, 'eval_f1': 0.8721788493590146, 'eval_runtime': 1.4992, 'eval_samples_per_second': 400.21, 'eval_steps_per_second': 50.026, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:31<07:50,  7.66it/s]\n",
      "100% 75/75 [00:01<00:00, 60.87it/s]\u001B[A\n",
      "{'loss': 0.0043, 'learning_rate': 5.801754385964913e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:11<06:32,  8.40it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 59.63it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 48.85it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 47.77it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 45.51it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 47.52it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 45.47it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 45.80it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 45.65it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 47.22it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 47.80it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 47.86it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 46.71it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9865142703056335, 'eval_f1': 0.8759382949661023, 'eval_runtime': 1.8807, 'eval_samples_per_second': 319.026, 'eval_steps_per_second': 39.878, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:13<06:32,  8.40it/s]\n",
      "100% 75/75 [00:01<00:00, 46.53it/s]\u001B[A\n",
      "{'loss': 0.0111, 'learning_rate': 5.275438596491228e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:53<05:25,  9.23it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 67.63it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 56.46it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 55.00it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.79it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.22it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 58.28it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.23it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 59.65it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 61.05it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.2259399890899658, 'eval_f1': 0.8467606912795482, 'eval_runtime': 1.5002, 'eval_samples_per_second': 399.944, 'eval_steps_per_second': 49.993, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:55<05:25,  9.23it/s]\n",
      "100% 75/75 [00:01<00:00, 62.63it/s]\u001B[A\n",
      "{'loss': 0.0105, 'learning_rate': 4.749122807017544e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:35<04:57,  9.07it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 68.75it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 56.62it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 55.41it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.92it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.80it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 58.79it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.06it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.60it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 61.53it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.1809234619140625, 'eval_f1': 0.8607893055674742, 'eval_runtime': 1.5075, 'eval_samples_per_second': 398.012, 'eval_steps_per_second': 49.751, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:37<04:57,  9.07it/s]\n",
      "100% 75/75 [00:01<00:00, 63.03it/s]\u001B[A\n",
      "{'loss': 0.0182, 'learning_rate': 4.22280701754386e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:17<05:01,  7.96it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 51.31it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 43.33it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 42.91it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 41.86it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 42.26it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 42.25it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 41.89it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 41.09it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 41.79it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 41.73it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 41.72it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 42.51it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 42.81it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0848815441131592, 'eval_f1': 0.859198558193236, 'eval_runtime': 2.0389, 'eval_samples_per_second': 294.27, 'eval_steps_per_second': 36.784, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:19<05:01,  7.96it/s]\n",
      "100% 75/75 [00:02<00:00, 42.53it/s]\u001B[A\n",
      "{'loss': 0.0071, 'learning_rate': 3.696491228070176e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:59<04:09,  8.43it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 67.55it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 56.89it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 54.66it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.38it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.16it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.54it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 58.14it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 54.87it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 50.68it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 49.74it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0987986326217651, 'eval_f1': 0.8594491927825262, 'eval_runtime': 1.6725, 'eval_samples_per_second': 358.745, 'eval_steps_per_second': 44.843, 'epoch': 13.0}\n",
      " 65% 3900/6000 [09:00<04:09,  8.43it/s]\n",
      "100% 75/75 [00:01<00:00, 48.68it/s]\u001B[A\n",
      "{'loss': 0.0026, 'learning_rate': 3.170175438596492e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:40<03:08,  9.53it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 70.64it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 58.24it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 56.56it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 59.55it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 61.44it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 58.33it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 61.19it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 61.20it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 61.29it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0520182847976685, 'eval_f1': 0.8735749665854233, 'eval_runtime': 1.4988, 'eval_samples_per_second': 400.312, 'eval_steps_per_second': 50.039, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:42<03:08,  9.53it/s]\n",
      "100% 75/75 [00:01<00:00, 61.48it/s]\u001B[A\n",
      "{'train_runtime': 590.7136, 'train_samples_per_second': 81.258, 'train_steps_per_second': 10.157, 'train_loss': 0.09398900107258842, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:50<03:08,  9.53it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      " 70% 4200/6000 [09:51<04:13,  7.11it/s]\n",
      "100% 75/75 [00:01<00:00, 49.51it/s]\n",
      "f1 0.7142390788224122\n",
      "efl_f1 0.8759382949661023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6364    0.4375    0.5185        16\n",
      "           1     0.6774    0.7636    0.7179        55\n",
      "           2     0.9134    0.8992    0.9063       129\n",
      "\n",
      "    accuracy                         0.8250       200\n",
      "   macro avg     0.7424    0.7001    0.7142       200\n",
      "weighted avg     0.8263    0.8250    0.8234       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "Some weights of the model checkpoint at textattack/roberta-base-MNLI were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at textattack/roberta-base-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.4828, 'learning_rate': 9.9e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:34<09:52,  9.63it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.86it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.16it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.53it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.55it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.79it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.37it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.09it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.76it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.31it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3677937984466553, 'eval_f1': 0.8607893055674742, 'eval_runtime': 1.5295, 'eval_samples_per_second': 392.292, 'eval_steps_per_second': 49.037, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:35<09:52,  9.63it/s]\n",
      "100% 75/75 [00:01<00:00, 59.67it/s]\u001B[A\n",
      "{'loss': 0.307, 'learning_rate': 9.480701754385966e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:16<10:24,  8.65it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.22it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 64.94it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.94it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 55.89it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.06it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.21it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 59.03it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.72it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 55.73it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5062361359596252, 'eval_f1': 0.8735749665854233, 'eval_runtime': 1.5645, 'eval_samples_per_second': 383.503, 'eval_steps_per_second': 47.938, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:17<10:24,  8.65it/s]\n",
      "100% 75/75 [00:01<00:00, 58.00it/s]\u001B[A\n",
      "{'loss': 0.2331, 'learning_rate': 8.956140350877193e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:58<11:02,  7.70it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 49.29it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 45.68it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 44.22it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 43.09it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 39.92it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 40.67it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 40.46it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 40.56it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 41.24it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 41.39it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 38.69it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 36.99it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 38.08it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 39.48it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.6355429291725159, 'eval_f1': 0.8600301147328908, 'eval_runtime': 2.1681, 'eval_samples_per_second': 276.735, 'eval_steps_per_second': 34.592, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:00<11:02,  7.70it/s]\n",
      "100% 75/75 [00:02<00:00, 37.58it/s]\u001B[A\n",
      "{'loss': 0.1497, 'learning_rate': 8.431578947368422e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:40<08:16,  9.67it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.10it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.17it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.77it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.71it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.26it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.03it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.01it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 61.53it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.40it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5841769576072693, 'eval_f1': 0.870060606060606, 'eval_runtime': 1.6007, 'eval_samples_per_second': 374.833, 'eval_steps_per_second': 46.854, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:41<08:16,  9.67it/s]\n",
      "100% 75/75 [00:01<00:00, 59.97it/s]\u001B[A\n",
      "{'loss': 0.0943, 'learning_rate': 7.905263157894737e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:22<08:09,  9.20it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.74it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 63.70it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.79it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.95it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 58.37it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 58.87it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 59.97it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 60.41it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 54.76it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 55.85it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6777963042259216, 'eval_f1': 0.870632300034498, 'eval_runtime': 1.5822, 'eval_samples_per_second': 379.226, 'eval_steps_per_second': 47.403, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:23<08:09,  9.20it/s]\n",
      "100% 75/75 [00:01<00:00, 53.51it/s]\u001B[A\n",
      "{'loss': 0.0589, 'learning_rate': 7.378947368421053e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:04<07:11,  9.73it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.37it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 64.22it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.06it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.24it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.09it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 59.75it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 61.38it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 60.79it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.32it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7354461550712585, 'eval_f1': 0.8845975576302769, 'eval_runtime': 1.5536, 'eval_samples_per_second': 386.194, 'eval_steps_per_second': 48.274, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:05<07:11,  9.73it/s]\n",
      "100% 75/75 [00:01<00:00, 58.61it/s]\u001B[A\n",
      "{'loss': 0.034, 'learning_rate': 6.8526315789473685e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:45<08:08,  7.98it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 49.95it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 44.85it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 41.28it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 40.45it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 38.47it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 39.48it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 40.87it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 41.53it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 42.34it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 42.82it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 42.94it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 40.44it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 40.77it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8115714192390442, 'eval_f1': 0.8854814939971651, 'eval_runtime': 2.0982, 'eval_samples_per_second': 285.956, 'eval_steps_per_second': 35.745, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:47<08:08,  7.98it/s]\n",
      "100% 75/75 [00:02<00:00, 39.25it/s]\u001B[A\n",
      "{'loss': 0.0232, 'learning_rate': 6.326315789473685e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:27<06:25,  9.35it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.38it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 64.48it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.34it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.04it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.10it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.18it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 52.51it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 50.71it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 46.28it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 46.63it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8270047903060913, 'eval_f1': 0.8843226031145828, 'eval_runtime': 1.7527, 'eval_samples_per_second': 342.328, 'eval_steps_per_second': 42.791, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:29<06:25,  9.35it/s]\n",
      "100% 75/75 [00:01<00:00, 44.72it/s]\u001B[A\n",
      "{'loss': 0.0114, 'learning_rate': 5.8e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:09<06:12,  8.86it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 66.33it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.83it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 61.53it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.42it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.15it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 58.59it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 60.15it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 59.13it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 55.28it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 58.86it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.800995409488678, 'eval_f1': 0.8831599577868234, 'eval_runtime': 1.568, 'eval_samples_per_second': 382.659, 'eval_steps_per_second': 47.832, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:10<06:12,  8.86it/s]\n",
      "100% 75/75 [00:01<00:00, 55.44it/s]\u001B[A\n",
      "{'loss': 0.0353, 'learning_rate': 5.273684210526317e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:51<05:06,  9.78it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.21it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.59it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.43it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 55.79it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 61.02it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 59.19it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 60.80it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 61.09it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 55.47it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7978224754333496, 'eval_f1': 0.8921649309004229, 'eval_runtime': 1.5249, 'eval_samples_per_second': 393.474, 'eval_steps_per_second': 49.184, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:52<05:06,  9.78it/s]\n",
      "100% 75/75 [00:01<00:00, 54.35it/s]\u001B[A\n",
      "{'loss': 0.0243, 'learning_rate': 4.749122807017544e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:32<05:38,  7.98it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 45.36it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 43.57it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 42.29it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 41.85it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 38.82it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 39.77it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 40.66it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 40.54it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 41.02it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 40.81it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 40.39it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 38.32it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 45.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8941135406494141, 'eval_f1': 0.8848966113688181, 'eval_runtime': 2.006, 'eval_samples_per_second': 299.108, 'eval_steps_per_second': 37.388, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:34<05:38,  7.98it/s]\n",
      "100% 75/75 [00:01<00:00, 45.11it/s]\u001B[A\n",
      "{'loss': 0.0029, 'learning_rate': 4.22280701754386e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:14<04:08,  9.67it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.37it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 51.28it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 49.73it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 44.84it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 45.64it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 47.70it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 48.78it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 49.62it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 50.98it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 47.62it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 46.39it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.908045768737793, 'eval_f1': 0.8842940196447256, 'eval_runtime': 1.8681, 'eval_samples_per_second': 321.188, 'eval_steps_per_second': 40.149, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:16<04:08,  9.67it/s]\n",
      "100% 75/75 [00:01<00:00, 44.23it/s]\u001B[A\n",
      "{'loss': 0.0093, 'learning_rate': 3.696491228070176e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:56<03:40,  9.50it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.07it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.43it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 62.73it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.45it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 58.77it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 58.22it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 60.42it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 62.32it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 57.02it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9337981939315796, 'eval_f1': 0.8937805730258561, 'eval_runtime': 1.5446, 'eval_samples_per_second': 388.438, 'eval_steps_per_second': 48.555, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:58<03:40,  9.50it/s]\n",
      "100% 75/75 [00:01<00:00, 61.27it/s]\u001B[A\n",
      "{'loss': 0.0132, 'learning_rate': 3.171929824561404e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:38<03:21,  8.93it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.42it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 66.05it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.22it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.20it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 62.32it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.82it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 62.54it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 62.43it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 57.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8961122632026672, 'eval_f1': 0.895, 'eval_runtime': 1.4934, 'eval_samples_per_second': 401.773, 'eval_steps_per_second': 50.222, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:39<03:21,  8.93it/s]\n",
      "100% 75/75 [00:01<00:00, 56.19it/s]\u001B[A\n",
      "{'loss': 0.0035, 'learning_rate': 2.6456140350877195e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:20<03:10,  7.86it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 50.86it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 44.92it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 43.13it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 42.68it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 38.41it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 40.16it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 41.02it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 41.70it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 42.33it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 42.10it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 45.35it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 44.99it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9668465256690979, 'eval_f1': 0.8854814939971651, 'eval_runtime': 1.9596, 'eval_samples_per_second': 306.179, 'eval_steps_per_second': 38.272, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:21<03:10,  7.86it/s]\n",
      "100% 75/75 [00:01<00:00, 50.44it/s]\u001B[A\n",
      "{'loss': 0.0076, 'learning_rate': 2.119298245614035e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [11:01<02:01,  9.91it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.88it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 48.18it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 48.63it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 43.57it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 46.05it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 47.93it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 47.17it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 48.42it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 48.43it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 47.70it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 44.52it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 44.39it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9389658570289612, 'eval_f1': 0.8972573951324911, 'eval_runtime': 1.9726, 'eval_samples_per_second': 304.166, 'eval_steps_per_second': 38.021, 'epoch': 16.0}\n",
      " 80% 4800/6000 [11:03<02:01,  9.91it/s]\n",
      "100% 75/75 [00:01<00:00, 42.30it/s]\u001B[A\n",
      "{'loss': 0.0047, 'learning_rate': 1.592982456140351e-06, 'epoch': 17.0}\n",
      " 85% 5100/6000 [11:42<02:06,  7.10it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 66.03it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.71it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 63.79it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.76it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 61.89it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 60.71it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 62.47it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 62.07it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 57.49it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9925355911254883, 'eval_f1': 0.8909760773944535, 'eval_runtime': 1.5044, 'eval_samples_per_second': 398.83, 'eval_steps_per_second': 49.854, 'epoch': 17.0}\n",
      " 85% 5100/6000 [11:44<02:06,  7.10it/s]\n",
      "100% 75/75 [00:01<00:00, 59.97it/s]\u001B[A\n",
      "{'loss': 0.0016, 'learning_rate': 1.066666666666667e-06, 'epoch': 18.0}\n",
      " 90% 5400/6000 [12:24<01:02,  9.59it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.27it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 65.33it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 64.25it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 56.90it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.79it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 56.20it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 58.87it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 61.34it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 55.72it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0163244009017944, 'eval_f1': 0.8793909241670436, 'eval_runtime': 1.5238, 'eval_samples_per_second': 393.762, 'eval_steps_per_second': 49.22, 'epoch': 18.0}\n",
      " 90% 5400/6000 [12:26<01:02,  9.59it/s]\n",
      "100% 75/75 [00:01<00:00, 60.25it/s]\u001B[A\n",
      "{'loss': 0.004, 'learning_rate': 5.403508771929825e-07, 'epoch': 19.0}\n",
      " 95% 5700/6000 [13:06<00:40,  7.46it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 47.30it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.04it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 41.91it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 41.92it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 38.94it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 46.80it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 52.04it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 53.74it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 57.12it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 57.91it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 54.68it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9780247211456299, 'eval_f1': 0.8909760773944535, 'eval_runtime': 1.7462, 'eval_samples_per_second': 343.596, 'eval_steps_per_second': 42.95, 'epoch': 19.0}\n",
      " 95% 5700/6000 [13:08<00:40,  7.46it/s]\n",
      "100% 75/75 [00:01<00:00, 53.86it/s]\u001B[A\n",
      "{'loss': 0.0, 'learning_rate': 1.4035087719298246e-08, 'epoch': 20.0}\n",
      "100% 6000/6000 [13:47<00:00,  8.82it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 58.64it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 51.87it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 48.88it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 44.49it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:01, 45.22it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 46.11it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 44.27it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 43.20it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 41.45it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 41.30it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 41.17it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 39.20it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 39.80it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9794989228248596, 'eval_f1': 0.8909760773944535, 'eval_runtime': 2.1461, 'eval_samples_per_second': 279.574, 'eval_steps_per_second': 34.947, 'epoch': 20.0}\n",
      "100% 6000/6000 [13:50<00:00,  8.82it/s]\n",
      "100% 75/75 [00:02<00:00, 38.00it/s]\u001B[A\n",
      "{'train_runtime': 838.4538, 'train_samples_per_second': 57.248, 'train_steps_per_second': 7.156, 'train_loss': 0.07503788510958354, 'epoch': 20.0}\n",
      "100% 6000/6000 [13:58<00:00,  8.82it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "100% 6000/6000 [13:58<00:00,  7.15it/s]\n",
      "100% 75/75 [00:01<00:00, 47.56it/s]\n",
      "f1 0.7397995109600307\n",
      "efl_f1 0.8972573951324911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4375    0.5385    0.4828        13\n",
      "           1     0.8846    0.7541    0.8142        61\n",
      "           2     0.9015    0.9444    0.9225       126\n",
      "\n",
      "    accuracy                         0.8600       200\n",
      "   macro avg     0.7412    0.7457    0.7398       200\n",
      "weighted avg     0.8662    0.8600    0.8609       200\n",
      "\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "Some weights of the model checkpoint at textattack/roberta-base-MNLI were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at textattack/roberta-base-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.4776, 'learning_rate': 9.9e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:33<10:04,  9.43it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 77.38it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 62.71it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 58.89it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 59.05it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.08it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 53.65it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 49.93it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 49.06it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 48.92it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 48.14it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 49.53it/s]\u001B[A\n",
      " 99% 74/75 [00:01<00:00, 49.52it/s]\u001B[A\n",
      "{'eval_loss': 0.4870377480983734, 'eval_f1': 0.8122148843414401, 'eval_runtime': 1.7152, 'eval_samples_per_second': 349.808, 'eval_steps_per_second': 43.726, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:35<10:04,  9.43it/s]\n",
      "{'loss': 0.3237, 'learning_rate': 9.480701754385966e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:15<10:08,  8.88it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 77.50it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 64.89it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 59.77it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 62.07it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 60.02it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 57.28it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 54.19it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 56.27it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 57.39it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3788749575614929, 'eval_f1': 0.86125, 'eval_runtime': 1.5149, 'eval_samples_per_second': 396.067, 'eval_steps_per_second': 49.508, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:16<10:08,  8.88it/s]\n",
      "100% 75/75 [00:01<00:00, 59.14it/s]\u001B[A\n",
      "{'loss': 0.2508, 'learning_rate': 8.956140350877193e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:57<09:02,  9.40it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.98it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 59.54it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 57.13it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 55.24it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 58.27it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 53.73it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 53.48it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 52.14it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 54.69it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 56.92it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3341021239757538, 'eval_f1': 0.9155135114376785, 'eval_runtime': 1.5834, 'eval_samples_per_second': 378.924, 'eval_steps_per_second': 47.366, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:58<09:02,  9.40it/s]\n",
      "100% 75/75 [00:01<00:00, 57.56it/s]\u001B[A\n",
      "{'loss': 0.1585, 'learning_rate': 8.42982456140351e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:39<09:55,  8.06it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 53.35it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 44.45it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 42.68it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 41.68it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 41.04it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 41.72it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 41.80it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 41.40it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 41.58it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 44.99it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 49.54it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 53.83it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5039039850234985, 'eval_f1': 0.8945387026982112, 'eval_runtime': 1.8277, 'eval_samples_per_second': 328.282, 'eval_steps_per_second': 41.035, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:41<09:55,  8.06it/s]\n",
      "100% 75/75 [00:01<00:00, 55.42it/s]\u001B[A\n",
      "{'loss': 0.1012, 'learning_rate': 7.903508771929826e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:20<08:14,  9.11it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 57.56it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 52.55it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 49.55it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 48.03it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 49.06it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 48.75it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 47.39it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 46.91it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 45.84it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 46.71it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 45.92it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 46.96it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 44.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5337231159210205, 'eval_f1': 0.8989292218503581, 'eval_runtime': 1.8902, 'eval_samples_per_second': 317.42, 'eval_steps_per_second': 39.677, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:22<08:14,  9.11it/s]\n",
      "100% 75/75 [00:01<00:00, 44.04it/s]\u001B[A\n",
      "{'loss': 0.0666, 'learning_rate': 7.37719298245614e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:02<08:02,  8.71it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 72.82it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 63.05it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 59.40it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.71it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 60.01it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 56.05it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 53.08it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 54.93it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 55.25it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 58.08it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6872178912162781, 'eval_f1': 0.8868687901671721, 'eval_runtime': 1.5444, 'eval_samples_per_second': 388.498, 'eval_steps_per_second': 48.562, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:03<08:02,  8.71it/s]\n",
      "100% 75/75 [00:01<00:00, 58.33it/s]\u001B[A\n",
      "{'loss': 0.049, 'learning_rate': 6.850877192982457e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:43<07:03,  9.21it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 79.59it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 63.88it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 58.10it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 61.79it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 59.11it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 55.61it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 53.55it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 57.31it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.70it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6854197382926941, 'eval_f1': 0.8880876583448987, 'eval_runtime': 1.5345, 'eval_samples_per_second': 390.999, 'eval_steps_per_second': 48.875, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:45<07:03,  9.21it/s]\n",
      "100% 75/75 [00:01<00:00, 58.98it/s]\u001B[A\n",
      "{'loss': 0.0289, 'learning_rate': 6.326315789473685e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:26<07:33,  7.93it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 52.46it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 48.36it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 50.85it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 50.35it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 57.98it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 54.56it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 55.11it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 52.21it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 56.94it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 57.25it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6678117513656616, 'eval_f1': 0.8974358974358975, 'eval_runtime': 1.6337, 'eval_samples_per_second': 367.264, 'eval_steps_per_second': 45.908, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:27<07:33,  7.93it/s]\n",
      "100% 75/75 [00:01<00:00, 58.99it/s]\u001B[A\n",
      "{'train_runtime': 336.1008, 'train_samples_per_second': 142.814, 'train_steps_per_second': 17.852, 'train_loss': 0.18202646573384604, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:35<07:33,  7.93it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      " 40% 2400/6000 [05:36<08:24,  7.13it/s]\n",
      "100% 75/75 [00:01<00:00, 40.50it/s]\n",
      "f1 0.8146348777927725\n",
      "efl_f1 0.9155135114376785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7619    0.6667    0.7111        24\n",
      "           1     0.7708    0.7872    0.7789        47\n",
      "           2     0.9466    0.9612    0.9538       129\n",
      "\n",
      "    accuracy                         0.8850       200\n",
      "   macro avg     0.8264    0.8050    0.8146       200\n",
      "weighted avg     0.8831    0.8850    0.8836       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7356057482639761\n",
      "efl_f1 0.8866124724879603\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 26 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-84\n"
     ]
    }
   ],
   "source": [
    "!python efl.py --base-model=textattack/roberta-base-MNLI --cross-validation=5 --config-name=efl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-MdY9w-wRzuI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2d7ff069-403a-4330-c2ef-b7484dce2976"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2023-03-16 16:09:11.106062: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 16:09:12.984243: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 16:09:12.984407: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 16:09:12.984430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "Downloading (…)okenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 7.02kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 630/630 [00:00<00:00, 74.6kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 344kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 40.1kB/s]\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 22 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 22 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 546.42it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-83265ee7ba365dcc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cbc2cd51716047b4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-85c2440d4862542c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3ad14430404674f7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-872b357518bea1c7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-34edefb2d6f82357.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-049cd683442841e5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-214017cccb23a7dd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-027ca9f049962911.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c524627fecc7af2b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ac52133acdf98f56.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-15bfcaf344fd118e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-64f71e3c038aed86.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2f90841d80697968.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-27fcc4de9593ef64.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22778a4737bf1b06.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-99d066bd6b9c335d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d9c9a94d10a0f608.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-748b5a246ee06e2b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8344082634a0f403.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0f845d6ec688f0d1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e9e3c99a73c9e8a8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-866eed363c382697.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7f98c8c9021a8576.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1a203f297b61dc35.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22bf99d1ddcb7162.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f1812c603afc100a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7588590f7eca1f61.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-945b99ee96fc5717.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dbf66b68a7a0a6b0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1079b40689d85528.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5e8f99677ff049eb.arrow\n",
      "                                                   \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "Downloading pytorch_model.bin: 100% 438M/438M [00:01<00:00, 284MB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at textattack/bert-base-uncased-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1276: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/6000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.4295, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:33<11:32,  8.24it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.13it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.35it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 59.19it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 57.25it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 57.53it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 57.81it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 49.81it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 53.26it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 55.01it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 56.10it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 50.04it/s]\u001B[A\n",
      "{'eval_loss': 0.3332015573978424, 'eval_f1': 0.8412918340869872, 'eval_runtime': 3.3996, 'eval_samples_per_second': 176.49, 'eval_steps_per_second': 22.061, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:36<11:32,  8.24it/s]\n",
      "{'loss': 0.2664, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:13<11:46,  7.64it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.52it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.34it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.19it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.46it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 59.95it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 52.60it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 55.26it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 57.31it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 57.99it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 58.16it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4385550916194916, 'eval_f1': 0.860024096308535, 'eval_runtime': 3.2742, 'eval_samples_per_second': 183.25, 'eval_steps_per_second': 22.906, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:17<11:46,  7.64it/s]\n",
      "100% 75/75 [00:03<00:00, 46.03it/s]\u001B[A\n",
      "{'loss': 0.1671, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:54<08:54,  9.54it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.12it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.70it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 59.20it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 59.41it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 58.52it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 57.80it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 49.76it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 52.50it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 49.63it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 46.57it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 40.67it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5058491230010986, 'eval_f1': 0.86670796842293, 'eval_runtime': 3.4894, 'eval_samples_per_second': 171.948, 'eval_steps_per_second': 21.493, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:58<08:54,  9.54it/s]\n",
      "100% 75/75 [00:03<00:00, 36.64it/s]\u001B[A\n",
      "{'loss': 0.0935, 'learning_rate': 8.428070175438597e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:35<07:49, 10.23it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.78it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.12it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 56.94it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 57.17it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 57.29it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 57.67it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 48.60it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 50.25it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 52.32it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 53.44it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 47.39it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.554277241230011, 'eval_f1': 0.8787633865427359, 'eval_runtime': 3.3552, 'eval_samples_per_second': 178.824, 'eval_steps_per_second': 22.353, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:39<07:49, 10.23it/s]\n",
      "100% 75/75 [00:03<00:00, 43.23it/s]\u001B[A\n",
      "{'loss': 0.0685, 'learning_rate': 7.901754385964913e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:17<08:48,  8.51it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.58it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 43.64it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 43.46it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 43.43it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 43.62it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 44.04it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 41.27it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 41.91it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 37.39it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 38.07it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 38.76it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 37.84it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 37.78it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 38.29it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 34.20it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.670771598815918, 'eval_f1': 0.8663288818729233, 'eval_runtime': 3.8904, 'eval_samples_per_second': 154.225, 'eval_steps_per_second': 19.278, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:20<08:48,  8.51it/s]\n",
      "100% 75/75 [00:03<00:00, 31.84it/s]\u001B[A\n",
      "{'loss': 0.0327, 'learning_rate': 7.37719298245614e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:58<06:53, 10.15it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 59.50it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 58.21it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 54.75it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 54.05it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 54.11it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 56.42it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 48.88it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 51.08it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 52.97it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 54.05it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 55.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7069783806800842, 'eval_f1': 0.8694628110899855, 'eval_runtime': 3.3819, 'eval_samples_per_second': 177.417, 'eval_steps_per_second': 22.177, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:02<06:53, 10.15it/s]\n",
      "100% 75/75 [00:03<00:00, 42.44it/s]\u001B[A\n",
      "{'loss': 0.0189, 'learning_rate': 6.850877192982457e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:40<08:25,  7.72it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 45.98it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 39.12it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 36.93it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 36.58it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 36.09it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 36.92it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 36.31it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 36.16it/s]\u001B[A\n",
      " 51% 38/75 [00:01<00:01, 36.38it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:01, 32.27it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 33.56it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 34.27it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 34.32it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 34.97it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 35.47it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 35.02it/s]\u001B[A\n",
      " 93% 70/75 [00:02<00:00, 32.09it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7572712302207947, 'eval_f1': 0.8759382949661023, 'eval_runtime': 4.0851, 'eval_samples_per_second': 146.874, 'eval_steps_per_second': 18.359, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:44<08:25,  7.72it/s]\n",
      "100% 75/75 [00:04<00:00, 30.37it/s]\u001B[A\n",
      "{'loss': 0.0172, 'learning_rate': 6.324561403508772e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:21<05:41, 10.54it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.87it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 57.64it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.83it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 58.63it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 58.06it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.22it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 50.36it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 51.78it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 53.98it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 55.42it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 47.66it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7162193059921265, 'eval_f1': 0.8851912381466227, 'eval_runtime': 3.3544, 'eval_samples_per_second': 178.871, 'eval_steps_per_second': 22.359, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:25<05:41, 10.54it/s]\n",
      "100% 75/75 [00:03<00:00, 44.53it/s]\u001B[A\n",
      "{'loss': 0.0076, 'learning_rate': 5.798245614035089e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:03<06:49,  8.05it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 60.52it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 57.35it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 57.75it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 58.55it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 57.55it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.49it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 49.50it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 50.51it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 52.97it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 54.06it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 47.93it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7368966341018677, 'eval_f1': 0.8866370243478479, 'eval_runtime': 3.3299, 'eval_samples_per_second': 180.185, 'eval_steps_per_second': 22.523, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:06<06:49,  8.05it/s]\n",
      "100% 75/75 [00:03<00:00, 44.34it/s]\u001B[A\n",
      "{'loss': 0.0042, 'learning_rate': 5.271929824561403e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:44<05:05,  9.84it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 57.28it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 58.09it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 56.80it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 56.93it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 56.30it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 57.14it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 49.48it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 53.39it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 55.73it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 55.34it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 55.84it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7418760657310486, 'eval_f1': 0.8915198443870871, 'eval_runtime': 3.4244, 'eval_samples_per_second': 175.212, 'eval_steps_per_second': 21.901, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:47<05:05,  9.84it/s]\n",
      "100% 75/75 [00:03<00:00, 43.12it/s]\u001B[A\n",
      "{'loss': 0.0012, 'learning_rate': 4.74561403508772e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:24<04:23, 10.25it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 58.14it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 59.03it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 57.68it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 58.85it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 58.67it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 58.80it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 50.90it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 54.15it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 56.17it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.51it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 49.62it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8051631450653076, 'eval_f1': 0.8909760773944535, 'eval_runtime': 3.2946, 'eval_samples_per_second': 182.118, 'eval_steps_per_second': 22.765, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:28<04:23, 10.25it/s]\n",
      "100% 75/75 [00:03<00:00, 46.61it/s]\u001B[A\n",
      "{'loss': 0.0001, 'learning_rate': 4.219298245614035e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:05<04:38,  8.62it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 51.86it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 42.71it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 41.69it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 42.57it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 42.26it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 43.03it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 41.75it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 37.50it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 38.72it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 38.67it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 38.49it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 38.01it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 37.62it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 37.97it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 33.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8113294243812561, 'eval_f1': 0.889512763772671, 'eval_runtime': 3.8849, 'eval_samples_per_second': 154.442, 'eval_steps_per_second': 19.305, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:09<04:38,  8.62it/s]\n",
      "100% 75/75 [00:03<00:00, 31.50it/s]\u001B[A\n",
      "{'loss': 0.0002, 'learning_rate': 3.692982456140351e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:47<03:26, 10.19it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.46it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 56.89it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.32it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 56.96it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 54.00it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 54.22it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 47.27it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 49.94it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 52.26it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 53.81it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 54.47it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8138524293899536, 'eval_f1': 0.8929909042268592, 'eval_runtime': 3.4094, 'eval_samples_per_second': 175.985, 'eval_steps_per_second': 21.998, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:50<03:26, 10.19it/s]\n",
      "100% 75/75 [00:03<00:00, 42.66it/s]\u001B[A\n",
      "{'loss': 0.0042, 'learning_rate': 3.1666666666666667e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:28<03:48,  7.89it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 45.68it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 41.49it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 40.62it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 38.99it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 39.28it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 38.80it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 38.13it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 37.95it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 38.19it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 34.08it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 35.27it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 34.95it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 35.48it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 35.97it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 35.38it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 35.99it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9022518992424011, 'eval_f1': 0.8808138709952041, 'eval_runtime': 3.9964, 'eval_samples_per_second': 150.136, 'eval_steps_per_second': 18.767, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:32<03:48,  7.89it/s]\n",
      "100% 75/75 [00:03<00:00, 32.73it/s]\u001B[A\n",
      "{'loss': 0.0004, 'learning_rate': 2.6403508771929826e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:10<02:35,  9.65it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.66it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.33it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 57.47it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 58.18it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 56.50it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 56.09it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 48.79it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 51.74it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 53.08it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 54.39it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 54.76it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8546067476272583, 'eval_f1': 0.8904157902029197, 'eval_runtime': 3.3551, 'eval_samples_per_second': 178.831, 'eval_steps_per_second': 22.354, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:13<02:35,  9.65it/s]\n",
      "100% 75/75 [00:03<00:00, 42.38it/s]\u001B[A\n",
      "{'loss': 0.0001, 'learning_rate': 2.114035087719298e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [10:51<02:28,  8.10it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 44.58it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 39.70it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 38.41it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 38.30it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 37.64it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 37.30it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 37.37it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 37.65it/s]\u001B[A\n",
      " 55% 41/75 [00:01<00:00, 37.76it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 42.68it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 46.20it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 49.14it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 51.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8358490467071533, 'eval_f1': 0.8974358974358975, 'eval_runtime': 3.6648, 'eval_samples_per_second': 163.72, 'eval_steps_per_second': 20.465, 'epoch': 16.0}\n",
      " 80% 4800/6000 [10:55<02:28,  8.10it/s]\n",
      "100% 75/75 [00:03<00:00, 46.49it/s]\u001B[A\n",
      "{'loss': 0.0001, 'learning_rate': 1.5877192982456141e-06, 'epoch': 17.0}\n",
      " 85% 5100/6000 [11:32<01:33,  9.63it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 59.15it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 59.33it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:00, 58.94it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 58.87it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 58.78it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 60.10it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 51.66it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 53.36it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 54.79it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 56.22it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 49.81it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8983340263366699, 'eval_f1': 0.8851912381466227, 'eval_runtime': 3.3917, 'eval_samples_per_second': 176.9, 'eval_steps_per_second': 22.113, 'epoch': 17.0}\n",
      " 85% 5100/6000 [11:36<01:33,  9.63it/s]\n",
      "100% 75/75 [00:03<00:00, 46.33it/s]\u001B[A\n",
      "{'loss': 0.0001, 'learning_rate': 1.0614035087719299e-06, 'epoch': 18.0}\n",
      " 90% 5400/6000 [12:13<01:01,  9.79it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.03it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 62.92it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.84it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 60.45it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.36it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 52.44it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 54.92it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 55.27it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 56.21it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 57.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9247960448265076, 'eval_f1': 0.8805003809445529, 'eval_runtime': 3.3, 'eval_samples_per_second': 181.819, 'eval_steps_per_second': 22.727, 'epoch': 18.0}\n",
      " 90% 5400/6000 [12:16<01:01,  9.79it/s]\n",
      "100% 75/75 [00:03<00:00, 45.28it/s]\u001B[A\n",
      "{'loss': 0.0, 'learning_rate': 5.350877192982457e-07, 'epoch': 19.0}\n",
      " 95% 5700/6000 [12:54<00:32,  9.12it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 41.62it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 41.20it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 40.75it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 42.14it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 43.34it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 43.96it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 43.91it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 44.48it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 37.16it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 38.75it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 39.29it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 40.35it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 41.23it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 37.53it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9171380400657654, 'eval_f1': 0.8828582584927762, 'eval_runtime': 3.8104, 'eval_samples_per_second': 157.464, 'eval_steps_per_second': 19.683, 'epoch': 19.0}\n",
      " 95% 5700/6000 [12:58<00:32,  9.12it/s]\n",
      "100% 75/75 [00:03<00:00, 35.28it/s]\u001B[A\n",
      "{'loss': 0.0, 'learning_rate': 8.771929824561404e-09, 'epoch': 20.0}\n",
      "100% 6000/6000 [13:35<00:00,  9.41it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.26it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 59.15it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 58.22it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 58.73it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 58.26it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 56.32it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 48.80it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 51.58it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 53.28it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 54.55it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 55.73it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9188036918640137, 'eval_f1': 0.8845975576302769, 'eval_runtime': 3.3257, 'eval_samples_per_second': 180.415, 'eval_steps_per_second': 22.552, 'epoch': 20.0}\n",
      "100% 6000/6000 [13:39<00:00,  9.41it/s]\n",
      "100% 75/75 [00:03<00:00, 43.57it/s]\u001B[A\n",
      "{'train_runtime': 826.7622, 'train_samples_per_second': 58.058, 'train_steps_per_second': 7.257, 'train_loss': 0.05560408262163401, 'epoch': 20.0}\n",
      "100% 6000/6000 [13:46<00:00,  9.41it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "100% 6000/6000 [13:47<00:00,  7.25it/s]\n",
      "100% 75/75 [00:03<00:00, 22.06it/s]\n",
      "f1 0.8128543499511242\n",
      "efl_f1 0.8974358974358975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.6667    0.7273        12\n",
      "           1     0.7761    0.8254    0.8000        63\n",
      "           2     0.9187    0.9040    0.9113       125\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.8316    0.7987    0.8129       200\n",
      "weighted avg     0.8667    0.8650    0.8652       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-253d68726aaa6eba.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1ce5ed5d0818e536.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b9edf76b9a2ea4a1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1486e0b7fb7638b9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5a9da5b1a4bbb6b6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d450a457fafdce3c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-27b4ea27be0304f7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ed48ccaab263bd7c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22ef49944d60307d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-df2d043ee91148a1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-200ed7f374a8a3c8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3ec382b63865306c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-15f7462ef1295d80.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-224ce19d719aebb3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8ba57b0bce5b6f8d.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d8999c5deeb66450.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0181ad2feee98877.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e7c895de1dcf38d8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3e019b2219e677a3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-67455b6c692be8bf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b41af4f3cc6d910d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-231859f9b1fcdecd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63f0a024b0d91d6c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-50993f9c0dcd1f4d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6206ef4fba6ed013.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-83b8516859ffc5d5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-69da52ba02823ee3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-aa9d14c0f40c3dd0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-53546208c318094f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2415b6711bc36e23.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ebef0be9a197dc36.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5d6129bb91eaa375.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at textattack/bert-base-uncased-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.4465, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:33<09:17, 10.22it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 64.26it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.10it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 58.04it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 58.71it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 57.15it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.87it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 58.68it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 56.94it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 56.34it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 58.44it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3422856032848358, 'eval_f1': 0.8454696215890245, 'eval_runtime': 3.1706, 'eval_samples_per_second': 189.239, 'eval_steps_per_second': 23.655, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:36<09:17, 10.22it/s]\n",
      "100% 75/75 [00:03<00:00, 57.30it/s]\u001B[A\n",
      "{'loss': 0.2767, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:14<08:57, 10.05it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.61it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 59.47it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 58.20it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 59.43it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 56.59it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 50.38it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 47.32it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 45.37it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 42.33it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 40.41it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 39.92it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 39.91it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4201752841472626, 'eval_f1': 0.8684194037519267, 'eval_runtime': 3.521, 'eval_samples_per_second': 170.406, 'eval_steps_per_second': 21.301, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:17<08:57, 10.05it/s]\n",
      "100% 75/75 [00:03<00:00, 40.45it/s]\u001B[A\n",
      "{'loss': 0.194, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:56<08:37,  9.85it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.24it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.78it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.32it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 58.22it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 58.13it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.53it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 57.87it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 58.29it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 56.24it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 58.40it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5264217257499695, 'eval_f1': 0.8586905322656618, 'eval_runtime': 3.1682, 'eval_samples_per_second': 189.383, 'eval_steps_per_second': 23.673, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:59<08:37,  9.85it/s]\n",
      "100% 75/75 [00:03<00:00, 57.34it/s]\u001B[A\n",
      "{'loss': 0.0987, 'learning_rate': 8.42982456140351e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:37<09:55,  8.06it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 47.38it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 39.74it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 39.11it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 38.99it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 37.65it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 38.20it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 38.55it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 37.39it/s]\u001B[A\n",
      " 52% 39/75 [00:01<00:00, 37.79it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 38.41it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 36.69it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 37.34it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 36.92it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 37.35it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 36.92it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 36.21it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 37.00it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6058602929115295, 'eval_f1': 0.8735749665854233, 'eval_runtime': 3.8689, 'eval_samples_per_second': 155.083, 'eval_steps_per_second': 19.385, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:41<09:55,  8.06it/s]\n",
      "100% 75/75 [00:03<00:00, 37.16it/s]\u001B[A\n",
      "{'loss': 0.0381, 'learning_rate': 7.903508771929826e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:19<07:25, 10.11it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.33it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 56.91it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.66it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 59.04it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 58.30it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 58.58it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 56.96it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 58.11it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 58.85it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 58.27it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7322401404380798, 'eval_f1': 0.8594491927825262, 'eval_runtime': 3.1628, 'eval_samples_per_second': 189.703, 'eval_steps_per_second': 23.713, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:22<07:25, 10.11it/s]\n",
      "100% 75/75 [00:03<00:00, 58.06it/s]\u001B[A\n",
      "{'loss': 0.0284, 'learning_rate': 7.37719298245614e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:00<08:51,  7.91it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 49.65it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 43.17it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 39.13it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 38.50it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 44.76it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 49.10it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 53.79it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 53.63it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 56.54it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 56.62it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 58.06it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8103125095367432, 'eval_f1': 0.8646032971957627, 'eval_runtime': 3.355, 'eval_samples_per_second': 178.837, 'eval_steps_per_second': 22.355, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:03<08:51,  7.91it/s]\n",
      "100% 75/75 [00:03<00:00, 56.70it/s]\u001B[A\n",
      "{'loss': 0.0126, 'learning_rate': 6.850877192982457e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:42<08:18,  7.82it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.40it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 56.78it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.22it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 55.05it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 54.78it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 55.22it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 54.73it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 55.52it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 55.87it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 54.95it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 49.92it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8144290447235107, 'eval_f1': 0.8776627947464012, 'eval_runtime': 3.3921, 'eval_samples_per_second': 176.882, 'eval_steps_per_second': 22.11, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:45<08:18,  7.82it/s]\n",
      "100% 75/75 [00:03<00:00, 45.96it/s]\u001B[A\n",
      "{'loss': 0.0152, 'learning_rate': 6.324561403508772e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:24<06:01,  9.96it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 58.42it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 56.72it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 54.67it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 54.39it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 55.17it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 51.96it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 52.60it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 53.77it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 53.07it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 53.37it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 53.57it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8330506682395935, 'eval_f1': 0.8759382949661023, 'eval_runtime': 3.2687, 'eval_samples_per_second': 183.559, 'eval_steps_per_second': 22.945, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:27<06:01,  9.96it/s]\n",
      "100% 75/75 [00:03<00:00, 54.36it/s]\u001B[A\n",
      "{'loss': 0.0064, 'learning_rate': 5.798245614035089e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:06<06:28,  8.49it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.41it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 41.85it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 39.82it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 38.06it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 37.18it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 36.15it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 35.90it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 36.06it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 36.02it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 35.59it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 35.48it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 35.57it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 35.72it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 35.90it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 36.56it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 36.01it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8594668507575989, 'eval_f1': 0.8787633865427359, 'eval_runtime': 3.9331, 'eval_samples_per_second': 152.552, 'eval_steps_per_second': 19.069, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:10<06:28,  8.49it/s]\n",
      "100% 75/75 [00:03<00:00, 36.23it/s]\u001B[A\n",
      "{'loss': 0.0099, 'learning_rate': 5.271929824561403e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:48<04:55, 10.14it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.72it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 59.53it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.63it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 56.98it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 56.55it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 56.91it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 55.81it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 55.29it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 55.72it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 53.39it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 53.46it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9466257691383362, 'eval_f1': 0.8687709800018384, 'eval_runtime': 3.2145, 'eval_samples_per_second': 186.657, 'eval_steps_per_second': 23.332, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:52<04:55, 10.14it/s]\n",
      "100% 75/75 [00:03<00:00, 55.21it/s]\u001B[A\n",
      "{'loss': 0.0057, 'learning_rate': 4.747368421052632e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:30<06:18,  7.13it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 43.80it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 39.14it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 38.11it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 35.92it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 35.19it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 40.92it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 45.57it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 48.97it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 49.81it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 52.25it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 52.87it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 53.95it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9138133525848389, 'eval_f1': 0.8779720837674712, 'eval_runtime': 3.4507, 'eval_samples_per_second': 173.878, 'eval_steps_per_second': 21.735, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:34<06:18,  7.13it/s]\n",
      "100% 75/75 [00:03<00:00, 53.50it/s]\u001B[A\n",
      "{'loss': 0.0062, 'learning_rate': 4.221052631578948e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:12<04:00,  9.98it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.67it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.22it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.60it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 57.75it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 54.93it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 55.92it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 55.65it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 55.46it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 56.00it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.12it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 55.80it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9859168529510498, 'eval_f1': 0.8732464977250585, 'eval_runtime': 3.2459, 'eval_samples_per_second': 184.849, 'eval_steps_per_second': 23.106, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:15<04:00,  9.98it/s]\n",
      "100% 75/75 [00:03<00:00, 54.16it/s]\u001B[A\n",
      "{'loss': 0.0047, 'learning_rate': 3.6947368421052637e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:53<03:38,  9.60it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.14it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.37it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 57.56it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 55.86it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 56.23it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 56.44it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 56.30it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 55.79it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 54.92it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 55.43it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 55.22it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9612503051757812, 'eval_f1': 0.8773488481798883, 'eval_runtime': 3.2252, 'eval_samples_per_second': 186.036, 'eval_steps_per_second': 23.254, 'epoch': 13.0}\n",
      " 65% 3900/6000 [08:57<03:38,  9.60it/s]\n",
      "100% 75/75 [00:03<00:00, 53.71it/s]\u001B[A\n",
      "{'loss': 0.0053, 'learning_rate': 3.168421052631579e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:35<03:32,  8.48it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 46.14it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 44.45it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 42.52it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 41.14it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 41.36it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 40.62it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 38.57it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 38.15it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 38.32it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 37.77it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 37.40it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 37.24it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 37.85it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 37.69it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 37.95it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 37.99it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.994492769241333, 'eval_f1': 0.8793909241670436, 'eval_runtime': 3.8221, 'eval_samples_per_second': 156.982, 'eval_steps_per_second': 19.623, 'epoch': 14.0}\n",
      " 70% 4200/6000 [09:39<03:32,  8.48it/s]\n",
      "100% 75/75 [00:03<00:00, 38.16it/s]\u001B[A\n",
      "{'loss': 0.0015, 'learning_rate': 2.6421052631578948e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:17<02:29, 10.03it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 59.73it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 59.55it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:00, 57.14it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 57.05it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 57.05it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 57.72it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 55.36it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 56.08it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 55.68it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 56.27it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 55.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0378012657165527, 'eval_f1': 0.8756218905472637, 'eval_runtime': 3.2025, 'eval_samples_per_second': 187.357, 'eval_steps_per_second': 23.42, 'epoch': 15.0}\n",
      " 75% 4500/6000 [10:20<02:29, 10.03it/s]\n",
      "100% 75/75 [00:03<00:00, 56.00it/s]\u001B[A\n",
      "{'loss': 0.0059, 'learning_rate': 2.1157894736842107e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [10:58<02:26,  8.21it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 48.91it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 42.19it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 41.42it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 40.33it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 39.14it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 38.13it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 36.86it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:01, 35.34it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 36.26it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 35.60it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 36.21it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 37.14it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 38.12it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 39.01it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0402354001998901, 'eval_f1': 0.8767067422443797, 'eval_runtime': 3.7768, 'eval_samples_per_second': 158.863, 'eval_steps_per_second': 19.858, 'epoch': 16.0}\n",
      " 80% 4800/6000 [11:02<02:26,  8.21it/s]\n",
      "100% 75/75 [00:03<00:00, 43.82it/s]\u001B[A\n",
      "{'loss': 0.0022, 'learning_rate': 1.5894736842105265e-06, 'epoch': 17.0}\n",
      " 85% 5100/6000 [11:40<01:35,  9.47it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.08it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.47it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 57.17it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 57.76it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 57.72it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.88it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 55.72it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 56.59it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 56.31it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 56.34it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.024994134902954, 'eval_f1': 0.8793909241670436, 'eval_runtime': 3.2305, 'eval_samples_per_second': 185.728, 'eval_steps_per_second': 23.216, 'epoch': 17.0}\n",
      " 85% 5100/6000 [11:43<01:35,  9.47it/s]\n",
      "100% 75/75 [00:03<00:00, 56.00it/s]\u001B[A\n",
      "{'loss': 0.0001, 'learning_rate': 1.0631578947368422e-06, 'epoch': 18.0}\n",
      " 90% 5400/6000 [12:21<01:00,  9.85it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.15it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 59.46it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 57.99it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 57.97it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 58.17it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 58.61it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 57.31it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 57.14it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 57.01it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 57.30it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 56.90it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.0635250806808472, 'eval_f1': 0.8749747423721963, 'eval_runtime': 3.1648, 'eval_samples_per_second': 189.586, 'eval_steps_per_second': 23.698, 'epoch': 18.0}\n",
      " 90% 5400/6000 [12:24<01:00,  9.85it/s]\n",
      "100% 75/75 [00:03<00:00, 57.79it/s]\u001B[A\n",
      "{'loss': 0.0046, 'learning_rate': 5.368421052631579e-07, 'epoch': 19.0}\n",
      " 95% 5700/6000 [13:02<00:31,  9.63it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 60.59it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 57.54it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 54.09it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 47.09it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 44.88it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 43.94it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 43.35it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 43.39it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 42.96it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 43.03it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 43.73it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 42.67it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 1.058923602104187, 'eval_f1': 0.8773488481798883, 'eval_runtime': 3.585, 'eval_samples_per_second': 167.362, 'eval_steps_per_second': 20.92, 'epoch': 19.0}\n",
      " 95% 5700/6000 [13:06<00:31,  9.63it/s]\n",
      "100% 75/75 [00:03<00:00, 42.96it/s]\u001B[A\n",
      "{'train_runtime': 794.0204, 'train_samples_per_second': 60.452, 'train_steps_per_second': 7.556, 'train_loss': 0.06119875713398582, 'epoch': 19.0}\n",
      " 95% 5700/6000 [13:13<00:31,  9.63it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      " 95% 5700/6000 [13:14<00:41,  7.17it/s]\n",
      "100% 75/75 [00:03<00:00, 23.39it/s]\n",
      "f1 0.7056751350867243\n",
      "efl_f1 0.8793909241670436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.2857    0.4211        14\n",
      "           1     0.7101    0.9074    0.7967        54\n",
      "           2     0.9206    0.8788    0.8992       132\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.8103    0.6906    0.7057       200\n",
      "weighted avg     0.8554    0.8450    0.8381       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d30dcc499a9e4901.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a51adfe856ae1e6f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-494b3ec4ce11ceae.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ce54a7a8e6559a6b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5c10526786c137f1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9cbc05347cad9a91.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1a5468a15327172b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2ce775faee68847c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b303aff5322e9159.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2d2212d31dc860f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5c9f2da4ff004d95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2a07026fb75911b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a652e225dc6fd622.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e3113caafd6a84e1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0dda2a1c75d11508.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c8f86ef360530e20.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-82d853a51a2e23e1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a472762b99394f5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2ff3e1c2a18e1e1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0c1a6c85873355ec.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7e8e338b296ff826.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4eefb5a2b564dc8d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9fc875bbbfb7ac4b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-54f8532427cc2a19.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3044b7b97a787fa7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cc8ee79d0e41287b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c74a8c06def56b07.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f8888634c43dd491.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9c84dfd9ad7fb8bb.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f524902fd85761f4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f90f14d71466ff22.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1cb991b97d49fba9.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at textattack/bert-base-uncased-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.4469, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:32<09:09, 10.38it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 67.06it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 51.20it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 48.85it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 52.56it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 54.79it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 53.69it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 49.89it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 53.31it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 54.31it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 54.57it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3404759168624878, 'eval_f1': 0.8531613223108976, 'eval_runtime': 3.2757, 'eval_samples_per_second': 183.17, 'eval_steps_per_second': 22.896, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:36<09:09, 10.38it/s]\n",
      "100% 75/75 [00:03<00:00, 55.81it/s]\u001B[A\n",
      "{'loss': 0.2927, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:14<11:55,  7.55it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.44it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 53.07it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 48.67it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 51.59it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 53.77it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 52.74it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 52.40it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 54.53it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 54.06it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 53.84it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.36221739649772644, 'eval_f1': 0.8753639213706133, 'eval_runtime': 3.2843, 'eval_samples_per_second': 182.689, 'eval_steps_per_second': 22.836, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:17<11:55,  7.55it/s]\n",
      "100% 75/75 [00:03<00:00, 54.60it/s]\u001B[A\n",
      "{'loss': 0.1837, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:55<08:38,  9.83it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 68.43it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 52.08it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 49.86it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 52.79it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 56.45it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 55.68it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 54.56it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 56.10it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 54.77it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 55.34it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4699971377849579, 'eval_f1': 0.8722312804523394, 'eval_runtime': 3.3037, 'eval_samples_per_second': 181.614, 'eval_steps_per_second': 22.702, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:58<08:38,  9.83it/s]\n",
      "100% 75/75 [00:03<00:00, 54.85it/s]\u001B[A\n",
      "{'loss': 0.0782, 'learning_rate': 8.428070175438597e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:36<07:59, 10.00it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.04it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 51.61it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 49.26it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 53.00it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 55.68it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 56.50it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 55.75it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 59.17it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 56.32it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 55.92it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5375903248786926, 'eval_f1': 0.8918812970450716, 'eval_runtime': 3.2726, 'eval_samples_per_second': 183.342, 'eval_steps_per_second': 22.918, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:39<07:59, 10.00it/s]\n",
      "100% 75/75 [00:03<00:00, 56.92it/s]\u001B[A\n",
      "{'loss': 0.0283, 'learning_rate': 7.901754385964913e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:17<08:20,  8.99it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 52.79it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:01, 43.94it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:01, 44.79it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 44.08it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 44.87it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 46.17it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 46.88it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 45.14it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 45.49it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 44.72it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 44.42it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 44.84it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 44.08it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6080073118209839, 'eval_f1': 0.8901293190543544, 'eval_runtime': 3.5996, 'eval_samples_per_second': 166.687, 'eval_steps_per_second': 20.836, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:21<08:20,  8.99it/s]\n",
      "100% 75/75 [00:03<00:00, 42.68it/s]\u001B[A\n",
      "{'loss': 0.0175, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [03:59<06:59, 10.02it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 66.98it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 49.96it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 47.26it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 49.52it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 50.30it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 51.56it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 48.96it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 50.61it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 51.22it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 51.92it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 50.90it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7102489471435547, 'eval_f1': 0.8799999999999999, 'eval_runtime': 3.3508, 'eval_samples_per_second': 179.062, 'eval_steps_per_second': 22.383, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:02<06:59, 10.02it/s]\n",
      "100% 75/75 [00:03<00:00, 51.69it/s]\u001B[A\n",
      "{'loss': 0.0113, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:40<08:14,  7.89it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 45.39it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 38.47it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 38.17it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 37.98it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 37.26it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 37.43it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 37.61it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 37.68it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 37.91it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 37.00it/s]\u001B[A\n",
      " 61% 46/75 [00:01<00:00, 37.23it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 36.47it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 37.45it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 37.60it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 35.80it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 36.49it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 36.61it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7154157161712646, 'eval_f1': 0.8825520307132754, 'eval_runtime': 3.9158, 'eval_samples_per_second': 153.227, 'eval_steps_per_second': 19.153, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:44<08:14,  7.89it/s]\n",
      "100% 75/75 [00:03<00:00, 35.88it/s]\u001B[A\n",
      "{'loss': 0.0093, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:22<06:29,  9.25it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 65.32it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 52.48it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 52.44it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 54.88it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 57.64it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 56.98it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 56.13it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 58.07it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 55.67it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 56.43it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7350879311561584, 'eval_f1': 0.8819257528820272, 'eval_runtime': 3.2024, 'eval_samples_per_second': 187.362, 'eval_steps_per_second': 23.42, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:25<06:29,  9.25it/s]\n",
      "100% 75/75 [00:03<00:00, 55.92it/s]\u001B[A\n",
      "{'loss': 0.0121, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:03<06:33,  8.38it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:01, 65.00it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 50.94it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 51.39it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 53.63it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 53.81it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 55.35it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 53.86it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 57.39it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 56.52it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 55.60it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.765247106552124, 'eval_f1': 0.8831599577868234, 'eval_runtime': 3.2072, 'eval_samples_per_second': 187.081, 'eval_steps_per_second': 23.385, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:06<06:33,  8.38it/s]\n",
      "100% 75/75 [00:03<00:00, 56.70it/s]\u001B[A\n",
      "{'train_runtime': 374.0281, 'train_samples_per_second': 128.333, 'train_steps_per_second': 16.042, 'train_loss': 0.12000768846935696, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:13<06:33,  8.38it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      " 45% 2700/6000 [06:14<07:37,  7.21it/s]\n",
      "100% 75/75 [00:03<00:00, 19.26it/s]\n",
      "f1 0.7419934151451661\n",
      "efl_f1 0.8918812970450716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.4375    0.5385        16\n",
      "           1     0.7258    0.8182    0.7692        55\n",
      "           2     0.9219    0.9147    0.9183       129\n",
      "\n",
      "    accuracy                         0.8500       200\n",
      "   macro avg     0.7826    0.7235    0.7420       200\n",
      "weighted avg     0.8502    0.8500    0.8469       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bb8bc0cbd4562908.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dfb36a44c8fa3a83.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-837c2a351563182b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0f412579adee1d02.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bd1aeb1a48264ee0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f152c895d650e655.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-99ffe4e028db6c1c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-097467430541a59a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ec5914fd125b2658.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c655357c240db9d8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a9e764713adc8faf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fd1bb9907365ad72.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0c1f4a379c890127.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7a2ed18ab9b663f4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2fa483df7e3e5166.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eb2882b3a2ae2ecf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-477b6488956ccded.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-03e85928e546693d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e58270f13956086f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e0821d96ed671ab4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-91834b92a925b255.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5e031be53c45d0ab.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8739752c42f1d2f7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-841350b8b2c4edff.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-517686ca08db857c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eee6d82cb4c69ef4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7343beabae73b2f6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6fb827f609ea0e67.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d0f8c08c9ce07f73.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-581142d9a30fdeef.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bcd24f3b18b8ad48.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-20b677bb8cb29399.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at textattack/bert-base-uncased-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5303, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:33<12:17,  7.72it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 46.25it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 41.54it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 38.71it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:01, 38.51it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:01, 38.69it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:01, 37.64it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:01, 37.49it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 38.19it/s]\u001B[A\n",
      " 52% 39/75 [00:01<00:00, 38.27it/s]\u001B[A\n",
      " 57% 43/75 [00:01<00:00, 38.21it/s]\u001B[A\n",
      " 63% 47/75 [00:01<00:00, 38.40it/s]\u001B[A\n",
      " 68% 51/75 [00:01<00:00, 35.45it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 36.56it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 36.33it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 36.06it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 36.93it/s]\u001B[A\n",
      " 95% 71/75 [00:01<00:00, 36.68it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3593831956386566, 'eval_f1': 0.794558860920417, 'eval_runtime': 3.8857, 'eval_samples_per_second': 154.411, 'eval_steps_per_second': 19.301, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:37<12:17,  7.72it/s]\n",
      "100% 75/75 [00:03<00:00, 36.44it/s]\u001B[A\n",
      "{'loss': 0.298, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:15<09:27,  9.52it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 57.52it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 56.68it/s]\u001B[A\n",
      " 25% 19/75 [00:00<00:00, 56.51it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 54.59it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 56.14it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 57.20it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 56.30it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 57.92it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 56.76it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 54.41it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3315936326980591, 'eval_f1': 0.8880541320463704, 'eval_runtime': 3.2152, 'eval_samples_per_second': 186.615, 'eval_steps_per_second': 23.327, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:18<09:27,  9.52it/s]\n",
      "100% 75/75 [00:03<00:00, 56.26it/s]\u001B[A\n",
      "{'loss': 0.2095, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:56<10:32,  8.06it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.88it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.27it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 41.81it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 41.04it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 40.03it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 40.10it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:01, 39.85it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 44.17it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 49.04it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 51.30it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 49.99it/s]\u001B[A\n",
      " 88% 66/75 [00:01<00:00, 52.31it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5029969215393066, 'eval_f1': 0.8646599581448389, 'eval_runtime': 3.4669, 'eval_samples_per_second': 173.065, 'eval_steps_per_second': 21.633, 'epoch': 3.0}\n",
      " 15% 900/6000 [02:00<10:32,  8.06it/s]\n",
      "100% 75/75 [00:03<00:00, 50.83it/s]\u001B[A\n",
      "{'loss': 0.1343, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:38<08:06,  9.87it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.23it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.82it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 58.53it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 56.00it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 56.95it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 57.09it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 56.98it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 56.91it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 56.81it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 53.50it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 54.31it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5077210068702698, 'eval_f1': 0.8767067422443797, 'eval_runtime': 3.2561, 'eval_samples_per_second': 184.268, 'eval_steps_per_second': 23.033, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:41<08:06,  9.87it/s]\n",
      "100% 75/75 [00:03<00:00, 52.46it/s]\u001B[A\n",
      "{'loss': 0.0644, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:19<07:48,  9.60it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 58.89it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:01, 57.42it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 59.26it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 56.72it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 58.17it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 58.73it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 57.48it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 58.62it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 58.08it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 55.71it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5557453632354736, 'eval_f1': 0.8860398860398861, 'eval_runtime': 3.1869, 'eval_samples_per_second': 188.273, 'eval_steps_per_second': 23.534, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:22<07:48,  9.60it/s]\n",
      "100% 75/75 [00:03<00:00, 56.43it/s]\u001B[A\n",
      "{'loss': 0.0349, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:01<07:35,  9.21it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.95it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 46.46it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 41.65it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 40.12it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 38.68it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 41.43it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 41.54it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 40.31it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 41.93it/s]\u001B[A\n",
      " 67% 50/75 [00:01<00:00, 42.82it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 43.23it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 42.62it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 43.15it/s]\u001B[A\n",
      " 93% 70/75 [00:01<00:00, 41.84it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6606672406196594, 'eval_f1': 0.8844672657252888, 'eval_runtime': 3.6804, 'eval_samples_per_second': 163.028, 'eval_steps_per_second': 20.378, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:04<07:35,  9.21it/s]\n",
      "100% 75/75 [00:03<00:00, 40.83it/s]\u001B[A\n",
      "{'loss': 0.0219, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:42<06:35,  9.86it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 63.61it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 55.63it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 58.22it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 56.71it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 56.31it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 58.49it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 58.73it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 58.17it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 57.86it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 56.83it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6895697116851807, 'eval_f1': 0.8886314468951215, 'eval_runtime': 3.1864, 'eval_samples_per_second': 188.302, 'eval_steps_per_second': 23.538, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:45<06:35,  9.86it/s]\n",
      "100% 75/75 [00:03<00:00, 55.85it/s]\u001B[A\n",
      "{'loss': 0.0177, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:24<07:38,  7.85it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.21it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 40.71it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 39.92it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 36.38it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 36.11it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 35.83it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 36.96it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 37.38it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 37.11it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 37.68it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 38.02it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 37.55it/s]\u001B[A\n",
      " 75% 56/75 [00:01<00:00, 34.86it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 33.26it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 33.63it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 34.36it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7874698042869568, 'eval_f1': 0.8787633865427359, 'eval_runtime': 3.9622, 'eval_samples_per_second': 151.431, 'eval_steps_per_second': 18.929, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:28<07:38,  7.85it/s]\n",
      "100% 75/75 [00:03<00:00, 34.70it/s]\u001B[A\n",
      "{'loss': 0.0082, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:06<05:33,  9.89it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.36it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.64it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 58.99it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 56.32it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 57.30it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 56.58it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 55.17it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 56.10it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 55.36it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 54.59it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8231866359710693, 'eval_f1': 0.8801821991321135, 'eval_runtime': 3.2014, 'eval_samples_per_second': 187.42, 'eval_steps_per_second': 23.427, 'epoch': 9.0}\n",
      " 45% 2700/6000 [06:09<05:33,  9.89it/s]\n",
      "100% 75/75 [00:03<00:00, 57.29it/s]\u001B[A\n",
      "{'loss': 0.0067, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:47<06:14,  8.02it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 46.81it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 39.20it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 38.90it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 38.76it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 37.24it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 36.44it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 37.38it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 43.40it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 46.83it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 50.61it/s]\u001B[A\n",
      " 73% 55/75 [00:01<00:00, 53.25it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 51.50it/s]\u001B[A\n",
      " 91% 68/75 [00:01<00:00, 55.24it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8020005822181702, 'eval_f1': 0.8866370243478479, 'eval_runtime': 3.4631, 'eval_samples_per_second': 173.256, 'eval_steps_per_second': 21.657, 'epoch': 10.0}\n",
      " 50% 3000/6000 [06:51<06:14,  8.02it/s]\n",
      "100% 75/75 [00:03<00:00, 54.39it/s]\u001B[A\n",
      "{'loss': 0.0008, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:29<04:28, 10.04it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 62.98it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 61.11it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.66it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 57.67it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 60.18it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 57.32it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 57.84it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 58.07it/s]\u001B[A\n",
      " 80% 60/75 [00:01<00:00, 53.92it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 56.47it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.825971782207489, 'eval_f1': 0.8854247558910772, 'eval_runtime': 3.2007, 'eval_samples_per_second': 187.458, 'eval_steps_per_second': 23.432, 'epoch': 11.0}\n",
      " 55% 3300/6000 [07:32<04:28, 10.04it/s]\n",
      "100% 75/75 [00:03<00:00, 54.93it/s]\u001B[A\n",
      "{'loss': 0.0107, 'learning_rate': 4.217543859649123e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:10<03:59, 10.04it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 61.40it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.91it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 60.78it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 58.27it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 58.38it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 58.20it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 58.27it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 58.77it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 57.65it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 55.36it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8834330439567566, 'eval_f1': 0.8845975576302769, 'eval_runtime': 3.1863, 'eval_samples_per_second': 188.308, 'eval_steps_per_second': 23.538, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:13<03:59, 10.04it/s]\n",
      "100% 75/75 [00:03<00:00, 54.96it/s]\u001B[A\n",
      "{'train_runtime': 501.3593, 'train_samples_per_second': 95.74, 'train_steps_per_second': 11.967, 'train_loss': 0.11145367561115159, 'epoch': 12.0}\n",
      " 60% 3600/6000 [08:21<03:59, 10.04it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 17 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 17 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      " 60% 3600/6000 [08:22<05:34,  7.17it/s]\n",
      "100% 75/75 [00:03<00:00, 19.30it/s]\n",
      "f1 0.7256415323482233\n",
      "efl_f1 0.8886314468951215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.3846    0.4348        13\n",
      "           1     0.8065    0.8197    0.8130        61\n",
      "           2     0.9219    0.9365    0.9291       126\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.7428    0.7136    0.7256       200\n",
      "weighted avg     0.8592    0.8650    0.8616       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9e63f20091859520.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a33021527eb8a188.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-480665de5aac5218.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7c3e8d774b99082f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a784aa1e23a6ccad.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f99cc31d80f6d6a3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-140ce776af1e63b7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e211bab5b8a6c151.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7f2149183938124f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-75a1039870cc790a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-78e7f352099e1a0f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7656cbaad7b70fb1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-62dcffc758e2ce5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-15f4f85727044ae2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-90903203a8463a2d.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a1f2ac9fafac5d12.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-83dcab1ee4c387ce.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c6669ed08d115a32.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-09f3014bd3be9a4c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b7511b613a85ab7d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8d06d6709c13927a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-13319056394452b1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b0d1d6f05975107b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dd80312c2fbb5ea7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d1cca667f79fb639.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8744559ec525c4c0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-522da4945bf30c78.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3fdf8a75f13295c9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2e034f93c3ad496.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-67fb33cf49ea3157.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-10afa5ed95c2597a.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-686afed83ddad719.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at textattack/bert-base-uncased-MNLI and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.534, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:33<11:48,  8.04it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 48.34it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 42.14it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 40.22it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:01, 39.51it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:01, 38.73it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:01, 38.58it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:01, 38.55it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:01, 38.37it/s]\u001B[A\n",
      " 53% 40/75 [00:01<00:00, 36.71it/s]\u001B[A\n",
      " 59% 44/75 [00:01<00:00, 36.92it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 37.36it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 38.06it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 43.10it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 46.46it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3344839811325073, 'eval_f1': 0.8438097564267786, 'eval_runtime': 3.6339, 'eval_samples_per_second': 165.11, 'eval_steps_per_second': 20.639, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:36<11:48,  8.04it/s]\n",
      "100% 75/75 [00:03<00:00, 50.60it/s]\u001B[A\n",
      "{'loss': 0.2868, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:14<09:16,  9.71it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 67.89it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:01, 58.34it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 56.65it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 55.16it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 56.18it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 52.44it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 51.60it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 50.85it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 53.34it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 54.64it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.39962607622146606, 'eval_f1': 0.8580631073261273, 'eval_runtime': 3.2621, 'eval_samples_per_second': 183.928, 'eval_steps_per_second': 22.991, 'epoch': 2.0}\n",
      " 10% 600/6000 [01:17<09:16,  9.71it/s]\n",
      "100% 75/75 [00:03<00:00, 56.11it/s]\u001B[A\n",
      "{'loss': 0.2169, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:55<09:28,  8.97it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 74.08it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 59.63it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 55.91it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:00, 57.10it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 54.80it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 51.09it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 50.89it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 51.95it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 52.74it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 53.93it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4550130069255829, 'eval_f1': 0.8787633865427359, 'eval_runtime': 3.2712, 'eval_samples_per_second': 183.421, 'eval_steps_per_second': 22.928, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:59<09:28,  8.97it/s]\n",
      "100% 75/75 [00:03<00:00, 54.16it/s]\u001B[A\n",
      "{'loss': 0.1384, 'learning_rate': 8.428070175438597e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:37<08:10,  9.79it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 68.88it/s]\u001B[A\n",
      " 20% 15/75 [00:00<00:00, 60.03it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 57.13it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 56.80it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 56.20it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 50.86it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 46.24it/s]\u001B[A\n",
      " 69% 52/75 [00:01<00:00, 42.76it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 43.08it/s]\u001B[A\n",
      " 83% 62/75 [00:01<00:00, 41.82it/s]\u001B[A\n",
      " 89% 67/75 [00:01<00:00, 43.11it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5088093280792236, 'eval_f1': 0.8646032971957627, 'eval_runtime': 3.4952, 'eval_samples_per_second': 171.664, 'eval_steps_per_second': 21.458, 'epoch': 4.0}\n",
      " 20% 1200/6000 [02:40<08:10,  9.79it/s]\n",
      "100% 75/75 [00:03<00:00, 43.38it/s]\u001B[A\n",
      "{'loss': 0.0944, 'learning_rate': 7.901754385964913e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:19<07:23, 10.14it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 67.68it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 58.28it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 56.76it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 52.74it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 55.90it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 51.88it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 51.58it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 51.18it/s]\u001B[A\n",
      " 77% 58/75 [00:01<00:00, 53.33it/s]\u001B[A\n",
      " 85% 64/75 [00:01<00:00, 54.24it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6488236784934998, 'eval_f1': 0.860904614566687, 'eval_runtime': 3.2653, 'eval_samples_per_second': 183.752, 'eval_steps_per_second': 22.969, 'epoch': 5.0}\n",
      " 25% 1500/6000 [03:22<07:23, 10.14it/s]\n",
      "100% 75/75 [00:03<00:00, 54.93it/s]\u001B[A\n",
      "{'loss': 0.0522, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:00<09:08,  7.66it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  8% 6/75 [00:00<00:01, 47.63it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:01, 41.85it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:01, 39.96it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 39.40it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:01, 38.79it/s]\u001B[A\n",
      " 39% 29/75 [00:00<00:01, 38.10it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:01, 37.69it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:01, 37.11it/s]\u001B[A\n",
      " 55% 41/75 [00:01<00:00, 36.91it/s]\u001B[A\n",
      " 60% 45/75 [00:01<00:00, 37.35it/s]\u001B[A\n",
      " 65% 49/75 [00:01<00:00, 37.72it/s]\u001B[A\n",
      " 71% 53/75 [00:01<00:00, 38.09it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 38.13it/s]\u001B[A\n",
      " 81% 61/75 [00:01<00:00, 38.01it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 37.38it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 36.88it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6901028156280518, 'eval_f1': 0.8605457560681442, 'eval_runtime': 3.8576, 'eval_samples_per_second': 155.538, 'eval_steps_per_second': 19.442, 'epoch': 6.0}\n",
      " 30% 1800/6000 [04:04<09:08,  7.66it/s]\n",
      "100% 75/75 [00:03<00:00, 37.30it/s]\u001B[A\n",
      "{'loss': 0.0374, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:42<06:32,  9.93it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 65.28it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 60.43it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:01, 53.30it/s]\u001B[A\n",
      " 36% 27/75 [00:00<00:00, 52.69it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 54.01it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 50.16it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 50.71it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 50.01it/s]\u001B[A\n",
      " 76% 57/75 [00:01<00:00, 52.46it/s]\u001B[A\n",
      " 84% 63/75 [00:01<00:00, 51.14it/s]\u001B[A\n",
      " 92% 69/75 [00:01<00:00, 52.94it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7508317828178406, 'eval_f1': 0.8677431950724893, 'eval_runtime': 3.3645, 'eval_samples_per_second': 178.334, 'eval_steps_per_second': 22.292, 'epoch': 7.0}\n",
      " 35% 2100/6000 [04:45<06:32,  9.93it/s]\n",
      "100% 75/75 [00:03<00:00, 53.37it/s]\u001B[A\n",
      "{'loss': 0.0265, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:23<07:25,  8.08it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  7% 5/75 [00:00<00:01, 46.09it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:01, 36.73it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:01, 37.40it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:01, 37.30it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:01, 35.03it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:01, 33.26it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:01, 33.40it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:01, 33.03it/s]\u001B[A\n",
      " 51% 38/75 [00:01<00:01, 33.86it/s]\u001B[A\n",
      " 56% 42/75 [00:01<00:00, 35.18it/s]\u001B[A\n",
      " 64% 48/75 [00:01<00:00, 39.90it/s]\u001B[A\n",
      " 72% 54/75 [00:01<00:00, 44.57it/s]\u001B[A\n",
      " 79% 59/75 [00:01<00:00, 44.79it/s]\u001B[A\n",
      " 87% 65/75 [00:01<00:00, 48.22it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7571966648101807, 'eval_f1': 0.8646599581448389, 'eval_runtime': 3.7015, 'eval_samples_per_second': 162.097, 'eval_steps_per_second': 20.262, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:27<07:25,  8.08it/s]\n",
      "100% 75/75 [00:03<00:00, 51.38it/s]\u001B[A\n",
      "{'train_runtime': 335.0856, 'train_samples_per_second': 143.247, 'train_steps_per_second': 17.906, 'train_loss': 0.17330217719078064, 'epoch': 8.0}\n",
      " 40% 2400/6000 [05:34<07:25,  8.08it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      " 40% 2400/6000 [05:35<08:23,  7.14it/s]\n",
      "100% 75/75 [00:03<00:00, 19.78it/s]\n",
      "f1 0.781988381988382\n",
      "efl_f1 0.8787633865427359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8095    0.7083    0.7556        24\n",
      "           1     0.7045    0.6596    0.6813        47\n",
      "           2     0.8889    0.9302    0.9091       129\n",
      "\n",
      "    accuracy                         0.8400       200\n",
      "   macro avg     0.8010    0.7660    0.7820       200\n",
      "weighted avg     0.8360    0.8400    0.8371       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7660512046057372\n",
      "efl_f1 0.8872205904171742\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 26 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-92\n"
     ]
    }
   ],
   "source": [
    "!python efl.py --base-model=textattack/bert-base-uncased-MNLI --cross-validation=5 --config-name=efl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ax71H9QwSAWD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5701547c-9f2c-4aa4-850f-49adab8a1f6a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2023-03-16 15:43:39.835564: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 15:43:42.009973: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 15:43:42.010153: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
      "2023-03-16 15:43:42.010180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "IS_CUDA_AVAILABLE True\n",
      "\n",
      " -------------------------------- Loading... -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      "Downloading (…)okenizer_config.json: 100% 258/258 [00:00<00:00, 33.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 776/776 [00:00<00:00, 95.8kB/s]\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Downloading (…)solve/main/vocab.txt: 100% 232k/232k [00:00<00:00, 347kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 39.3kB/s]\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\n",
      " -------------------------------- Training... -------------------------------- \n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 23 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 23 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100% 1/1 [00:00<00:00, 411.09it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-83265ee7ba365dcc.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cbc2cd51716047b4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-85c2440d4862542c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3ad14430404674f7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-872b357518bea1c7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-34edefb2d6f82357.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-049cd683442841e5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-214017cccb23a7dd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-027ca9f049962911.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c524627fecc7af2b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ac52133acdf98f56.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-15bfcaf344fd118e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-64f71e3c038aed86.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2f90841d80697968.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-27fcc4de9593ef64.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22778a4737bf1b06.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-99d066bd6b9c335d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d9c9a94d10a0f608.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-748b5a246ee06e2b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8344082634a0f403.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0f845d6ec688f0d1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e9e3c99a73c9e8a8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-866eed363c382697.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7f98c8c9021a8576.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1a203f297b61dc35.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22bf99d1ddcb7162.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f1812c603afc100a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7588590f7eca1f61.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-945b99ee96fc5717.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dbf66b68a7a0a6b0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1079b40689d85528.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5e8f99677ff049eb.arrow\n",
      "                                                   \n",
      " -------------------------------- Fold 1/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Downloading pytorch_model.bin: 100% 268M/268M [00:48<00:00, 5.51MB/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/integrations.py:1276: NeptuneDeprecationWarning: The object you're logging will be implicitly cast to a string. We'll end support of this behavior in `neptune-client==1.0.0`. To log the object as a string, use `str(object)` or `repr(object)` instead. For details, see https://docs.neptune.ai/setup/neptune-client_1-0_release_changes\n",
      "  self._metadata_namespace[NeptuneCallback.model_parameters_key] = model.config.to_dict()\n",
      "  0% 0/6000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.4975, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:19<06:05, 15.61it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 101.03it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 97.27it/s] \u001B[A\n",
      " 43% 32/75 [00:00<00:00, 94.96it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 89.84it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 90.42it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 90.91it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3932655453681946, 'eval_f1': 0.8135463738654167, 'eval_runtime': 2.7162, 'eval_samples_per_second': 220.895, 'eval_steps_per_second': 27.612, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:22<06:05, 15.61it/s]\n",
      "100% 75/75 [00:02<00:00, 86.98it/s]\u001B[A\n",
      "{'loss': 0.3038, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:44<06:25, 14.01it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 79.23it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:00, 72.56it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 70.43it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 70.02it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 66.78it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 66.21it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 66.31it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 65.81it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4083458483219147, 'eval_f1': 0.8605659987605867, 'eval_runtime': 3.0391, 'eval_samples_per_second': 197.428, 'eval_steps_per_second': 24.679, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:47<06:25, 14.01it/s]\n",
      "100% 75/75 [00:03<00:00, 63.35it/s]\u001B[A\n",
      "{'loss': 0.2156, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:08<05:02, 16.87it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 108.60it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 104.45it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 105.94it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 98.56it/s] \u001B[A\n",
      " 75% 56/75 [00:00<00:00, 101.14it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5216221213340759, 'eval_f1': 0.8622674988522292, 'eval_runtime': 2.6344, 'eval_samples_per_second': 227.758, 'eval_steps_per_second': 28.47, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:11<05:02, 16.87it/s]\n",
      "100% 75/75 [00:02<00:00, 103.35it/s]\u001B[A\n",
      "{'loss': 0.1429, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:32<05:32, 14.43it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 78.32it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 71.90it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 69.61it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 68.86it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 68.79it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 63.80it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 65.16it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 66.55it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 67.57it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.562411367893219, 'eval_f1': 0.8676940088395741, 'eval_runtime': 3.0185, 'eval_samples_per_second': 198.776, 'eval_steps_per_second': 24.847, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:35<05:32, 14.43it/s]\n",
      "100% 75/75 [00:03<00:00, 61.37it/s]\u001B[A\n",
      "{'loss': 0.0957, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:56<04:13, 17.77it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 113.72it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 105.57it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 103.12it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 96.38it/s] \u001B[A\n",
      " 76% 57/75 [00:00<00:00, 99.24it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6800506711006165, 'eval_f1': 0.8527607361963192, 'eval_runtime': 2.6395, 'eval_samples_per_second': 227.319, 'eval_steps_per_second': 28.415, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:59<04:13, 17.77it/s]\n",
      "100% 75/75 [00:02<00:00, 101.89it/s]\u001B[A\n",
      "{'loss': 0.0626, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:20<04:57, 14.13it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 76.83it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 69.63it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 69.44it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 69.04it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 66.73it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 63.30it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 63.96it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 65.33it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 66.41it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6920515894889832, 'eval_f1': 0.870148056676554, 'eval_runtime': 3.0297, 'eval_samples_per_second': 198.042, 'eval_steps_per_second': 24.755, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:23<04:57, 14.13it/s]\n",
      "100% 75/75 [00:03<00:00, 61.19it/s]\u001B[A\n",
      "{'loss': 0.0421, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:45<03:42, 17.56it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 114.55it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 110.36it/s]\u001B[A\n",
      " 48% 36/75 [00:00<00:00, 108.70it/s]\u001B[A\n",
      " 63% 47/75 [00:00<00:00, 100.16it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 99.19it/s] \u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.769932746887207, 'eval_f1': 0.8598201656900385, 'eval_runtime': 2.6472, 'eval_samples_per_second': 226.652, 'eval_steps_per_second': 28.332, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:47<03:42, 17.56it/s]\n",
      "100% 75/75 [00:02<00:00, 98.31it/s]\u001B[A\n",
      "{'loss': 0.0325, 'learning_rate': 6.3228070175438606e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:09<04:12, 14.27it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 72.48it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 69.40it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 68.84it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 68.93it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 69.19it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 65.12it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 67.07it/s]\u001B[A\n",
      " 80% 60/75 [00:00<00:00, 68.17it/s]\u001B[A\n",
      " 89% 67/75 [00:00<00:00, 68.47it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8242807984352112, 'eval_f1': 0.8527836012112742, 'eval_runtime': 2.9957, 'eval_samples_per_second': 200.29, 'eval_steps_per_second': 25.036, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:12<04:12, 14.27it/s]\n",
      "100% 75/75 [00:02<00:00, 65.38it/s]\u001B[A\n",
      "{'loss': 0.023, 'learning_rate': 5.796491228070176e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:33<03:09, 17.43it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 112.83it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 108.89it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 109.36it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 98.41it/s] \u001B[A\n",
      " 76% 57/75 [00:00<00:00, 100.97it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8056405782699585, 'eval_f1': 0.8639644292174176, 'eval_runtime': 2.6401, 'eval_samples_per_second': 227.266, 'eval_steps_per_second': 28.408, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:36<03:09, 17.43it/s]\n",
      "100% 75/75 [00:02<00:00, 100.49it/s]\u001B[A\n",
      "{'loss': 0.0112, 'learning_rate': 5.2701754385964925e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:57<03:27, 14.43it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 78.70it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 71.62it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 70.10it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 70.21it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 70.37it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 82.20it/s]\u001B[A\n",
      " 84% 63/75 [00:00<00:00, 89.58it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8701465129852295, 'eval_f1': 0.8598964479079426, 'eval_runtime': 2.8118, 'eval_samples_per_second': 213.387, 'eval_steps_per_second': 26.673, 'epoch': 10.0}\n",
      " 50% 3000/6000 [04:00<03:27, 14.43it/s]\n",
      "100% 75/75 [00:02<00:00, 82.35it/s]\u001B[A\n",
      "{'loss': 0.0065, 'learning_rate': 4.743859649122807e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:22<02:38, 17.03it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 110.39it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 108.82it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 105.93it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 96.39it/s] \u001B[A\n",
      " 75% 56/75 [00:00<00:00, 97.19it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8885813355445862, 'eval_f1': 0.86125, 'eval_runtime': 2.6449, 'eval_samples_per_second': 226.851, 'eval_steps_per_second': 28.356, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:24<02:38, 17.03it/s]\n",
      "100% 75/75 [00:02<00:00, 100.27it/s]\u001B[A\n",
      "{'train_runtime': 268.4455, 'train_samples_per_second': 178.807, 'train_steps_per_second': 22.351, 'train_loss': 0.13031078056855636, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:28<02:38, 17.03it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      " 55% 3300/6000 [04:29<03:40, 12.27it/s]\n",
      "100% 75/75 [00:03<00:00, 24.89it/s]\n",
      "f1 0.7271027242976752\n",
      "efl_f1 0.870148056676554\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5455    0.5000    0.5217        12\n",
      "           1     0.7869    0.7619    0.7742        63\n",
      "           2     0.8750    0.8960    0.8854       125\n",
      "\n",
      "    accuracy                         0.8300       200\n",
      "   macro avg     0.7358    0.7193    0.7271       200\n",
      "weighted avg     0.8275    0.8300    0.8285       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-253d68726aaa6eba.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1ce5ed5d0818e536.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b9edf76b9a2ea4a1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1486e0b7fb7638b9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5a9da5b1a4bbb6b6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d450a457fafdce3c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-27b4ea27be0304f7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ed48ccaab263bd7c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22ef49944d60307d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-df2d043ee91148a1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-200ed7f374a8a3c8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3ec382b63865306c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-15f7462ef1295d80.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-224ce19d719aebb3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8ba57b0bce5b6f8d.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d8999c5deeb66450.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0181ad2feee98877.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e7c895de1dcf38d8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3e019b2219e677a3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-67455b6c692be8bf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b41af4f3cc6d910d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-231859f9b1fcdecd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-63f0a024b0d91d6c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-50993f9c0dcd1f4d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6206ef4fba6ed013.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-83b8516859ffc5d5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-69da52ba02823ee3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-aa9d14c0f40c3dd0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-53546208c318094f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2415b6711bc36e23.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ebef0be9a197dc36.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5d6129bb91eaa375.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 2/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5469, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:19<06:39, 14.27it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 78.69it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 70.30it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 70.65it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 83.65it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 89.39it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 93.71it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3509316146373749, 'eval_f1': 0.8190863862505653, 'eval_runtime': 2.7183, 'eval_samples_per_second': 220.723, 'eval_steps_per_second': 27.59, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:21<06:39, 14.27it/s]\n",
      "100% 75/75 [00:02<00:00, 92.93it/s]\u001B[A\n",
      "{'loss': 0.3181, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:43<05:13, 17.22it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 113.86it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 106.86it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 105.14it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 104.81it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 104.76it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.36367321014404297, 'eval_f1': 0.8544703716602816, 'eval_runtime': 2.5841, 'eval_samples_per_second': 232.189, 'eval_steps_per_second': 29.024, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:46<05:13, 17.22it/s]\n",
      "100% 75/75 [00:02<00:00, 104.97it/s]\u001B[A\n",
      "{'loss': 0.2254, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:08<05:59, 14.20it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 114.90it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 107.24it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 106.09it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 105.66it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 105.19it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4929030239582062, 'eval_f1': 0.8565851326587522, 'eval_runtime': 2.5781, 'eval_samples_per_second': 232.732, 'eval_steps_per_second': 29.091, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:10<05:59, 14.20it/s]\n",
      "100% 75/75 [00:02<00:00, 103.75it/s]\u001B[A\n",
      "{'loss': 0.1549, 'learning_rate': 8.428070175438597e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:32<04:40, 17.09it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 112.95it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 104.15it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 100.54it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 101.90it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 101.04it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.531616747379303, 'eval_f1': 0.8691194648440341, 'eval_runtime': 2.6116, 'eval_samples_per_second': 229.744, 'eval_steps_per_second': 28.718, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:34<04:40, 17.09it/s]\n",
      "100% 75/75 [00:02<00:00, 99.61it/s]\u001B[A\n",
      "{'loss': 0.0819, 'learning_rate': 7.901754385964913e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:56<04:37, 16.19it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 111.86it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 104.79it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 104.88it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 103.50it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 103.17it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6285439729690552, 'eval_f1': 0.8725747197119302, 'eval_runtime': 2.6168, 'eval_samples_per_second': 229.288, 'eval_steps_per_second': 28.661, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:59<04:37, 16.19it/s]\n",
      "100% 75/75 [00:02<00:00, 99.18it/s]\u001B[A\n",
      "{'loss': 0.0717, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:20<04:03, 17.24it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 105.83it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 101.37it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 102.79it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 103.46it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 103.64it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6546472907066345, 'eval_f1': 0.8751455858374098, 'eval_runtime': 2.5948, 'eval_samples_per_second': 231.228, 'eval_steps_per_second': 28.903, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:23<04:03, 17.24it/s]\n",
      "100% 75/75 [00:02<00:00, 103.27it/s]\u001B[A\n",
      "{'loss': 0.0574, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:45<04:17, 15.16it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 103.95it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 100.40it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 99.82it/s] \u001B[A\n",
      " 59% 44/75 [00:00<00:00, 102.63it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 104.30it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7211998701095581, 'eval_f1': 0.8670407835230447, 'eval_runtime': 2.5893, 'eval_samples_per_second': 231.725, 'eval_steps_per_second': 28.966, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:48<04:17, 15.16it/s]\n",
      "100% 75/75 [00:02<00:00, 101.40it/s]\u001B[A\n",
      "{'loss': 0.0318, 'learning_rate': 6.324561403508772e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:09<03:23, 17.69it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 107.39it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 100.96it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 102.02it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 103.60it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 104.35it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7388848066329956, 'eval_f1': 0.8738985730462905, 'eval_runtime': 2.6405, 'eval_samples_per_second': 227.233, 'eval_steps_per_second': 28.404, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:12<03:23, 17.69it/s]\n",
      "100% 75/75 [00:02<00:00, 101.42it/s]\u001B[A\n",
      "{'loss': 0.0144, 'learning_rate': 5.798245614035089e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:34<03:08, 17.46it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 107.61it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 100.89it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 103.70it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 103.73it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 102.09it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7816278338432312, 'eval_f1': 0.8687499999999999, 'eval_runtime': 2.5916, 'eval_samples_per_second': 231.514, 'eval_steps_per_second': 28.939, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:36<03:08, 17.46it/s]\n",
      "100% 75/75 [00:02<00:00, 102.71it/s]\u001B[A\n",
      "{'loss': 0.0103, 'learning_rate': 5.271929824561403e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:58<03:05, 16.16it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 74.80it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 76.14it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 70.10it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 70.17it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 72.03it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 72.48it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 73.08it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 73.39it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8003330826759338, 'eval_f1': 0.8777864062838157, 'eval_runtime': 2.9557, 'eval_samples_per_second': 202.998, 'eval_steps_per_second': 25.375, 'epoch': 10.0}\n",
      " 50% 3000/6000 [04:01<03:05, 16.16it/s]\n",
      "100% 75/75 [00:02<00:00, 72.32it/s]\u001B[A\n",
      "{'loss': 0.0056, 'learning_rate': 4.74561403508772e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:22<02:56, 15.28it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 105.04it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 104.16it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 105.25it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 106.68it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 105.67it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8270869851112366, 'eval_f1': 0.8790794926377046, 'eval_runtime': 2.5681, 'eval_samples_per_second': 233.636, 'eval_steps_per_second': 29.205, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:25<02:56, 15.28it/s]\n",
      "100% 75/75 [00:02<00:00, 106.09it/s]\u001B[A\n",
      "{'loss': 0.008, 'learning_rate': 4.219298245614035e-06, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:47<02:38, 15.17it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 78.59it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 75.26it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 73.49it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 70.51it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 70.01it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 69.74it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 68.48it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 68.65it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8361536264419556, 'eval_f1': 0.8749747423721963, 'eval_runtime': 2.9532, 'eval_samples_per_second': 203.171, 'eval_steps_per_second': 25.396, 'epoch': 12.0}\n",
      " 60% 3600/6000 [04:50<02:38, 15.17it/s]\n",
      "100% 75/75 [00:02<00:00, 68.85it/s]\u001B[A\n",
      "{'loss': 0.0028, 'learning_rate': 3.692982456140351e-06, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:12<02:07, 16.49it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 109.14it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 105.83it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 102.52it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 101.77it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 102.92it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8627159595489502, 'eval_f1': 0.8767067422443797, 'eval_runtime': 2.5948, 'eval_samples_per_second': 231.232, 'eval_steps_per_second': 28.904, 'epoch': 13.0}\n",
      " 65% 3900/6000 [05:15<02:07, 16.49it/s]\n",
      "100% 75/75 [00:02<00:00, 103.67it/s]\u001B[A\n",
      "{'loss': 0.0079, 'learning_rate': 3.1666666666666667e-06, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:37<02:09, 13.95it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 75.26it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 69.99it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 68.38it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 67.46it/s]\u001B[A\n",
      " 51% 38/75 [00:00<00:00, 66.84it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 66.82it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 66.39it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 66.82it/s]\u001B[A\n",
      " 89% 67/75 [00:00<00:00, 67.67it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8559544682502747, 'eval_f1': 0.8770301843601312, 'eval_runtime': 3.0026, 'eval_samples_per_second': 199.827, 'eval_steps_per_second': 24.978, 'epoch': 14.0}\n",
      " 70% 4200/6000 [05:40<02:09, 13.95it/s]\n",
      "100% 75/75 [00:02<00:00, 67.82it/s]\u001B[A\n",
      "{'loss': 0.0017, 'learning_rate': 2.6403508771929826e-06, 'epoch': 15.0}\n",
      " 75% 4500/6000 [06:01<01:26, 17.27it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 101.38it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 101.31it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 102.34it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 102.00it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 100.69it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9029064774513245, 'eval_f1': 0.8746438746438747, 'eval_runtime': 2.6037, 'eval_samples_per_second': 230.442, 'eval_steps_per_second': 28.805, 'epoch': 15.0}\n",
      " 75% 4500/6000 [06:04<01:26, 17.27it/s]\n",
      "100% 75/75 [00:02<00:00, 101.31it/s]\u001B[A\n",
      "{'loss': 0.0048, 'learning_rate': 2.114035087719298e-06, 'epoch': 16.0}\n",
      " 80% 4800/6000 [06:26<01:27, 13.69it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 111.47it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 104.46it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 103.05it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 100.65it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 97.11it/s] \u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.9184585809707642, 'eval_f1': 0.8753007267826327, 'eval_runtime': 2.5997, 'eval_samples_per_second': 230.796, 'eval_steps_per_second': 28.85, 'epoch': 16.0}\n",
      " 80% 4800/6000 [06:29<01:27, 13.69it/s]\n",
      "100% 75/75 [00:02<00:00, 98.28it/s]\u001B[A\n",
      "{'train_runtime': 392.788, 'train_samples_per_second': 122.203, 'train_steps_per_second': 15.275, 'train_loss': 0.09647219316413005, 'epoch': 16.0}\n",
      " 80% 4800/6000 [06:32<01:27, 13.69it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 38 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 38 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      " 80% 4800/6000 [06:33<01:38, 12.20it/s]\n",
      "100% 75/75 [00:02<00:00, 28.41it/s] \n",
      "f1 0.6622235665792835\n",
      "efl_f1 0.8790794926377046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.2143    0.3158        14\n",
      "           1     0.6970    0.8519    0.7667        54\n",
      "           2     0.9147    0.8939    0.9042       132\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.7372    0.6534    0.6622       200\n",
      "weighted avg     0.8339    0.8350    0.8259       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d30dcc499a9e4901.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a51adfe856ae1e6f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-494b3ec4ce11ceae.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ce54a7a8e6559a6b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5c10526786c137f1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9cbc05347cad9a91.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1a5468a15327172b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2ce775faee68847c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b303aff5322e9159.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2d2212d31dc860f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5c9f2da4ff004d95.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2a07026fb75911b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a652e225dc6fd622.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e3113caafd6a84e1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0dda2a1c75d11508.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c8f86ef360530e20.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-82d853a51a2e23e1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a472762b99394f5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2ff3e1c2a18e1e1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0c1a6c85873355ec.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7e8e338b296ff826.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4eefb5a2b564dc8d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9fc875bbbfb7ac4b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-54f8532427cc2a19.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3044b7b97a787fa7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-cc8ee79d0e41287b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c74a8c06def56b07.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f8888634c43dd491.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9c84dfd9ad7fb8bb.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f524902fd85761f4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f90f14d71466ff22.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1cb991b97d49fba9.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 3/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5368, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:18<05:26, 17.45it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 105.55it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 101.23it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 104.28it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 101.17it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 104.09it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.37551578879356384, 'eval_f1': 0.8180332739156269, 'eval_runtime': 2.6157, 'eval_samples_per_second': 229.382, 'eval_steps_per_second': 28.673, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:21<05:26, 17.45it/s]\n",
      "100% 75/75 [00:02<00:00, 101.69it/s]\u001B[A\n",
      "{'loss': 0.3158, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:43<05:31, 16.31it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 107.20it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 98.43it/s] \u001B[A\n",
      " 43% 32/75 [00:00<00:00, 98.35it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 96.81it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 99.04it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 100.12it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.38805222511291504, 'eval_f1': 0.835597134692919, 'eval_runtime': 2.613, 'eval_samples_per_second': 229.621, 'eval_steps_per_second': 28.703, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:45<05:31, 16.31it/s]\n",
      "100% 75/75 [00:02<00:00, 101.65it/s]\u001B[A\n",
      "{'loss': 0.2046, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:07<05:00, 16.94it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 105.69it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 96.90it/s] \u001B[A\n",
      " 44% 33/75 [00:00<00:00, 99.29it/s]\u001B[A\n",
      " 57% 43/75 [00:00<00:00, 95.16it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 98.15it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4554745554924011, 'eval_f1': 0.870148056676554, 'eval_runtime': 2.6378, 'eval_samples_per_second': 227.46, 'eval_steps_per_second': 28.433, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:09<05:00, 16.94it/s]\n",
      "100% 75/75 [00:02<00:00, 99.06it/s]\u001B[A\n",
      "{'loss': 0.114, 'learning_rate': 8.428070175438597e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:32<04:48, 16.67it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 103.24it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 98.22it/s] \u001B[A\n",
      " 43% 32/75 [00:00<00:00, 95.21it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 96.80it/s]\u001B[A\n",
      " 72% 54/75 [00:00<00:00, 101.94it/s]\u001B[A\n",
      " 87% 65/75 [00:00<00:00, 98.28it/s] \u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6199385523796082, 'eval_f1': 0.8582019005920691, 'eval_runtime': 2.613, 'eval_samples_per_second': 229.618, 'eval_steps_per_second': 28.702, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:34<04:48, 16.67it/s]\n",
      "100% 75/75 [00:02<00:00, 98.08it/s]\u001B[A\n",
      "{'loss': 0.0658, 'learning_rate': 7.901754385964913e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:56<04:14, 17.68it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 78.93it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:00, 75.95it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 74.22it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 73.41it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 75.00it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 75.30it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 75.04it/s]\u001B[A\n",
      " 87% 65/75 [00:00<00:00, 76.10it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6827962398529053, 'eval_f1': 0.8691194648440341, 'eval_runtime': 2.8802, 'eval_samples_per_second': 208.316, 'eval_steps_per_second': 26.04, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:59<04:14, 17.68it/s]\n",
      "100% 75/75 [00:02<00:00, 75.53it/s]\u001B[A\n",
      "{'loss': 0.0407, 'learning_rate': 7.375438596491229e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:20<03:55, 17.87it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 13% 10/75 [00:00<00:00, 98.67it/s]\u001B[A\n",
      " 27% 20/75 [00:00<00:00, 96.39it/s]\u001B[A\n",
      " 41% 31/75 [00:00<00:00, 99.69it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 99.58it/s]\u001B[A\n",
      " 69% 52/75 [00:00<00:00, 99.21it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 99.01it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7319170832633972, 'eval_f1': 0.8670465337132003, 'eval_runtime': 2.6335, 'eval_samples_per_second': 227.83, 'eval_steps_per_second': 28.479, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:23<03:55, 17.87it/s]\n",
      "100% 75/75 [00:02<00:00, 100.78it/s]\u001B[A\n",
      "{'loss': 0.0262, 'learning_rate': 6.849122807017544e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:44<04:08, 15.68it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 80.30it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 79.25it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 75.56it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 76.87it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 75.98it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 75.07it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 76.36it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 76.77it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7508760094642639, 'eval_f1': 0.8642339255836189, 'eval_runtime': 2.8711, 'eval_samples_per_second': 208.98, 'eval_steps_per_second': 26.122, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:47<04:08, 15.68it/s]\n",
      "100% 75/75 [00:02<00:00, 77.01it/s]\u001B[A\n",
      "{'loss': 0.0188, 'learning_rate': 6.324561403508772e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:09<03:38, 16.45it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 102.46it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 94.44it/s] \u001B[A\n",
      " 44% 33/75 [00:00<00:00, 100.37it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 99.84it/s] \u001B[A\n",
      " 73% 55/75 [00:00<00:00, 98.92it/s]\u001B[A\n",
      " 87% 65/75 [00:00<00:00, 98.95it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7505654692649841, 'eval_f1': 0.8684172888240509, 'eval_runtime': 2.6307, 'eval_samples_per_second': 228.076, 'eval_steps_per_second': 28.51, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:12<03:38, 16.45it/s]\n",
      "100% 75/75 [00:02<00:00, 98.00it/s]\u001B[A\n",
      "{'train_runtime': 195.4129, 'train_samples_per_second': 245.634, 'train_steps_per_second': 30.704, 'train_loss': 0.1653431193033854, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:15<03:38, 16.45it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 38 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 38 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      " 40% 2400/6000 [03:16<04:54, 12.24it/s]\n",
      "100% 75/75 [00:02<00:00, 25.79it/s]\n",
      "f1 0.7235555555555555\n",
      "efl_f1 0.870148056676554\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.5000    0.5333        16\n",
      "           1     0.6769    0.8000    0.7333        55\n",
      "           2     0.9339    0.8760    0.9040       129\n",
      "\n",
      "    accuracy                         0.8250       200\n",
      "   macro avg     0.7274    0.7253    0.7236       200\n",
      "weighted avg     0.8342    0.8250    0.8274       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bb8bc0cbd4562908.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dfb36a44c8fa3a83.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-837c2a351563182b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0f412579adee1d02.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bd1aeb1a48264ee0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f152c895d650e655.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-99ffe4e028db6c1c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-097467430541a59a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ec5914fd125b2658.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c655357c240db9d8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a9e764713adc8faf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fd1bb9907365ad72.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0c1f4a379c890127.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7a2ed18ab9b663f4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2fa483df7e3e5166.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eb2882b3a2ae2ecf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-477b6488956ccded.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-03e85928e546693d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e58270f13956086f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e0821d96ed671ab4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-91834b92a925b255.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5e031be53c45d0ab.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8739752c42f1d2f7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-841350b8b2c4edff.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-517686ca08db857c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-eee6d82cb4c69ef4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7343beabae73b2f6.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6fb827f609ea0e67.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d0f8c08c9ce07f73.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-581142d9a30fdeef.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bcd24f3b18b8ad48.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-20b677bb8cb29399.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 4/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.4937, 'learning_rate': 9.966666666666667e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:18<05:24, 17.54it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 74.58it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 70.04it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 71.58it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 71.64it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 71.33it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 73.15it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 74.22it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 72.57it/s]\u001B[A\n",
      " 96% 72/75 [00:00<00:00, 73.72it/s]\u001B[A\n",
      "{'eval_loss': 0.36817628145217896, 'eval_f1': 0.83875, 'eval_runtime': 2.9125, 'eval_samples_per_second': 206.008, 'eval_steps_per_second': 25.751, 'epoch': 1.0}\n",
      "\n",
      "  5% 300/6000 [00:21<05:24, 17.54it/s]\n",
      "{'loss': 0.3131, 'learning_rate': 9.477192982456142e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:43<05:20, 16.83it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 105.17it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 108.74it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 101.01it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 101.70it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 103.78it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.34777772426605225, 'eval_f1': 0.8886704929632834, 'eval_runtime': 2.6089, 'eval_samples_per_second': 229.985, 'eval_steps_per_second': 28.748, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:45<05:20, 16.83it/s]\n",
      "100% 75/75 [00:02<00:00, 101.80it/s]\u001B[A\n",
      "{'loss': 0.2234, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:07<05:31, 15.36it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 86.57it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 75.88it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 75.76it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 73.23it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 71.83it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 70.81it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 70.58it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 68.33it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.4732988476753235, 'eval_f1': 0.8704992128383526, 'eval_runtime': 2.9865, 'eval_samples_per_second': 200.904, 'eval_steps_per_second': 25.113, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:10<05:31, 15.36it/s]\n",
      "100% 75/75 [00:02<00:00, 67.58it/s]\u001B[A\n",
      "{'loss': 0.1387, 'learning_rate': 8.426315789473684e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:31<04:34, 17.49it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 102.53it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 103.61it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 102.59it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 101.33it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 103.83it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5442207455635071, 'eval_f1': 0.8670407835230447, 'eval_runtime': 2.6119, 'eval_samples_per_second': 229.722, 'eval_steps_per_second': 28.715, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:34<04:34, 17.49it/s]\n",
      "100% 75/75 [00:02<00:00, 101.58it/s]\u001B[A\n",
      "{'loss': 0.0718, 'learning_rate': 7.9e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:56<05:05, 14.72it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 86.81it/s]\u001B[A\n",
      " 24% 18/75 [00:00<00:00, 77.93it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 74.69it/s]\u001B[A\n",
      " 45% 34/75 [00:00<00:00, 74.96it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 74.42it/s]\u001B[A\n",
      " 67% 50/75 [00:00<00:00, 75.06it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 74.83it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 73.52it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5794552564620972, 'eval_f1': 0.882241215574549, 'eval_runtime': 2.9314, 'eval_samples_per_second': 204.679, 'eval_steps_per_second': 25.585, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:59<05:05, 14.72it/s]\n",
      "100% 75/75 [00:02<00:00, 71.98it/s]\u001B[A\n",
      "{'loss': 0.0494, 'learning_rate': 7.37719298245614e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:20<04:00, 17.47it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 108.90it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 102.99it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 99.59it/s] \u001B[A\n",
      " 59% 44/75 [00:00<00:00, 101.71it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 104.42it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6095436811447144, 'eval_f1': 0.8889554598467335, 'eval_runtime': 2.6034, 'eval_samples_per_second': 230.472, 'eval_steps_per_second': 28.809, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:23<04:00, 17.47it/s]\n",
      "100% 75/75 [00:02<00:00, 100.32it/s]\u001B[A\n",
      "{'loss': 0.0291, 'learning_rate': 6.850877192982457e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:45<04:26, 14.64it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:00, 69.40it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 67.27it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 66.54it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 66.43it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 66.72it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 67.62it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 65.38it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 65.84it/s]\u001B[A\n",
      " 84% 63/75 [00:00<00:00, 65.65it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6642121076583862, 'eval_f1': 0.8845975576302769, 'eval_runtime': 3.0282, 'eval_samples_per_second': 198.139, 'eval_steps_per_second': 24.767, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:48<04:26, 14.64it/s]\n",
      "100% 75/75 [00:03<00:00, 65.07it/s]\u001B[A\n",
      "{'loss': 0.0187, 'learning_rate': 6.324561403508772e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:09<03:30, 17.14it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 109.29it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 105.98it/s]\u001B[A\n",
      " 44% 33/75 [00:00<00:00, 104.35it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 106.38it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 101.88it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7372103333473206, 'eval_f1': 0.88116458704694, 'eval_runtime': 2.6475, 'eval_samples_per_second': 226.632, 'eval_steps_per_second': 28.329, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:12<03:30, 17.14it/s]\n",
      "100% 75/75 [00:02<00:00, 102.25it/s]\u001B[A\n",
      "{'loss': 0.0168, 'learning_rate': 5.798245614035089e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:34<04:01, 13.67it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      "  9% 7/75 [00:00<00:01, 67.21it/s]\u001B[A\n",
      " 19% 14/75 [00:00<00:00, 66.02it/s]\u001B[A\n",
      " 28% 21/75 [00:00<00:00, 66.75it/s]\u001B[A\n",
      " 37% 28/75 [00:00<00:00, 66.04it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 66.66it/s]\u001B[A\n",
      " 56% 42/75 [00:00<00:00, 67.14it/s]\u001B[A\n",
      " 65% 49/75 [00:00<00:00, 67.05it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 67.37it/s]\u001B[A\n",
      " 84% 63/75 [00:00<00:00, 67.96it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7475398778915405, 'eval_f1': 0.8825520307132754, 'eval_runtime': 2.9913, 'eval_samples_per_second': 200.58, 'eval_steps_per_second': 25.072, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:37<04:01, 13.67it/s]\n",
      "100% 75/75 [00:02<00:00, 66.65it/s]\u001B[A\n",
      "{'loss': 0.0118, 'learning_rate': 5.271929824561403e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:59<02:50, 17.64it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 110.44it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 100.88it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 103.06it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 101.61it/s]\u001B[A\n",
      " 76% 57/75 [00:00<00:00, 102.99it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7525490522384644, 'eval_f1': 0.8782767736478577, 'eval_runtime': 2.5939, 'eval_samples_per_second': 231.311, 'eval_steps_per_second': 28.914, 'epoch': 10.0}\n",
      " 50% 3000/6000 [04:02<02:50, 17.64it/s]\n",
      "100% 75/75 [00:02<00:00, 102.40it/s]\u001B[A\n",
      "{'loss': 0.0093, 'learning_rate': 4.74561403508772e-06, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:24<03:14, 13.91it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 78.42it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 70.95it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 69.58it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 70.44it/s]\u001B[A\n",
      " 53% 40/75 [00:00<00:00, 70.04it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 68.52it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 76.32it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7649559378623962, 'eval_f1': 0.8851912381466227, 'eval_runtime': 2.848, 'eval_samples_per_second': 210.678, 'eval_steps_per_second': 26.335, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:27<03:14, 13.91it/s]\n",
      "100% 75/75 [00:02<00:00, 82.29it/s]\u001B[A\n",
      "{'train_runtime': 270.5185, 'train_samples_per_second': 177.437, 'train_steps_per_second': 22.18, 'train_loss': 0.12506714004458802, 'epoch': 11.0}\n",
      " 55% 3300/6000 [04:30<03:14, 13.91it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      " 55% 3300/6000 [04:31<03:42, 12.14it/s]\n",
      "100% 75/75 [00:02<00:00, 28.59it/s]\n",
      "f1 0.7140416099014016\n",
      "efl_f1 0.8889554598467335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5556    0.3846    0.4545        13\n",
      "           1     0.7619    0.7869    0.7742        61\n",
      "           2     0.9062    0.9206    0.9134       126\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.7412    0.6974    0.7140       200\n",
      "weighted avg     0.8394    0.8450    0.8411       200\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9e63f20091859520.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a33021527eb8a188.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-480665de5aac5218.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7c3e8d774b99082f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a784aa1e23a6ccad.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f99cc31d80f6d6a3.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-140ce776af1e63b7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e211bab5b8a6c151.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7f2149183938124f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-75a1039870cc790a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-78e7f352099e1a0f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-7656cbaad7b70fb1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-62dcffc758e2ce5c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-15f4f85727044ae2.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-90903203a8463a2d.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a1f2ac9fafac5d12.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-83dcab1ee4c387ce.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c6669ed08d115a32.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-09f3014bd3be9a4c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b7511b613a85ab7d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8d06d6709c13927a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-13319056394452b1.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b0d1d6f05975107b.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dd80312c2fbb5ea7.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d1cca667f79fb639.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8744559ec525c4c0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-522da4945bf30c78.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3fdf8a75f13295c9.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f2e034f93c3ad496.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-67fb33cf49ea3157.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-10afa5ed95c2597a.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-d206f4069233af02/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-686afed83ddad719.arrow\n",
      "                                       \n",
      " -------------------------------- Fold 5/5 -------------------------------- \n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at typeform/distilbert-base-uncased-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "{'loss': 0.5385, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:18<05:24, 17.59it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15% 11/75 [00:00<00:00, 108.73it/s]\u001B[A\n",
      " 29% 22/75 [00:00<00:00, 99.66it/s] \u001B[A\n",
      " 44% 33/75 [00:00<00:00, 101.44it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 96.24it/s] \u001B[A\n",
      " 72% 54/75 [00:00<00:00, 97.22it/s]\u001B[A\n",
      " 85% 64/75 [00:00<00:00, 97.90it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.41079410910606384, 'eval_f1': 0.7872202640318985, 'eval_runtime': 2.6191, 'eval_samples_per_second': 229.09, 'eval_steps_per_second': 28.636, 'epoch': 1.0}\n",
      "  5% 300/6000 [00:20<05:24, 17.59it/s]\n",
      "100% 75/75 [00:02<00:00, 96.93it/s]\u001B[A\n",
      "{'loss': 0.3358, 'learning_rate': 9.478947368421053e-06, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:42<06:09, 14.61it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 70.92it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 69.77it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 67.96it/s]\u001B[A\n",
      " 40% 30/75 [00:00<00:00, 65.61it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 66.53it/s]\u001B[A\n",
      " 59% 44/75 [00:00<00:00, 66.23it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 62.82it/s]\u001B[A\n",
      " 77% 58/75 [00:00<00:00, 64.67it/s]\u001B[A\n",
      " 88% 66/75 [00:00<00:00, 67.03it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.3683234751224518, 'eval_f1': 0.8275217220626918, 'eval_runtime': 2.9796, 'eval_samples_per_second': 201.367, 'eval_steps_per_second': 25.171, 'epoch': 2.0}\n",
      " 10% 600/6000 [00:45<06:09, 14.61it/s]\n",
      "100% 75/75 [00:02<00:00, 67.00it/s]\u001B[A\n",
      "{'loss': 0.2275, 'learning_rate': 8.95263157894737e-06, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:07<04:54, 17.33it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:00, 123.88it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 106.47it/s]\u001B[A\n",
      " 49% 37/75 [00:00<00:00, 101.20it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 97.91it/s] \u001B[A\n",
      " 79% 59/75 [00:00<00:00, 98.56it/s]\u001B[A\n",
      "                                      \n",
      "\u001B[A{'eval_loss': 0.5468817353248596, 'eval_f1': 0.8429770046444753, 'eval_runtime': 2.6222, 'eval_samples_per_second': 228.812, 'eval_steps_per_second': 28.602, 'epoch': 3.0}\n",
      " 15% 900/6000 [01:09<04:54, 17.33it/s]\n",
      "100% 75/75 [00:02<00:00, 96.68it/s]\u001B[A\n",
      "{'loss': 0.1447, 'learning_rate': 8.428070175438597e-06, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:31<05:51, 13.64it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 79.28it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 73.03it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 70.90it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 68.64it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 68.13it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 67.18it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 65.47it/s]\u001B[A\n",
      " 81% 61/75 [00:00<00:00, 66.99it/s]\u001B[A\n",
      " 91% 68/75 [00:00<00:00, 67.02it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5758581757545471, 'eval_f1': 0.8571410669306632, 'eval_runtime': 2.971, 'eval_samples_per_second': 201.951, 'eval_steps_per_second': 25.244, 'epoch': 4.0}\n",
      " 20% 1200/6000 [01:34<05:51, 13.64it/s]\n",
      "100% 75/75 [00:02<00:00, 65.73it/s]\u001B[A\n",
      "{'loss': 0.1, 'learning_rate': 7.903508771929826e-06, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:56<04:12, 17.83it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 17% 13/75 [00:00<00:00, 122.77it/s]\u001B[A\n",
      " 35% 26/75 [00:00<00:00, 98.54it/s] \u001B[A\n",
      " 49% 37/75 [00:00<00:00, 99.24it/s]\u001B[A\n",
      " 64% 48/75 [00:00<00:00, 96.98it/s]\u001B[A\n",
      " 79% 59/75 [00:00<00:00, 98.96it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.5706300139427185, 'eval_f1': 0.8721788493590146, 'eval_runtime': 2.6886, 'eval_samples_per_second': 223.164, 'eval_steps_per_second': 27.896, 'epoch': 5.0}\n",
      " 25% 1500/6000 [01:58<04:12, 17.83it/s]\n",
      "100% 75/75 [00:02<00:00, 99.67it/s]\u001B[A\n",
      "{'loss': 0.0776, 'learning_rate': 7.37719298245614e-06, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:20<05:19, 13.14it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 12% 9/75 [00:00<00:00, 79.48it/s]\u001B[A\n",
      " 23% 17/75 [00:00<00:00, 73.52it/s]\u001B[A\n",
      " 33% 25/75 [00:00<00:00, 69.60it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 69.56it/s]\u001B[A\n",
      " 52% 39/75 [00:00<00:00, 68.41it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 68.28it/s]\u001B[A\n",
      " 71% 53/75 [00:00<00:00, 67.84it/s]\u001B[A\n",
      " 80% 60/75 [00:00<00:00, 65.37it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.6892320513725281, 'eval_f1': 0.8552638261134529, 'eval_runtime': 2.8954, 'eval_samples_per_second': 207.225, 'eval_steps_per_second': 25.903, 'epoch': 6.0}\n",
      " 30% 1800/6000 [02:23<05:19, 13.14it/s]\n",
      "100% 75/75 [00:02<00:00, 74.97it/s]\u001B[A\n",
      "{'loss': 0.0483, 'learning_rate': 6.850877192982457e-06, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:44<03:49, 16.98it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 115.10it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 100.95it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 98.01it/s] \u001B[A\n",
      " 60% 45/75 [00:00<00:00, 96.36it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 95.30it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7231398224830627, 'eval_f1': 0.8622533218020285, 'eval_runtime': 2.6298, 'eval_samples_per_second': 228.158, 'eval_steps_per_second': 28.52, 'epoch': 7.0}\n",
      " 35% 2100/6000 [02:47<03:49, 16.98it/s]\n",
      "100% 75/75 [00:02<00:00, 97.84it/s]\u001B[A\n",
      "{'loss': 0.0325, 'learning_rate': 6.324561403508772e-06, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:09<04:09, 14.40it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 79.97it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 66.99it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 66.56it/s]\u001B[A\n",
      " 43% 32/75 [00:00<00:00, 73.79it/s]\u001B[A\n",
      " 55% 41/75 [00:00<00:00, 78.61it/s]\u001B[A\n",
      " 68% 51/75 [00:00<00:00, 85.22it/s]\u001B[A\n",
      " 83% 62/75 [00:00<00:00, 90.70it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.7926733493804932, 'eval_f1': 0.8561891400430154, 'eval_runtime': 2.7776, 'eval_samples_per_second': 216.014, 'eval_steps_per_second': 27.002, 'epoch': 8.0}\n",
      " 40% 2400/6000 [03:12<04:09, 14.40it/s]\n",
      "100% 75/75 [00:02<00:00, 94.64it/s]\u001B[A\n",
      "{'loss': 0.0272, 'learning_rate': 5.798245614035089e-06, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:33<03:15, 16.89it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 16% 12/75 [00:00<00:00, 118.88it/s]\u001B[A\n",
      " 32% 24/75 [00:00<00:00, 99.87it/s] \u001B[A\n",
      " 47% 35/75 [00:00<00:00, 99.61it/s]\u001B[A\n",
      " 61% 46/75 [00:00<00:00, 97.28it/s]\u001B[A\n",
      " 75% 56/75 [00:00<00:00, 97.76it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8542016744613647, 'eval_f1': 0.8523541837774959, 'eval_runtime': 2.6429, 'eval_samples_per_second': 227.023, 'eval_steps_per_second': 28.378, 'epoch': 9.0}\n",
      " 45% 2700/6000 [03:36<03:15, 16.89it/s]\n",
      "100% 75/75 [00:02<00:00, 98.35it/s]\u001B[A\n",
      "{'loss': 0.0192, 'learning_rate': 5.271929824561403e-06, 'epoch': 10.0}\n",
      " 50% 3000/6000 [03:58<03:36, 13.83it/s]\n",
      "  0% 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 11% 8/75 [00:00<00:00, 71.83it/s]\u001B[A\n",
      " 21% 16/75 [00:00<00:00, 66.43it/s]\u001B[A\n",
      " 31% 23/75 [00:00<00:00, 64.39it/s]\u001B[A\n",
      " 47% 35/75 [00:00<00:00, 80.86it/s]\u001B[A\n",
      " 60% 45/75 [00:00<00:00, 85.74it/s]\u001B[A\n",
      " 73% 55/75 [00:00<00:00, 89.12it/s]\u001B[A\n",
      "                                       \n",
      "\u001B[A{'eval_loss': 0.8603712916374207, 'eval_f1': 0.856031521519499, 'eval_runtime': 2.7596, 'eval_samples_per_second': 217.423, 'eval_steps_per_second': 27.178, 'epoch': 10.0}\n",
      " 50% 3000/6000 [04:01<03:36, 13.83it/s]\n",
      "100% 75/75 [00:02<00:00, 93.33it/s]\u001B[A\n",
      "{'train_runtime': 244.4446, 'train_samples_per_second': 196.364, 'train_steps_per_second': 24.545, 'train_loss': 0.15512966378529866, 'epoch': 10.0}\n",
      " 50% 3000/6000 [04:04<03:36, 13.83it/s]Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 14 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 14 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      " 50% 3000/6000 [04:04<04:04, 12.25it/s]\n",
      "100% 75/75 [00:02<00:00, 28.76it/s]\n",
      "f1 0.7703585938880056\n",
      "efl_f1 0.8721788493590146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8667    0.5417    0.6667        24\n",
      "           1     0.6727    0.7872    0.7255        47\n",
      "           2     0.9154    0.9225    0.9189       129\n",
      "\n",
      "    accuracy                         0.8450       200\n",
      "   macro avg     0.8183    0.7505    0.7704       200\n",
      "weighted avg     0.8525    0.8450    0.8432       200\n",
      "\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n",
      "\n",
      " -------------------------------- End -------------------------------- \n",
      "\n",
      "test_f1 0.7291648882502502\n",
      "efl_f1 0.8761019830393121\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 26 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 26 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/k4black/jb-reaction-prediction/e/JBREAC-91\n"
     ]
    }
   ],
   "source": [
    "!python efl.py --base-model=typeform/distilbert-base-uncased-mnli --cross-validation=5 --config-name=efl"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "b_O7UUZ3bitY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "eaaAkObsqMAT"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
